{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Gualtiero Testa's blog","text":"<p>The Articles section contains posts on different topics.</p> <p>The Books Reviews section is dedicated to book reviews.</p> <p>The Certifications section is dedicated to material I collected during my studies for some IT certifications.</p> <p>The Languages section contains info on several programming languages.</p> <p>The My Books section lists the books I co-wrote.</p> <p>The Methodologies section summarizes my notes on key methodology concepts.</p> <p>The Security section includes a set of notes and explanations on application security.</p> <p>The Tools section includes a set of handy and quick references to several developers' tools.</p>"},{"location":"about/","title":"About me","text":"<p>Dad, husband, Italian. </p> <p>Software architect and developer.</p> <p>Blogger.</p> <p>Amateur photographer.</p> <p>Love code quality, testing and automation.</p> <p>Note: the picture has been taken in Rapallo (GE), Italy.</p> <p>You can found me here:</p> <ul> <li>Personal site: https://gualtierotesta.github.io/blog</li> <li>Blog on Medium @gualtierotesta: https://medium.com/@gualtierotesta</li> <li>LinkedIn: https://www.linkedin.com/in/gualtierotesta</li> <li>GitHub: https://github.com/gualtierotesta</li> <li>Mastodon @gualtierotesta: https://mastodon.uno/@gualtierotesta</li> <li>Goodreads: https://www.goodreads.com/gualtierotesta</li> <li>Codewars: </li> <li>Credly: https://www.credly.com/users/gualtierotesta</li> </ul>"},{"location":"about/#certifications","title":"Certifications","text":"When What 2024.11 Auth0 Consumer Application Operations 2024.11 Auth0 Consumer Application Prototypes 2024.10 Auth0 fundamentals 2024.06 AWS Certified Cloud Practitioner 2024.06 Confluent Fundamentals Accreditation 2024.04 OKTA CIC Presales Accreditation 2023.12 Auth0 Certified Developer 2021.04 Oracle Java SE 11 Developer 2021.01 OKTA Certified Developer 2020.04 Vaadin 14 Developer 2019.04 MongoDB 2019.01 NewRelic 2018.12 LightBend Reactive Microservices 2018.11 LightBend Domain Driven Design 2016.05 Vaadin 7 2013.06 HL7 2008.05 CISCO CCNA 2003.07 RHCE"},{"location":"my-books/","title":"My Books","text":"<p>Javascript. Guida completa  (in Italian)</p> <p>Authors: Alessandra Salvaggio, Gualtiero Testa</p> <p>ISBN: 9788868956318</p> <p>Publisher: Edizioni LSWR</p> <p>Year: 2018</p> <p></p> <p> </p> <p>Flash CS3 e Actionscript 3  (in Italian)</p> <p>Authors: Alessandra Salvaggio, Gualtiero Testa</p> <p>ISBN: 9788882336905</p> <p>Publisher: FAG</p> <p>Year: 2008</p>"},{"location":"articles/","title":"Articles","text":""},{"location":"articles/#java","title":"Java","text":"<ul> <li>The 5 Java logging rules</li> <li>Java EE Schedulers</li> <li>SLF4J correct usage</li> </ul>"},{"location":"articles/#maven","title":"Maven","text":"<ul> <li>Maven Enforcer</li> </ul>"},{"location":"articles/#testing","title":"Testing","text":"<ul> <li>Testing Java classes' immutability</li> <li>POJO and JavaBean testing using the bean-matchers library</li> </ul>"},{"location":"articles/#vaadin","title":"Vaadin","text":"<ul> <li>Vaadin 7 maven dependencies</li> </ul>"},{"location":"articles/java-ee-schedulers/","title":"Java EE Schedulers","text":"<p>Last update: 22 Sep 2018</p> <p>Java EE application servers have native scheduling support and, in most of the applications, there is no need to include external dependencies like the famous Quartz scheduler library.</p> <p>The Java EE 6 Timer Service, available on Java EE 6 and 7 full profile, gives us many options to define the scheduling interval and what happens if we stop and restart the application which contains our scheduler.</p> <p>A Java EE scheduler can be:</p> <ul> <li>persistent: the application server saves the scheduling events when the application is down\u00a0to not lose them</li> <li>automatic: simple scheduler definition, most of the details are handled by the application server</li> <li>programmatic: we have full control of all scheduler parameters.</li> </ul> <p>To decide which is the best option, we should first answer the following questions:</p>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/java-ee-schedulers/#is-it-allowed-to-miss-some-scheduling-events","title":"Is it allowed to miss some scheduling events?","text":"<p>If we stop or restart the application (for example during an update) the scheduler will be stopped and some scheduling events could be lost.</p> <p>The scheduler can be configured to save the missed events and execute them when the application will be up again. The application server uses an internal database (it is usually a Java DB like Derby) to store the missed events.</p> <p>This is a persistent scheduler.</p> <p>Note: the application server will generate all missed events at application (re)start. This burst of events is configurable in frequency and delay. See your application server documentation for the details.</p> <p>We have also the option to not persist the scheduling events which will be lost if the application is not running.</p> <p>In the not persistent case, the scheduler life cycle is the same as the application: it is created at application startup and then destroyed at application shutdown.</p> <p>On the contrary, a persistent scheduler survives the application restarts; it is simply sleeping when the application is not running.</p> <p>How to choose?</p> <p>If the scheduled functionality is business-critical and we cannot afford to miss an event, the persistent scheduler is the way to go.</p> <p>In all other cases, the not persistent scheduler is lighter (no DB is used) and easier to manage (less hurdle when updating the application because there is no burst of scheduling events at application restart; the scheduler is always created new at application start ).</p>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/java-ee-schedulers/#will-the-application-run-in-a-cluster","title":"Will the application run in a cluster?","text":"<p>In a cluster, more than one instance of our application is running (one instance per cluster node) and all instances have their copy of our scheduler.</p> <p>But we need to have just one scheduler running among all cluster nodes otherwise we will have multiple copies of the same event.</p> <p>Every application server has its way to handle the \"multiple scheduler instances\" problem (for example see [link 2] for WebSphere) but, in general, it is required that the scheduler should be persistent when we are using a cluster.</p>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/java-ee-schedulers/#should-the-scheduling-interval-be-programmable-at-production","title":"Should the scheduling interval be programmable at production?","text":"<p>Another important question to be answered: should we be able to change the scheduling after the application has been deployed?</p> <p>If the scheduling parameters (its frequency) are fixed, the automatic scheduler is the best solution because very simple to code: just one annotation (or a few XML lines if you prefer the old way).</p> <p>On the contrary, if the scheduler should be somehow configurable, the best solution is the programmatic scheduler which allows us to define all scheduler parameters during the application startup, reading them from a property file, a DB or any configuration solution we are using.</p> <p>Remember:</p> <ul> <li>the automatic scheduler schedule is defined at build time</li> <li>the programmatic scheduler schedule is defined at the application start time</li> </ul>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/java-ee-schedulers/#automatic-scheduler","title":"Automatic scheduler","text":"<p>It's very easy to define an automatic scheduler:</p> <ol> <li>Create a singleton EJB executed at the startup</li> <li>Create a method that will be invoked at every scheduling event</li> </ol> <p>Note: the complete code can be found in the article project [see link 3].</p> <p>First step:</p> <pre><code>@Startup\n@Singleton\npublic class MyScheduler\n</code></pre> <p>The <code>@javax.ejb.Startup</code> annotation asks the EJB container to create the EJB (and so our scheduler) at application startup.</p> <p>The <code>@javax.ejb.Singleton</code> annotation forces the EJB container to create just one instance.</p> <p>Important: the scheduler is used by the application server (the EJB container); it should be never instantiated by the rest of the application code.</p> <p>Then we need the method which will be invoked at scheduling events:</p> <pre><code>@Schedule(/** scheduling parameters */)\npublic void doSomeThing() {..}\n</code></pre> <p>The method should be public and return void.</p> <p>The <code>@javax.ejb.Schedule</code> annotation defines:</p> <ul> <li>the scheduling interval, in cron format, see link [4]</li> <li>the name of the scheduler (you could have many schedulers in the application)</li> <li>a persistent boolean flag which defines if the scheduler is persistent or not</li> </ul> <p>For example:</p> <pre><code>@Schedule(\n    minute = \"*/15\",\n    hour = \"*\",\n    info = \"15MinScheduler\",\n    persistent = false )\n</code></pre> <p>which defines a non-persistent scheduler that runs every 15 minutes.</p> <p>See <code>AutomaticPersistentScheduler</code> and <code>AutomaticNonPersistentScheduler</code> classes in the article project [link 3] for a complete example.</p> <p>Note: there is also the <code>@Schedules</code> annotation [see link 1] which allows the define multiple @Schedule definitions.</p> <p>It is useful when there are schedule requirements that cannot be expressed in a single cron definition.</p>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/java-ee-schedulers/#programmatic-scheduler","title":"Programmatic scheduler","text":"<p>The programmatic scheduler is more complex to build but it gives us the complete freedom to define the scheduler parameters.</p> <p>We have more steps:</p> <ol> <li>Create a singleton EJB executed at the startup</li> <li>Lookup the TimerService resource</li> <li>Create the scheduler at EJB initialization</li> <li>Create a @Timeout method</li> </ol> <p>The first step is the same as the automatic scheduler:</p> <pre><code>@Startup\n@Singleton\npublic class MyScheduler\n</code></pre> <p>Then (second step) we need to lookup the application server timer service but the injection helps us:</p> <pre><code>@Resource\nprivate TimerService timerService;\n</code></pre> <p>At application startup, the EJB container will inject a <code>TimerService</code> instance which allows us to interact with the Timer service. For example, we can list (and even delete) all schedulers defined for the application.</p> <p>In our case, the Timer service will be used to create the new scheduler as follows (third step):</p> <pre><code>String minuteSchedule = \"*/15\";\nString hourSchedule = \"*\";\nScheduleExpression schedule = new ScheduleExpression()\n  .minute(minuteSchedule)\n  .hour(hourSchedule);\n</code></pre> <p>The <code>javax.ejb.ScheduleExpression</code> defines the cron [see link 4] schedule like the <code>@Schedule</code> annotation.</p> <p>The very important difference between <code>@Schedule</code> and <code>ScheduleExpression</code> is that the first one is fixed at build time: to change the schedule parameters (for example, from every 15min to every 30min) we need to change the class code and build and deploy again the application.</p> <p>In the latter case (SchedulerExpression), the schedule parameters (in the example above the variables minuteSchedule and hourSchedule ) can be defined and changed at application startup, reading the minuteSchedule and hourSchedule from, for example, a property file or a connected DBMS.</p> <pre><code>TimerConfig timerConfig = new TimerConfig();\ntimerConfig.setInfo(\"ProgrammaticPersistentScheduler\");\ntimerConfig.setPersistent(true);\n</code></pre> <p>The <code>javax.ejb.TimerConfig</code> gives us the option to define the name of the scheduler (<code>setInfo(String)</code>) and if it is persistent or not ( <code>setPersistent(boolean)</code> ) .</p> <p>Using the <code>ScheduleExpression</code> and the <code>TimerConfig</code> instance, we can use the Timer service to create the scheduler ( a calendar timer, to be more precise).</p> <pre><code>timerService.createCalendarTimer(schedule, timerConfig);\n</code></pre> <p>The <code>createCalendarTime()</code> method returns a <code>javax.ejb.</code>Timer` instance can be used to interrogate the timer like when the next future event will happen or even destroy the scheduler.</p> <p>The last step is to define a method in the class which will be invoked at every scheduling event</p> <pre><code>@Timeout\npublic void doSomeThing() {..}\n</code></pre> <p>The method should be public and return void.</p> <p>And we have our scheduler up and running.</p>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/java-ee-schedulers/#conclusions","title":"Conclusions","text":"<p>Java EE standard gives us many options to define a scheduler that runs our code in a periodical and repetitive way. There is no need for additional project dependencies.</p>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/java-ee-schedulers/#links","title":"Links","text":"<ol> <li>Oracle Java EE6 Tutorial on the Timer Service API</li> <li>IBM WebSphere 8.x Creating timers using the EJB timer service for enterprise beans</li> <li>Article project on GitHub</li> <li>Cron on Wikipedia</li> </ol>","tags":["@Schedule","@Singleton","@Startup","@Timeout","calendar","cron","github","java","javaee","quartz","scheduler","timer service","websphere"]},{"location":"articles/maven-enforcer/","title":"Maven enforcer","text":"<p>This article is obsolete. You can find an updated and expanded version on Medium.</p> <p>Last update: 16 Dec 2018</p> <p>The Maven Enforcer Plugin lets us define and, if required, enforce some environmental conditions like the:</p> <ul> <li>the Java version</li> <li>the Maven version</li> <li>the operating system type (windows, linux\u2026)</li> </ul> <p>If an environmental condition is enforced, the project build will fail if that condition is not respected.</p> <p>In this post, we will focus on enforcing the Maven tool version but let\u2019s begin with why we should enforce the Maven version.</p> <p>Every Maven release has linked, in its super pom, a specific set of plugin versions; changing the Maven version will also change the plugin versions unless the plugin is specified in the build\u2192 plugins or in the build \u2192 pluginManagement sections in the pom.xml files.</p> <p>In the Maven build process, several key plugins are involved: clean, compile, assembly, jar, war, ear, surefire\u2026 A different plugin version can potentially bring to different results.</p> <p>If we are working in a team, with several developers with their own PC and local development environment, and/or if we are using a CI/CD pipeline, we can have:</p> <ol> <li>Developer A use Maven version x to build the project</li> <li>Developer B or the CI tool like Jenkins uses Maven version y to build the project</li> </ol> <p>The two build outputs could be different because the Maven versions x and y could use a different version of the plugins involved in the build (if not explicitly set in the pom.xml file).</p> <p>If we want to avoid this \u201chidden\u201d dependency between the Maven version and the build results we can:</p> <ol> <li>Specify the version of the most common/important plugins in the pom.xml file</li> <li>Enforce the Maven tool version to be used to build the project; in our example, we could say: that everybody should use version y (or x).</li> </ol> <p>How to enforce the Maven release? By using the Maven Enforcer Plugin add the following lines in the build \u2192 plugins section of the pom.xml:</p> pom.xml fragment<pre><code>&lt;build&gt;\n    &lt;plugins&gt;\n        &lt;plugin&gt;\n            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n            &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt;\n            &lt;version&gt;3.1.0&lt;/version&gt;\n            &lt;executions&gt;\n                &lt;execution&gt;\n                    &lt;id&gt;enforce-maven&lt;/id&gt;\n                    &lt;goals&gt;\n                        &lt;goal&gt;enforce&lt;/goal&gt;\n                    &lt;/goals&gt;\n                    &lt;configuration&gt;\n                        &lt;rules&gt;\n                            &lt;requireMavenVersion&gt;\n                                &lt;version&gt;[3.6.1,)&lt;/version&gt;\n                            &lt;/requireMavenVersion&gt;\n                        &lt;/rules&gt;\n                    &lt;/configuration&gt;\n                &lt;/execution&gt;\n            &lt;/executions&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/build&gt;\n</code></pre> <p>In this example, the <code>requireMavenVersion</code> rule has been specified. Within this rule, the Maven release can be specified as:</p> <p><code>&lt;version&gt;3.8.6&lt;/version&gt;</code> : only version 3.8.6 can be used</p> <p><code>&lt;version&gt;[3.6.1,)&lt;/version&gt;</code> : minimum accepted version is 3.6.1; 3.8.4 and 3.8.0 are ok, 3.5.1 is not valid</p> <p><code>&lt;version&gt;3.5&lt;/version&gt;</code> : accepted versions are any 3.5.x</p> <p>Another interesting rule is the <code>requirePluginVersions</code> which can help us to check a common Maven best practice: Maven plugins version should be explicitly defined in the pom.xml, to prevent build results variation when a different Maven tool release is used.</p> <p>An example of the <code>requirePluginVersions</code> rule is the following:</p> pom.xml fragment<pre><code>&lt;requirePluginVersions&gt;\n    &lt;message&gt;Best Practice is to always define plugin versions!&lt;/message&gt;\n    &lt;banLatest&gt;true&lt;/banLatest&gt;\n    &lt;banSnapshots&gt;true&lt;/banSnapshots&gt;\n    &lt;phases&gt;[ERROR] clean,deploy,site&lt;/phases&gt;\n&lt;/requirePluginVersions&gt;\n</code></pre> <p>which forces the best practice to always define the version of the Maven plugins used in the project. The build will fail if the plugin version is not explicitly defined or it is a snapshot (<code>banSnapshots</code> tag) or latest (<code>banLatest</code> tag) version.</p> <p>See here for all details about this rule configuration.</p> <p>More usages can be found on the plugin site.</p>","tags":["coding","java","maven","plugins","versions","pom.xml","super pom","maven-enforcer-plugin"]},{"location":"articles/slf4j-correct-usage/","title":"SLF4J correct usage","text":"<p>Last update: 22 Set 2018</p> <p>SLF4J is a very popular logging facade but, like all libraries we use, there is a chance that we use it in a wrong or at least in a not optimal way.</p> <p>In this tutorial, we will list common logging errors and how we can detect them using FindBugs. We will also mention PMD and Sonar Squid checks when relevant.</p> <p>We will use two external FindBugs plugins which add logging detectors to FindBugs.</p> <p>The first one is a SLF4J only plugin by Kengo Toda which contains SLF4J detectors only.</p> <p>The second plugin is the popular FB Contrib which contains, among many others, some logging detectors.</p> <p>For how to use FindBugs plugins, please refer to the following posts:</p> <ul> <li>Using FindBugs with Maven</li> <li>NetBeans FindBugs plugin</li> </ul> <p>Note: in all examples, we will assume the following imports:</p> <pre><code>import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n</code></pre>","tags":["coding","exception","findbugs","spotbugs","java","logging","plugin","slf4j"]},{"location":"articles/slf4j-correct-usage/#1-logger-definition","title":"1. Logger definition","text":"<p>Wrong way:</p> <pre><code>W1a. Logger log = LoggerFactory.getLogger(MyClass.class);\nW1b. private Logger logger = LoggerFactory.getLogger(MyClass.class);\nW1c. static Logger LOGGER = LoggerFactory.getLogger(AnotherClass.class);\n</code></pre> <p>Correct way:</p> <pre><code>C1a. private static final Logger LOGGER = LoggerFactory.getLogger(MyClass.class);\nC1b. private final Logger logger = LoggerFactory.getLogger(getClass());\n</code></pre> <p>General rule: the logger should be final and private because there are no reasons to share it with other classes or to re-assign it.</p> <p>On the contrary, there is no general agreement if the logger should be static or not. SLF4J plugin favors non static version (C1b) while PMD (\u201cLoggerIsNotStaticFinal\u201d rule) and Sonar (squid rule S1312) prefer a static logger (C1a) so both options should be considered valid.</p> <p>Additional info:</p> <ul> <li>SLF4J FAQ</li> <li>Apache Commons Static Log.</li> </ul> <p>Please note that</p> <ul> <li> <p>in the static version (C1a), the logger name is usually in uppercase characters as all constant fields. If not, PMD will report a \u201cVariableNamingConventions\u201d violation.</p> </li> <li> <p>in both cases, the suggested name is \u201clogger/LOGGER\u201d and not \u201clog/LOG\u201d because some naming conventions avoid too short names (less than four characters). Moreover, log is the verb, more suited for a method name.</p> </li> <li> <p>the W1c is wrong because we are referring to a class (AnotherClass) that is not the class where the logger is defined. In 99% of the cases, this is due to a copy &amp; paste from one class to another.</p> </li> </ul> <p>Related FindBugs (SLF4J plugin) checks:</p> <ul> <li>SLF4J_LOGGER_SHOULD_BE_PRIVATE</li> <li>SLF4J_LOGGER_SHOULD_BE_NON_STATIC</li> <li>SLF4J_LOGGER_SHOULD_BE_FINAL</li> <li>SLF4J_ILLEGAL_PASSED_CLASS</li> </ul>","tags":["coding","exception","findbugs","spotbugs","java","logging","plugin","slf4j"]},{"location":"articles/slf4j-correct-usage/#2-format-string","title":"2. Format string","text":"<p>Wrong way:</p> <pre><code>W2a. LOGGER.info(\"Obj=\" + myObj);\nW2b. LOGGER.info(String.format(\u201cObj=%s\u201d, myObj));\n</code></pre> <p>Correct way:</p> <pre><code>C2. LOGGER.info(\"Obj={}\",myObj);\n</code></pre> <p>General rule: the format string (the first argument) should be constant, without any string concatenation. Dynamic contents (the myObj value in the example) should be added using the placeholders (the '{}' ).</p> <p>Motivation is simple: we should delay logging message creation after the logger has established if the message should be logged or not, depending on the current logging level. If we use string concatenation, the message is built anyway, regardless of the logging level which is a waste of CPU and memory resources.</p> <p>Related FindBugs (SLF4J plugin) checks:</p> <ul> <li>SLF4J_FORMAT_SHOULD_BE_CONST Format should be constant</li> <li>SLF4J_SIGN_ONLY_FORMAT Format string should not contain placeholders only</li> </ul> <p>Related FindBugs (FB Contrib plugin) checks:</p> <ul> <li>LO_APPENDED_STRING_IN_FORMAT_STRING Method passes a concatenated string to SLF4J's format string</li> </ul>","tags":["coding","exception","findbugs","spotbugs","java","logging","plugin","slf4j"]},{"location":"articles/slf4j-correct-usage/#3-placeholder-arguments","title":"3. Placeholder arguments","text":"<p>Wrong way:</p> <pre><code>W3a. LOGGER.info(\"Obj={}\",myObj.getSomeBigField());\nW3b. LOGGER.info(\"Obj={}\",myObj.toString());\nW3c. LOGGER.info(\"Obj={}\",myObj, anotherObj);\nW3d. LOGGER.info(\"Obj={} another={}\",myObj);\n</code></pre> <p>Correct way:</p> <pre><code>C3a. LOGGER.info(\"Obj={}\",myObj);\nC3b. LOGGER.info(\"Obj={}\",myObj.log());\n</code></pre> <p>General rule: the placeholder should be an object (C3a), not a method return value (W3a) to post-pone its evaluation after logging level analysis (see the previous paragraph). In the W3a example, the method getSomeBigField() will be always called, regardless of the logging level. For the same reason, we should avoid W3b which is semantically equivalent to C3a but it always incurs in the toString() method invocation.</p> <p>Solutions W3c and W3d are wrong because the number of placeholders in the format string does not match the number of placeholders' arguments.</p> <p>Solution C3b could be somehow misleading because it includes a method invocation but it could be useful whenever the myObj contains several fields (for example it is a big JPA entity) but we do not want to log all its contents.</p> <p>For example, let's consider the following class:</p> <pre><code>public class Person {\n    private String id;\n    private String name;\n    private String fullName;\n    private Date birthDate;\n    private Object address;\n    private Map&lt;String, String&gt; attributes;\n    private List phoneNumbers;\n</code></pre> <p>its toString() method will most probably include all fields. Using solution C3a, all their values will be printed in the log file.</p> <p>If you do not need all this data, it is useful to define a helper method like the following:</p> <pre><code>public String log() {\n    return String.format(\"Person: id=%s name=%s\", this.id, this.name);\n}\n</code></pre> <p>which prints relevant information only. This solution is also CPU and memory lighter than toString().</p> <p>What is relevant? It depends on the application and on the object type. For a JPA entity, I usually include in the log() method the ID field (in order to let me find the record in the DB if I need all columns data) and, maybe, one or two important fields.</p> <p>For no reason, password fields and/or sensitive info (phone numbers,...) should be logged. This is an additional reason to not log in using toString().</p> <p>Related FindBugs (SLF4J plugin) checks:</p> <ul> <li>SLF4J_PLACE_HOLDER_MISMATCH</li> </ul>","tags":["coding","exception","findbugs","spotbugs","java","logging","plugin","slf4j"]},{"location":"articles/slf4j-correct-usage/#4-debug-messages","title":"4. Debug messages","text":"<p>IMPORTANT: rule #4 (see 5 rules article) guide us to use a guarded debug logging</p> <pre><code>if (LOGGER.isDebugEnabled()) {\n    LOGGER.debug(\u201cObj={}\u201d, myObj);\n}\n</code></pre> <p>Using SLF4J, if the placeholder argument is an object reference (see solutions C3a/C3b), we can avoid the if to keep the code cleaner.</p> <p>So it is safe to use the following:</p> <pre><code>LOGGER.debug(\u201cObj={}\u201d, myObj);\n</code></pre>","tags":["coding","exception","findbugs","spotbugs","java","logging","plugin","slf4j"]},{"location":"articles/slf4j-correct-usage/#5-exceptions","title":"5. Exceptions","text":"<p>Proper exceptions logging is an important support for problem analysis but it is easy to neglect its usefulness.</p> <p>Wrong way:</p> <pre><code>W5a. catch (SomeException ex) { LOGGER.error(ex); }..\nW5b. catch (SomeException ex) { LOGGER.error(\"Error:\" + ex.getMessage()); }..\n</code></pre> <p>Correct way:</p> <pre><code>C5. catch (SomeException ex) { LOGGER.error(\"Read operation failed: id={}\", idRecord, ex); }..`\n</code></pre> <p>General rules:</p> <ol> <li> <p>Do not remove the stack trace information by using getMessage() (see W5b) and not the complete exception. The stack trace often includes the real cause of the problem which is easily another exception raised by the underlying code. Logging only the message will prevent us to discover the real cause of the problem.</p> </li> <li> <p>Do show significant (for the human who will analyze the log file) information in the logging message showing a text explaining what we wanted to perform while the exception was raised (not the exception kind or messages like \"error\": we know already something bad happened). What we need to know is what we were doing and on which data.</p> </li> </ol> <p>The C5 example tells us we were trying to read the record with a specific ID whose value has been written in the log with the message.</p> <p>Please note that C5 uses one placeholder in the format string but there are two additional arguments. This is not an error but a special pattern that is recognized by SLF4J as an exception logging case: the last argument (ex in the C5 example) is considered by SLF4J as a Throwable (exception) so it should be not included in the format string.</p> <p>Related FindBugs (SLF4J plugin) checks:</p> <ul> <li>SLF4J_MANUALLY_PROVIDED_MESSAGE: the message should not be based on Exception getMessage()</li> </ul>","tags":["coding","exception","findbugs","spotbugs","java","logging","plugin","slf4j"]},{"location":"articles/vaadin-7-maven-dependencies/","title":"Vaadin 7 maven dependencies","text":"<p>Last update: 17 Sep 2018</p> <p>The Vaadin framework has several dependencies but not all of them should be included in our war/ear artifacts.</p> <p>The following table shows all Vaadin version 7.6/7.7 main modules and their meaning and usage</p> Module Description and usage server This is the core of the framework. It has the following (transitive) dependencies: vaadin-shared and vaadin-sass-compiler themes Compiled version of the standard Vaadin themes client-compiled Compiled version of the standard Vaadin widgets set client Vaadin and GWT classes for widgets client-compiler Widgets compiler based on GWT Google Web Toolkit push Optional module. It includes the support for push protocols (server to client) thanks to the Atmosphere framework shared Common modules code. It is included as dependency in the server module sass-compiler SASS to CSS compiler, used at build time and at run-time (\"on-the-fly\" compilation). It is included as dependency in the server module <p>Depending on the project requirements, the above modules should be included or not as project dependencies. We can identify two possible scenarios:</p> <ol> <li>A project without a custom widget set. It can have a custom theme</li> <li>A project with a custom widget set</li> </ol> <p>In the first case (without a custom widget set) we need the following modules:</p> <ul> <li>server</li> <li>themes</li> <li>push (optional)</li> <li>client-compiled</li> </ul> <p>while, if we have a custom widget set, we need to compile the widgets so the dependencies become:</p> <ul> <li>server</li> <li>themes</li> <li>push (optional)</li> <li>client (for build only)</li> <li>client-compiler (for build only)</li> </ul> <p>Note: the compiled custom widgets are included in our artifact</p> <p>The following table summarizes the Maven dependencies:</p> Module ArtifactId Scope Required? server vaadin-server compile yes themes vaadin-themes compile yes client-compiled vaadin-client-compiled runtime only if the project does not use custom widget set client vaadin-client provided only with custom widget set client-compiler vaadin-client-compiler provided only with custom widget sett. See also note below. push vaadin-push compile optional shared vaadin-shared - vaadin-server dependency. No need to be specified in the pom.xml sass-compiler vaadin-sass-compiler - vaadin-server dependency. No need to be specified in the pom.xml <p>Note: the <code>vaadin-client-compiler</code> dependency is automatically included in the classpath by the Vaadin Maven plugin (<code>vaadin-maven-plugin</code>) when the custom widgets set should be compiled.</p>","tags":["atmosphere","coding","dependency","gwt","java","maven","plugin","vaadin","vaadin7","vaadin compiler"]},{"location":"articles/java/logging-rules/","title":"The 5 Java logging rules","text":"<p>Last update: 08 Sep 2022</p> <p>Logging is a critical factor that should be always kept into account during software development.</p> <p>When something bad happens in production, the log files are usually the starting point of our fault analysis. And, often, they are the only information in our hands to understand what is happening and which is the root cause of the problem.</p> <p>It is so very important to have the required information logged properly.</p> <p>The following five logging rules are a way to check and, possibly, improve how we handle the logging in our code.</p> <p>Please note that we will not discuss how to configure a logging engine nor we will compare them to each other.</p>","tags":["java","coding","logging","rules","log levels","slf4j","jul","production"]},{"location":"articles/java/logging-rules/#rule-1-logging-is-for-readers","title":"Rule 1. Logging is for readers","text":"<p>The logging messages should be meaningful to who will read the log files, not only to who wrote the (logging) code.</p> <p>It seems a very obvious rule but it is often violated.</p> <p>For example, let's consider a log message like the following</p> <pre><code>ERROR: Save failure - SQLException stack trace .....\n</code></pre> <p>Saving what? This message could mean something to the developer but it is completely useless for the poor guy which is looking at the production problem.</p> <p>A much better message is</p> <pre><code>ERROR: Save failure - Entity=Person, Data=[id=123 surname=\"Mario\"] - SQLException stack trace....\n</code></pre> <p>which explains what you wanted to save (here a Person, a JPA entity) and the relevant contents of the Person instance. Please note the word <code>relevant</code>, instead of the word <code>all</code>: we should not clutter log files with useless info like the complete print of all entity fields. </p> <p>Entity name and its logical keys are usually enough to identify a record in a table.</p>","tags":["java","coding","logging","rules","log levels","slf4j","jul","production"]},{"location":"articles/java/logging-rules/#rule-2-match-the-logging-levels-with-the-execution-environment","title":"Rule 2. Match the logging levels with the execution environment","text":"<p>All logging facades and engines available in the Java ecosystem have the concept of logging level (ERROR, INFO...), with the possibility to filter out messages with a too low level.</p> <p>For example, the Java Util Logging (JUL) engine that is included in the JDK, uses the following levels: SEVERE, WARN, INFO, FINE, FINER, FINEST (+ CONFIG and OFF).</p> <p>On the contrary, the two most popular logging facade, Apache Commons Logging and SLF4J, prefer the following levels: FATAL, ERROR, WARN, INFO, DEBUG, and TRACE.</p> <p>Logging level filtering should depend on which stage of the development is your code: logging level in the production should not be the same as in test/integrations environments.</p> <p>Moreover, the logging level should also depend on the code owner. In general, our application code should have more detailed logging compared to any third-party library we are using. There is usually no big meaning to see, for example, Google Guava library debug messages in our log files.</p> <p>I usually configure the logging as follows:</p> <ul> <li>Production: INFO level for my code and WARN for third-party libraries.</li> <li>Test/Integration: DEBUG level for my code and WARN (or INFO if needed) for third-party libraries.</li> <li>Development: whatever makes sense</li> </ul> <p>You should choose the logging strategy that better fit your requirements. </p> <p>Note: many application servers allow to change the logging configuration at runtime. In a production environment, you can temporarily enable DEBUG messages during a debug session and disable them when the session is finished.</p> <p>I discourage the use of the TRACE/FINEST level (and I'm not alone, see for example here). I don't see a big difference between DEBUG and TRACE and it is usually difficult for the young members of the team to decide which one, DEBUG or TRACE, to use. </p> <p>Following the Kiss principle, I suggest using ERROR, WARN, INFO and DEBUG levels only.</p>","tags":["java","coding","logging","rules","log levels","slf4j","jul","production"]},{"location":"articles/java/logging-rules/#rule-3-remove-coding-help-logging-before-the-commit","title":"Rule 3. Remove coding help logging before the commit.","text":"<p>While coding, we usually add logging messages, using the logger or the System.out, in our code for a better understanding of what it is happening in our application during execution /debugging sessions.</p> <p>Something like:</p> <pre><code>    void aMethod(String aParam) {\n        LOGGER.debug(\u201cEnter in aMethod\u201d);\n        if (\u201cno\u201d.equals(aParam)) {\n           LOGGER.debug(\u201cUser says no\u201d);\n          \u2026.\n</code></pre> <p>The main purpose of these messages is to trace application behaviour by showing which method is invoked and by dumping internal variables and method parameter values. Quite popular among non TDD devotes.</p> <p>Unfortunately, these messages do not have usually a big meaning once the code has been released (to test and then production).</p> <p>So this rule simply says: once you have finished developing, remove all temporary and unnecessary logging messages just before committing the code to the SCM system (git, svn..) in use.</p> <p>This rule does not require removing all DEBUG messages but only the ones that do not have meaning once the application is completed and released; in other words when we are reasonably sure that the application is working properly.</p>","tags":["java","coding","logging","rules","log levels","slf4j","jul","production"]},{"location":"articles/java/logging-rules/#rule-4-check-log-level-before-logging-debug-messages","title":"Rule 4: Check log level before logging DEBUG messages","text":"<p>According to Rule 2, in the production log files, we will show ERROR, WARN and INFO messages only but in our code, we can have many DEBUG messages that should not affect production execution.</p> <p>Every time you want to log a DEBUG message (all the ones which remain after rule 3), add in front a check if DEBUG logging is enabled:</p> <pre><code>    if ( LOGGER.isDebugEnabled() ) {\n        LOGGER.debug (\u2026\u2026.)\n    }\n</code></pre> <p>This will prevent your code to build the log messages and call the logger. It is for efficiency in the program execution at production.</p>","tags":["java","coding","logging","rules","log levels","slf4j","jul","production"]},{"location":"articles/java/logging-rules/#rule-5-know-your-logger","title":"Rule 5: Know your logger","text":"<p>How we use the logger methods can have a significant cost:</p> <ul> <li>To build the message string</li> <li>to collect the data to be included in the message string</li> </ul> <p>We should review the JavaDoc of the selected logging facade/engine and understand the most efficient way to use its logger.</p> <p>For example, we could create a message like this:</p> <pre><code>    LOGGER.info(\u201cPerson name is \u201c + person.getName());\n</code></pre> <p>which creates a few unnecessary string instances.</p> <p>Using SLF4J, the correct use is :</p> <pre><code>    LOGGER.info(\u201cPerson name is {}\u201c, person.getName());\n</code></pre> <p>where the format string is constant and the final message is built only if logging is enabled. See here for more details.</p>","tags":["java","coding","logging","rules","log levels","slf4j","jul","production"]},{"location":"articles/testing/bean-matchers/","title":"POJO and JavaBean testing using Bean-matchers","text":"<p>Last update: 04 Sep 2022</p> <p>Project repo</p> <p>The first reaction to the question \u201cShould I unit test a data class?\u201d  is usually a big NO because there is no meaning to checking a data class that contains boilerplate code (getter, setter, equals..) often generated using the  IDE or by libraries like Lombok and Immutables.</p> <p>Real-world projects are, unfortunately, not so perfect and there are cases where unit testing a POJO or a JavaBean could make sense. Let me list some situations I have encountered.</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/bean-matchers/#security-sensitive-fields","title":"Security sensitive fields","text":"<p>If the data class contains a security-sensitive field like a password or an API token, this field should NOT be logged.</p> <p>See what OWASP has to say about this problem: Logging Cheat Sheet.</p> <p>For this reason, the class should not include sensitive fields in the toString method.</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/bean-matchers/#logging-unfriendly-fields","title":"Logging unfriendly fields","text":"<p>The data class can contain fields that are binary data or very long strings. In this case, we should exclude these fields from the toString method to prevent them to bloat our logging files.</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/bean-matchers/#non-standard-equals-and-hashcode-methods","title":"Non-standard equals and hashCode methods","text":"<p>The standard way to implement an equals method is to compare, one by one, all fields in the class. Sometimes one field contains a \u201cunique\u201d identifier that makes other field comparisons useless or even dangerous.</p> <p>For example, this unique field can contain the primary key column of the database record from which the POJO data has been extracted. In this case, the POJO equality could or should depend only on its primary key field and the equals (and hashCode) implementation should use just this field.</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/bean-matchers/#no-args-constructor-javabeans","title":"No args constructor (JavaBeans)","text":"<p>The JavaBean standard requires the presence of a no-args constructor, a constructor without parameters.</p> <p>The compiler usually creates the no-args constructor unless there are already other (non no-args) constructors defined in the class.</p> <p>At runtime, the library (for example, the JPA libraries) which rely on the no-args constructor presence will fail due to the missing no-args constructor.</p> <p>One important remark: checking all the above conditions using a unit test is a way to guarantee the correct behaviour of the data class throughout all project life. The class can be correctly implemented when created but, later, one developer, while adding a new field in the class, can wrongly regenerate the toString method, restore the log of a password field, or forget to regenerate the equals/hashCode methods properly.</p> <p>Checking data class quality is also useful in legacy projects to detect improper class definitions because the unit class can be added aside from the \u201cmain\u201d classes, without disturbing the legacy code.</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/bean-matchers/#the-bean-matchers-library","title":"The Bean-matchers library","text":"<p>The Bean-matchers library, created by Orien Madgwick, lets us test the class conditions to guarantee that all future changes in the class will not break them.</p> <p>Let\u2019s assume we have the following data class:</p> <pre><code>class BasicBean {\n  private int id;\n  private String string;\n  private char[] password;\n  private Long[] longArray;\n  private String veryLongString;\n\n  public BasicBean(final int id) {\n    this.id = id;\n  }\n\n   // getter, setter, equals, hashCode, toString\n</code></pre> <p>Conditions we want to assure:</p> <ul> <li>the id field should be used to check equality and to generate the class hash</li> <li>the password field should not be logged because security sensible</li> <li>the veryLongString field should not be logged to avoid logging files bloating</li> <li>the class should have a no-args constructor to be used with JPA.</li> </ul> <p>Using the Bean-matchers library, the test class can be:</p> <pre><code>@Test\npublic void testTheClassIsGoodJavaBean() {\n  MatcherAssert.assertThat(BasicBean.class,\n    CoreMatchers.allOf(\n      // This is Java Bean so we want an empty constructor\n      BeanMatchers.hasValidBeanConstructor(),\n      // All fields should have getter and setter\n      BeanMatchers.hasValidGettersAndSetters(),\n      // Only the 'id' field in hashcode and equals\n      BeanMatchers.hasValidBeanHashCodeFor(\"id\"),\n      BeanMatchers.hasValidBeanEqualsFor(\"id\"),\n      // Password and veryLongString fields should not \n      // be included in the toString method\n      BeanMatchers.hasValidBeanToStringExcluding(\n          \"password\", \"veryLongString\")\n    ));\n}\n</code></pre> <p>The Beans-matchers library creates an instance of our BasicBean class and checks if all conditions we have defined on</p> <ul> <li>the constructor</li> <li>the getters and setters,</li> <li>the equals method</li> <li>the hashCode method</li> <li>the toString method</li> </ul> <p>are valid otherwise, the test fails.</p> <p>A test failure will alert us whenever a change in the BasicBean class breaks the above conditions. It is a kind of safety net.</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/bean-matchers/#for-lombok-users","title":"For Lombok users","text":"<p>Using Lombok, we can implement our bean conditions using Lombok annotations. The following code is the equivalent version of the plain Java version reported above:</p> <pre><code>@Data\n@NoArgsConstructor\n@EqualsAndHashCode(of = \"id\")\n@ToString(exclude = {\"password\", \"veryLongString\"})\npublic class LombokBean {\n  private int id;\n  private String string;\n  private char[] password;\n  private Long[] longArray;\n  private String veryLongString;\n\n  public LombokBean(final int id) {\n    this.id = id;\n  }\n}\n</code></pre> <p>When using Lombok, I believe there is no need to have a unit test to check our bean conditions because, in general, we should not test third-party libraries (Lombok in this case), but having it will not damage your project ;-)</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/bean-matchers/#final-remarks","title":"Final remarks","text":"<p>Another possible benefit of testing data classes, especially in legacy projects, is that their code coverage is easily 100%. This should not be the main purpose (remember, there is no meaning to test getter and setter) but a side effect which I\u2019ve found useful: having the data classes at 100%, the low test coverage is due to the logic classes, the classes which contain the code which implement the application logic. They should be the main target of any test!</p>","tags":["code checks","coding","data class","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"articles/testing/java-classes-immutability/","title":"Test Java classes' immutability","text":"<p>Last update: 17 Sep 2018</p> <p>In all our project, we use <code>data classes</code> which, by definition, contain data (fields) but no (business) logic.</p> <p>According to the best coding practices, a data class should preferably be <code>immutable</code> because immutability means thread safety. Main reference here is Joshua Bloch's Effective Java book; this Yegor Bugayenko's post is also very interesting reading.</p> <p>An immutable class has several interesting properties:</p> <ul> <li> <p>it should be not sub-classable (i.e. it should be final or it should have a static factory method and a private constructor)</p> </li> <li> <p>all fields should be private (to prevent direct access)</p> </li> <li> <p>all fields should be written once (at instance creation time) (i.e. they should be final and without setters)</p> </li> <li> <p>all mutable type (like java.util.Date) fields should be protected to prevent client write access by reference</p> </li> </ul> <p>An example of immutable class is the following:</p> <pre><code>public final class ImmutableBean {\n      private final String aStr;\n      private final int anInt;\n      public ImmutableBean(String aStr, int anInt) {\n        this.aStr = aStr;\n        this.anInt = anInt;\n      }\n      public String getAStr() {\n        return aStr;\n      }\n      public int getAnInt() {\n        return anInt;\n      }\n}\n</code></pre> <p>Note: as frequent in Java, there is a lot of boilerplate code which hides the immutability definitions.</p> <p>Libraries like Project Lombok makes our life easier because we can use the <code>@Value</code> annotation to easily define an immutable class as follows:</p> <pre><code>@Value\npublic class LombokImmutableBean {\n    String aStr;\n    int anInt;\n}\n</code></pre> <p>which is a lot more more readable.</p> <p>Should we (unit) test a class to check its immutability?</p> <p>In a perfect world, the answer is no.</p> <p>With the help of our preferred IDE automatic code generation features or with libraries like Lombok it is not difficult to add immutability to a class.</p> <p>But in a real world, human errors can be happen, when we create the class or when we (or may be a junior member of the team) modify the class later on. What happen if a new field is added without final and a setter is generated by using IDE code generator? The class is no more immutable.</p> <p>It is important to guarantee that the class is and remains immutable along all project lifetime.</p> <p>And with the help of the Mutability Detector we can easily create a test to check the immutability status of a class.</p> <p>As usual, Maven/Gradle dependencies can be found on Maven Central.</p> <p>To test our ImmutableBean we can create the following jUnit test class:</p> <pre><code>import static org.mutabilitydetector.unittesting.MutabilityAssert.assertImmutable;\n\npublic class ImmutableBeanTest {\n\n    @Test\n    public void testClassIsImmutable() {\n    assertImmutable(ImmutableBean.class);\n    }\n}\n</code></pre> <p>the test will fail if the class is not immutable.</p> <p>For example, if a field is not final and it has a setter method, the test fails and the error message is very descriptive:</p> <pre><code>org.mutabilitydetector.unittesting.MutabilityAssertionError: \nExpected: it.gualtierotesta.testsolutions.general.beans.ImmutableBean to be IMMUTABLE\n    but: it.gualtierotesta.testsolutions.general.beans.ImmutableBean is actually NOT_IMMUTABLE\nReasons:\n    Field is not final, if shared across threads the Java Memory Model will not guarantee it is initialised before it is read. \n        [Field: aStr, Class: it.gualtierotesta.testsolutions.general.beans.ImmutableBean]\n    Field [aStr] can be reassigned within method [setaStr] \n        [Field: aStr, Class: it.gualtierotesta.testsolutions.general.beans.ImmutableBean]\n</code></pre> <p>The complete project can be found on my Test Solutions gallery project on GitHub. See module general.</p> <p>The approach I suggest is to use Lombok without any immutability test. If Lombok cannot be used (for example in a legacy project), use the Mutability Detector to assert that the class is immutable.</p>","tags":["code checks","coding","data class","gradle","immutable","java","junit","maven","quality checks","unit test","unit testing"]},{"location":"books/","title":"Book reviews","text":"<ul> <li>Data-Oriented Programming - Reduce software complexity</li> <li>DevOps with Kubernetes (1st ed.)</li> <li>Get your hands dirty on clean architecture</li> <li>Groovy in Action</li> <li>GO Programming blueprints</li> <li>Learning IBM Bluemix</li> <li>Learning DevOps</li> <li>The Pragmatic Programmer, 20th Anniversary Edition</li> </ul>"},{"location":"books/bluemix-learning/","title":"Learning IBM Bluemix","text":"<p>Last update: 04 Sep 2022</p> <p></p> <p>Learning IBM Bluemix</p> <p>Author: Sreelatha Sankaranarayanan</p> <p>ISBN: 9781785887741</p> <p>Publisher: Packt Publishing Ltd.</p> <p>Year: 2016</p> <p></p> <p>This book is a showcase, with working examples, of what we can do with IBM Bluemix cloud environment and its integrated IBM and third-party services.  </p> <p>The main focus is to show what can be done, not how it works. Most of the technical backgrounds are left to several links to external resources on the web.</p> <p>The first step is how to create an application using Bluemix templates (\"boilerplates\").</p> <p>The following chapters are on how to integrate the application to the available services like security, SQL and NoSQL databases and IBM Watson functions, how to use the Bluemix development environment (git repository, continuous integration and deployment, test..) and how to monitor and tune application performances.  </p> <p>Other chapters are devoted to hybrid (cloud and on-premise) solutions and mobile applications.  </p> <p>Most of the examples are based on Node.js or Java using IBM Liberty application server.  </p> <p>A couple of warnings:  </p> <ul> <li>Bluemix IaaS (Infrastructure as-a-service) features like Docker and Open Stack virtual machines support are mentioned but not described. The book's focus is on PaaS (Platform as-a-service) features, based on Pivotal's Cloud Foundry solution.  </li> <li>Bluexmix is a fast-evolving environment so book screenshots and features lists can be easily obsoleted after a few months  </li> </ul>","tags":["bluemix","book review","cloud","cloud foundry","ibm","openstack"]},{"location":"books/data-oriented-programming/","title":"Data Oriented Programming","text":"<p>Last update: 14 Oct 2022</p> <p></p> <p>Data Oriented Programming (Reduce software complexity)</p> <p>Author: Yehonathan Sharvit</p> <p>ISBN: 9781617298578</p> <p>Publisher: Manning</p> <p>Year: 2022</p> <p></p> <p>The data-oriented programming (DOP) is a programming paradigm like the more famous object-oriented programming (OOP) and functional programming (FP) paradigms.</p> <p>DOP adopts some concepts from FP (immutability, pure functions) but its main focus is to reduce the complexity of the applications by streamlining the flow of the data inside the application and exchanged by the application with external resources.</p> <p>For a better introduction to DOP concepts, you can read this article from the book's author.</p> <p>The Clojure language popularised the DOP paradigm but it can be also applied to other languages. The code examples in the book are in JavaScript with the Lodash library. OOP code examples are in Java.</p> <p>Sharvit's book does a good job explaining the DOP approach to the most common development problems, comparing it with the OOP one. The four DOP principles are gradually introduced with a lot of code and simple but effective writing. Dialogues between fictitious characters easy the description of the least simple DOP concepts.</p> <p>The appendixes are very important: Appendix A summarises the DOP concepts while appendix B shows how to implement the DOP paradigm using static-type languages like Java. The remaining two appendixes are on DOP history and the Lodash library.</p> <p>Highly recommended.</p>","tags":["book review","dop","programming","paradigm","data-oriented-programming"]},{"location":"books/devops-with-kubernetes/","title":"DevOps with Kubernetes (First Edition)","text":"<p>Last update: 30 Oct 2022</p> <p></p> <p>DevOps with Kubernetes (Accelerating software delivery with container orchestration)</p> <p>Authors: Hideto Saito, Hui-Chuan Chloe Lee, Cheng-Yang Wu</p> <p>ISBN: 978-1788396646</p> <p>Publisher: Packt Publishing Ltd.</p> <p>Year: 2017</p> <p></p> <p>DevOps and Kubernetes are prevalent topics nowadays.</p> <p>This 2017 book by Packt Publishing wants to explain DevOps using containerised platforms, especially Kubernetes.</p> <p>The first chapter is a very quick tour of the foundation of the DevOps methodology: micro-services, continuous delivery, and infrastructure as a code...</p> <p>The second chapter describes what the containers are and how to use Docker, the dockerfiles and the docker-compose command.</p> <p>The central part of the book is on Kubernetes, its resources, how to deploy and monitor our applications on Kubernetes, and how we configure and manage a Kubernetes cluster.</p> <p>The final chapters are on AWS and Google Cloud offerings and how to implement a Kubernetes cluster on them.</p> <p>The book could have been a good overview of the DevOps approach using Kubernetes but it is affected by one negative aspect: the book's English is not good, there are several grammar errors and the reading is sometimes not easy. This is somehow surprising for a UK-based publisher.</p> <p>Important: there is a second edition of the book, published in 2019 (ISBN 978-1789533996). I expect the new edition will also cover some more recent DevOps concepts like, for example, the GitOps approach.</p>","tags":["book review","devops","cloud","kubernetes","docker","aws","gcp"]},{"location":"books/get-your-hands-dirty-on-clean-architecture/","title":"Get Your Hands Dirty on Clean Architecture","text":"<p>Last update: 04 Sep 2022</p> <p></p> <p>Get Your Hands Dirty on Clean Architecture</p> <p>Author: Tom Hombergs</p> <p>ISBN: 9781839211966</p> <p>Publisher: Packt Publishing Ltd.</p> <p>Year: 2019</p> <p></p> <p>As developers, we organize our application code in classes and packages/modules.\u00a0</p> <p>There are several ways to compose the code. A popular approach is the \"three-tiered\" architecture where the logic is divided into the presentation, business and data layers. See link #1.</p> <p>Alternative architectures are the \"hexagonal\" or \"port/adapters\" (see link #2) and R. Martin's Clean Architecture (link #3).</p> <p>Some books explain architecture's benefits and downsides. Among them, I suggest R. Martin's \"Clean Architecture\" book.\u00a0</p> <p>On the other hand, Tom Hombergs's book complements these theoretical books by showing how to implement the architectures using significant examples in Java.</p> <p>The author explains:</p> <ul> <li>how to organize code in packages</li> <li>how to interface the logic between different application layers</li> <li>how to pass and map data objects through layers</li> </ul> <p>All examples are clear and well-documented.</p> <p>I've personally found quite useful the explanation of the possible data mapping strategies.</p> <p>I highly suggest Tom Hombergs's \"Get your hands dirty on clean architecture\" book.</p> <p>Links:</p> <ol> <li>Three-tier architecture on Wikipedia</li> <li>Hexagonal architecture on Wikipedia</li> <li>Clean Architecture on Robert Martin's blog site</li> </ol>","tags":["book review","architecture"]},{"location":"books/go-programming-blueprints/","title":"Go Programming Blueprints (second edition)","text":"<p>Last update: 04 Sep 2022</p> <p></p> <p>Go Programming Blueprints (second edition)</p> <p>Author: Mat Ryer</p> <p>ISBN: 9781786468949</p> <p>Publisher: Packt Publishing Ltd.</p> <p>Year: 2016</p> <p></p> <p>This book is not a primer on GO Language (here I suggest \u201cThe GO Programming Language\u201d by Donovan &amp; Kernighan, Addison-Wesley) but it should be read just after learning the basic concepts of the language and its toolchain.</p> <p>The author introduces and describes in detail several important concepts about the GO way to program, structure the code and organize the projects.</p> <p>The examples are clear and simple to be easily understood but, at the same time, they can be used in our projects.</p> <p>The first project is a web-based chat application that introduces main web application concepts like HTML templates, requests routing, Websocket and Oauth protocols, JSON, images plus some GO specifics like how to use the channels to handle client-server communication.</p> <p>The second project is a WHOIS client which shows how to interact with a RESTful API and how to create a command line utility in GO.</p> <p>The third and main project is a multi application system that analyze Twitter data streams to count specific tags, using a MongoDB as a storage solution and a messaging system to decouple the applications and exposing a REST API for a web-based client. With this project, the author shows how to integrate a NoSQL database and a Queue Messaging system and how to create a REST API.</p> <p>The book includes other projects with cover additional topics like how to interact with the file system.</p> <p>All projects and their code are well described and, again, it is something we can use in our projects.</p> <p>Highly suggested.</p>","tags":["book review","golang","programming","web development"]},{"location":"books/groovy-in-action/","title":"Groovy in Action (second edition)","text":"<p>Last update: 04 Sep 2022</p> <p></p> <p>Groovy in Action Second Edition</p> <p>Authors: Dierk K\u00f6nig, Paul Kink</p> <p>ISBN: 9781935182443</p> <p>Publisher: Manning Publications Co.</p> <p>Year: 2015</p> <p></p> <p>A must-have for any serious Groovy developer.</p> <p>Update: the book refers to Groovy version 2.x. The latest Groovy version is 4.x. Nevertheless, the book contents are still fully valid.</p> <p>Groovy in Action (Second Edition) is, at the same time, a detailed overview of the language and core libraries' characteristics and an in-depth description of how it works.</p> <p>The first part is dedicated to the language, with the usual list of syntax elements descriptions (operators, data structures, control structures..), including Groovy unique features like being together a dynamic and a static typing language or supporting both object-oriented and functional programming styles, not mentioning the scripting capabilities.</p> <p>The second part is devoted to the Groovy core library: the Groovy Design Kit, how to work with databases and web services and how to handle JSON and XML.</p> <p>The final part is dedicated to unit testing, concurrency and, of course, domain-specific languages, one of the traditional Groovy applications areas.</p> <p>I have found very interesting chapter 16 on how to integrate Groovy in a Java application, using, for example, Groovy as a dynamic business rules engine, and chapter 20 on the Groovy ecosystem (Gradle, Grails..) introduction.</p> <p>The authors show not only a very strong knowledge of the language and its ecosystem but also an understanding of how Groovy fits in real-world applications. Very interesting and useful the adoption of the assertions statements to better explain the code examples.</p> <p>This book, with 900+ pages, is not targeting occasional Groovy users but I think it is a must-have (as an introduction at the beginning, as a reference later) for anybody intended to seriously use Groovy.</p>","tags":["book review","groovy"]},{"location":"books/learning-devops/","title":"Learning DevOps (first edition)","text":"<p>Last update: 10 Dec 2022</p> <p></p> <p>Learning DevOps First Edition</p> <p>Author: Mikael Krief</p> <p>ISBN: 9781838642730</p> <p>Publisher: Packt</p> <p>Year: 2019</p> <p></p> <p>NOTE: the second edition of this book has been published on March 2022 with ISBN 978-1801818964. This review is for the first edition.</p> <p>This book is a nice read and a great resource. Five stars.</p> <p>The devops concepts, tools and methodologies are well explained and introduced smoothly and logically. The author describes, with practical examples and clear explanations, all the topics related to the devops world:</p> <ul> <li>Git ops</li> <li>Infrastructure as a code</li> <li>Pipelines</li> <li>Continuous integration</li> <li>Continuous delivery</li> <li>Testing and quality checks</li> <li>Security</li> <li>Containers</li> </ul> <p>In the different chapters, many tools are introduced and used in simple but realistic examples. </p> <p>The book examples are available on a dedicated repo on GitHub. </p> <p>There are also many videos which complement the book descriptions. They are linked in the book and available on a dedicated playlist on YouTube.</p> <p>The book is not (and it could not be) a reference for all devops tools and methodologies so every chapter ends with a useful list of links to additional resources that help the reader increase the knowledge on the specific topic.</p> <p>This book is highly recommended.</p>","tags":["book review","groovy"]},{"location":"books/pragmatic-programmer-2/","title":"The Pragmatic Programmer (20th Anniversary Edition)","text":"<p>Last update: 13 Mar 2024</p> <p></p> <p>The Pragmatic Programmer (20th Anniversary Edition)</p> <p>Authors: David Thomas, Andrew Hunt</p> <p>ISBN: 978-0-13-595705-9</p> <p>Publisher: Pearson Addison-Wesley</p> <p>Year: 2020</p> <p></p> <p>Twenty years ago, the first edition of this book was fundamental to growing a new generation of software programmers. This new revision has been updated to be aligned with how the software development has evolved. A \"pragmatic\" programmer is a programmer who knows how to code (several chapters are dedicated to coding and software architectures) but is also able to work in a team and with the customer, contributing to the project's success. The book contains several suggestions and tips on how to behave as a human being, a programmer and a member of a project. It is not easy to read because most chapters are dense with concepts, ideas and suggestions, nevertheless, I firmly suggest it to everybody who wants to improve as a professional programmer. My preferred phrase from the book: \"Testing, design, coding - it's all programming\".</p>","tags":["book review","programming"]},{"location":"certifications/","title":"Certifications material studies","text":"<ul> <li>AWS Certified Cloud Practitioner</li> <li>CCDAK Confluent Certified Developer for Apache KAFKA</li> </ul>"},{"location":"certifications/aws-practictioner/","title":"AWS Certified Cloud Practitioner","text":"<p>Last update: 24 Ago 2024</p>","tags":["certification","aws","practitioner","cloud"]},{"location":"certifications/ccdak/","title":"Confluent Certified Developer for Apache KAFKA (CCDAK)","text":"<p>Last update: 09 Oct 2024</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#references","title":"References","text":"<ul> <li>Apache Kafka documentation</li> <li>Confluent documentation</li> <li>Confluent certification training</li> <li>CloudGuru/Pluralsight training</li> <li>Spring Kafka</li> <li>Baeldung Kafka series</li> </ul>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#training","title":"Training","text":"<p>On the Confluence Training platform, suggested (free) courses are:</p> <ul> <li>Apache Kafka Fundamentals</li> <li>Confluent Fundamentals Accreditation</li> <li>Confluent Certified Bootcamp: Developer</li> </ul>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#introduction","title":"Introduction","text":"<p>Apache Kafka is an open source product developed by LinkedIn and now under Apache Foundation.</p> <p>Confluent is a private company that gives commercial/enterprise support and provides additional tools to the Kafka ecosystem.</p> <p>Key concepts:</p> <ul> <li>Events vs static data, flow vs storage. Kafka handle unlimited temporal series of events.</li> <li>Real time handling</li> <li>Events persistence</li> </ul> <p>An event streaming platform has 2 primary uses:</p> <ul> <li>Stream processing: continuous real time data processing. Evolution of the Enterprise Messaging.</li> <li>Data integration: flow of data changes to align systems. A sort of streaming ETL.</li> </ul> <p>Kafka solution is based on several components:</p> <ul> <li>the <code>broker</code>, the Kafka instance in the cluster which has its own storage. Usually there are more than one broker in the cluster, for fault tolerance.</li> <li>the <code>cluster manager</code>, that controls the brokers cluster, handling brokers failures and recoveries</li> <li>the <code>producers</code>, client applications that run outside the cluster and send events to the brokers</li> <li>the <code>consumers</code>, client applications that run outside the cluster and receive (pull) events from the brokers</li> <li>the <code>streams</code>, applications that process events, producing other events</li> </ul> <p>Within Kafka, events are saved in named topics.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#brokers","title":"Brokers","text":"<p>A broker is a Kafka instance running in a cluster.</p> <p>The broker configuration is stored in the <code>server.properties</code> file, and it can be modified by command line arguments and the <code>AdminClient</code> API.</p> <p>Some configuration parameters are <code>read-only</code> because they require a broker restart.</p> <p>Configurations are \"per-broker\" o \"cluster-wide\".</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#topics-and-messages","title":"Topics and messages","text":"<p>A topic is a sequence (or stream) of events or messages. A broker can have an unlimited number of topics, each with a unique name.</p> <p>A message has the following structure:</p> <ul> <li>timestamp, when the messages have been created or received (ingested) by Kafka</li> <li>key, optional, the key of the message</li> <li>value, the message body</li> <li>headers, optional message metadata</li> </ul> <p>Each topic is implemented in 1 or more <code>partitions</code>.</p> <p>Messages are distributed among the partitions using the hash of the message key. Messages with the same key are sent to the same partition. If the message does not have a key, the messages are sent to a random partition using a round-robin approach. It is also possible to define a custom partitioning logic.</p> <p>IMPORTANT: messages are not deleted after being consumed. They remain in the topic for 1 week (default value for the retention period). Business or legal considerations influence the retention period for all topics in a cluster and/or for a specific topic.</p> <p>A topic can be defined as <code>compacted</code>: only the most recent message with the same key will be maintained in the topics while the less recent messages will be deleted.</p> <p>The topic configuration has broker-level defaults. It can be changed programmatically or via command line arguments.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#partition","title":"Partition","text":"<p>A partition is a log structure, an immutable and time ordered sequence of events; new events are always attached at the end of the partition.</p> <p>Each partition can have 0 to N replicas, for fault tolerance. The number of replicas is the replication factor; its typical value is 3 because 3 is the typical number of brokers in the cluster. Replicas are distributed across the brokers, with one as leader and the others as followers.</p> <p>The partitions are stored as files (<code>segments</code>) on the disk. No structured storage is required by Kafka.</p> <p>The messages are guaranteed to be in temporal order at partition level but not a topic level.</p> <p>Within a partition, each message has its own position number, a unique integer assigned by Kafka and used to track until where the consumers have read the messages. This is the partition offset.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#infrastructure","title":"Infrastructure","text":"<p>Kafka runs in a cluster of brokers. Each broker is a Kafka instance with its own storage. A broker can be a node, a virtual machine, a docker compose service o a pod in a Kubernetes cluster.</p> <p>Kafka cluster is managed by an external cluster manager, <code>ZooKeeper</code>, who requires its own cluster of instances. Later, another cluster manager, <code>KRaft</code>, has been introduced. KRaft runs inside the Kafka cluster, reducing the operational complexity.</p> <p>Both ZooKeeper and KRaft implements a consensus protocol that establish who, among the Kafka brokers in the cluster, is the master one.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#producer","title":"Producer","text":"<p>A producer is an application that uses the Kafka Producer API to write events to Kafka topics.</p> <p>Producers write to leader partition(s).</p> <p>Producer properties:</p> <ul> <li>bootstrap.servers: the brokers</li> <li>key.serializer: how to serialize the message key</li> <li>value.serializer: how to serialize the message value</li> <li>acks: \"all\" (wait all in-sync replicas)</li> </ul>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#consumer","title":"Consumer","text":"<p>A consumer is an application that uses the Kafka Consumer API to wait (via polling) for new messages arrive from 1 to N topics. The consumer specify which topics are interested into.</p> <p>Consumers work in groups and consumer groups are handled by Kafka. Each consumer group has a unique ID (<code>group id</code>). Every consumer with the same group ID belong to the same group.</p> <p>During the configuration phase, the Kafka consumer groups manager assign one or more partitions to each consumer in the same consumer group.</p> <p>Consumers never stop; they run in an infinite loop waiting for the arrival of new messages.</p> <p>By default, consumers read from the leader partition but, in multi region/AZ/data center, it can be configured to read from partition followers.</p> <p>Consumers have their own topic offset.</p> <p>Consumer properties:</p> <ul> <li>bootstrap.servers: the brokers</li> <li>group.id: the ID of the consumer group</li> <li>key.serializer: how to serialize the message key</li> <li>value.serializer: how to serialize the message value</li> <li>enable.auto.commit: true or false</li> </ul>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#security","title":"Security","text":"<p>Kafka default security level is very low.</p> <ul> <li>Encryption in transit is optional and off in the default configuration. Here transit means from producer to broker, between brokers, from broker to consumer.</li> <li>Authentication and authorization are optional and off in the default configuration</li> <li>Encryption at rest (partition segments) is not available in the standard Apache Kafka, but it is available in the Confluent version. Workaround 1: encrypt the disk. Workaround 2: encrypt/decrypt the messages using the producers/consumers.</li> </ul>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#kafka-streams","title":"Kafka Streams","text":"<p>Java API included in Apache Kafka.</p> <p>It allows creating applications that run inside the Kafka cluster with an \"exactly-once\" semantic. Streams can be stateful with the state managed by the Kafka cluster.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#kafka-connect-and-connectors","title":"Kafka Connect and Connectors","text":"<p>It is a separate system with its own cluster. Its purpose is to:</p> <ol> <li>Read data from \"sources\", writing messages to Kafka topics</li> <li>Write data to \"sinks\", reading messages from Kafka topics.</li> </ol> <p>The Connect cluster has several workers that use Kafka REST API to integrate with Kafka. Each worker handle 1 to N connections. The state of the Connect cluster is stored in a Kafka topic.</p> <p>Each connector has its own configuration file and can have 1 to N tasks.</p> <p>Structure: 1 connect cluster --&gt; 1 to N workers --&gt; 1 to N tasks</p> <p>Connectors can be single instance/thread or multi-instance distributed among the workers.</p> <p>Kafka Connectors are a set of ready-to-use components, maintained by Confluent or by the community.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#confluent-rest-proxy","title":"Confluent REST Proxy","text":"<p>REST API to produce and consume messages.</p> <p>It is developed and maintained by Confluent, and it is free to use.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#confluent-schema-registry","title":"Confluent Schema Registry","text":"<p>It is a configuration server dedicated to message schemas stored in a dedicated Kafka topic.</p> <p>Producers mark the messages with a specific schema ID and the consumers check the message against the schema from the registry before processing it.</p> <p>It is developed and maintained by Confluent, and it is free to use.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#confluent-ksqldb","title":"Confluent KSQLDB","text":"<p>KSQLDB allows writing SQL like queries that filter, join and aggregates stream data producing new streams.</p> <p>KQSLDB queries never stops. They can be used to create real-time stream processing applications.</p> <p>KQSLDB runs on a dedicated server and expose its own REST API.</p> <p>Use cases for KSQLDB:</p> <ul> <li>Streaming ETL</li> <li>Anomaly detection</li> <li>Events monitoring</li> </ul>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#confluent-platform","title":"Confluent Platform","text":"<p>Functionalities not available in Apache Kafka:</p> <ul> <li>replication between clusters</li> <li>automatic balancing</li> <li>enterprise grade security</li> <li>cluster and topics monitoring</li> </ul> <p>Products included in the platform which are free to use:</p> <ul> <li>KSQLDB</li> <li>REST Proxy</li> <li>Connectors</li> <li>Schema Registry</li> </ul> <p>Products included in the platform which have commercial license:</p> <ul> <li>Replicator</li> <li>Auto Data balancers</li> <li>MQTT Proxy</li> <li>K8S operator</li> <li>Control Center</li> </ul>","tags":["certification","confluent","kafka","apache"]},{"location":"certifications/ccdak/#broker-and-topic-design","title":"Broker and topic design","text":"<p>The two main factors to be considered when design a topic are the partitions and the replication factor.</p> <p>The brokers number in the cluster limit the replication factor: 3 brokers, max 3 replicas.</p> <p>The number of consumers in the consume group limit the minimum number of partitions: 4 consumers, minimum 4 partitions.</p> <p>Also, the memory (RAM) should be controlled.</p>","tags":["certification","confluent","kafka","apache"]},{"location":"languages/","title":"Languages","text":"<p>Articles, notes and info on the languages I use and their ecosystems.</p>","tags":["languages","java","groovy","GO"]},{"location":"languages/#java","title":"Java","text":"<ul> <li>Application Servers</li> <li>Collections</li> <li>EE Bean validation</li> <li>EE CDI</li> <li>EE EJB</li> <li>EE JARS</li> <li>EE JNDI</li> <li>EE transactions</li> <li>Exceptions</li> <li>FindBugs / SpotBugs</li> <li>Gradle</li> <li>Java 17</li> <li>Java EE / Jakarta</li> <li>JDBC</li> <li>JDK commands</li> <li>JDK data structures</li> <li>Maven</li> <li>Mockito</li> <li>Open J9</li> <li>Portal &amp; portlet</li> <li>SLF4J</li> <li>Spring Boot</li> <li>Spring Security</li> <li>Streams</li> <li>Vavr</li> </ul>","tags":["languages","java","groovy","GO"]},{"location":"languages/#groovy","title":"Groovy","text":"<ul> <li>GMaven Plus plugin</li> <li>Book review: Groovy in Action</li> </ul>","tags":["languages","java","groovy","GO"]},{"location":"languages/#go","title":"GO","text":"<ul> <li>Book review: GO Programming blueprints</li> </ul>","tags":["languages","java","groovy","GO"]},{"location":"languages/groovy/gmaven-plugin/","title":"The GMavenPlus plugin","text":"<p>Last update: 21 Oct 2022</p> <p>The Maven plugin GMavenPlus allows you to build Groovy and mixed Java-Groovy projects. The plugin integrates itself into the Maven lifecycle, compiling the Groovy code and creating the stub code where required.</p> <p>As the first step, you should add the Groovy dependency in the project <code>pom.xml</code>:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.groovy&lt;/groupId&gt;\n    &lt;artifactId&gt;groovy&lt;/artifactId&gt;\n    &lt;version&gt;${groovy.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Remember that starting from the 4.0 release, Groovy groupId is <code>org.apache.groovy</code> and not anymore <code>org.codehaus.groovy</code>.</p> <p>The second step is to specify the GMavenPLus plugin in the <code>pom.xml</code> build plugins:</p> <pre><code>&lt;plugin&gt;\n    &lt;groupId&gt;org.codehaus.gmavenplus&lt;/groupId&gt;\n    &lt;artifactId&gt;gmavenplus-plugin&lt;/artifactId&gt;\n    &lt;version&gt;${gmavenplus-plugin.version}&lt;/version&gt;\n    &lt;executions&gt;\n        &lt;execution&gt;\n            &lt;goals&gt;\n                &lt;goal&gt;addSources&lt;/goal&gt;\n                &lt;goal&gt;addTestSources&lt;/goal&gt;\n                &lt;goal&gt;generateStubs&lt;/goal&gt;\n                &lt;goal&gt;compile&lt;/goal&gt;\n                &lt;goal&gt;generateTestStubs&lt;/goal&gt;\n                &lt;goal&gt;compileTests&lt;/goal&gt;\n                &lt;goal&gt;removeStubs&lt;/goal&gt;\n                &lt;goal&gt;removeTestStubs&lt;/goal&gt;\n            &lt;/goals&gt;\n        &lt;/execution&gt;\n    &lt;/executions&gt;\n    &lt;configuration&gt;\n        &lt;invokeDynamic&gt;true&lt;/invokeDynamic&gt;\n    &lt;/configuration&gt;\n&lt;/plugin&gt;\n</code></pre> <p>The final step is to add your Groovy files in the <code>src/main/groovy</code> dir. The Java files should be placed as usual in the <code>src/main/java</code> dir.</p> <p>See a complete example on my GitHub repo.</p>","tags":["groovy","java","maven","build","plugin"]},{"location":"languages/groovy/gmaven-plugin/#links","title":"Links","text":"<ul> <li>Project example</li> <li>Plugin repository on GitHub</li> <li>Plugin Home Page</li> </ul>","tags":["groovy","java","maven","build","plugin"]},{"location":"languages/java/application-servers/","title":"Application Servers","text":"<p>Last update: 21 Oct 2022</p>","tags":["java","application servers","websphere","tomcat"]},{"location":"languages/java/application-servers/#ibm-websphere","title":"IBM WebSphere","text":"Version Year EE Java 7.0 2008 EE5 6 8.0 2011 EE6 6 8.5 2012 EE6 6/7 8.5.5 2013 EE6 7 9.0 EE7 8","tags":["java","application servers","websphere","tomcat"]},{"location":"languages/java/application-servers/#apache-tomcat","title":"Apache Tomcat","text":"<p>Home page</p> Version Servlet Java 6.x 2.5 5 7.x 3.0 6+ 8.x 3.1 7+ 8.5.x 3.1 7+ 9.x 4.0 8+ 10.x 5.0 8+ 10.1.x 6.0 11+","tags":["java","application servers","websphere","tomcat"]},{"location":"languages/java/collections/","title":"COLLECTIONS","text":"<p>Last update: 2019</p>","tags":["java","collections","jdk","map","list","set"]},{"location":"languages/java/collections/#jdk-maps","title":"JDK maps","text":"<p>Java 7 and before</p> <pre><code>int size();\nboolean isEmpty();\nboolean containsKey(Object key);\nboolean containsValue(Object value);\nV get(Object key);\nV put(K key, V value);\nV remove(Object key);\nvoid putAll(Map&lt;? extends K, ? extends V&gt; m);\nvoid clear();\nSet&lt;K&gt; keySet();\nCollection&lt;V&gt; values();\nSet&lt;Map.Entry&lt;K, V&gt;&gt; entrySet();\n// Comparison and hashing\nboolean equals(Object o);\nint hashCode();\n</code></pre> <p>JAVA 8</p> <pre><code>void forEach(BiConsumer&lt;? super K, ? super V&gt; action);\n// Loop on all entries\n// Example:  map.forEach((k, v) -&gt; System.out.println(\"Key=\" + k + \" - value=\" + v));\n\n\nvoid replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function)\n// In-place update of the value for all keys\n// Example: map.replaceAll((k, v) -&gt; v == null ? \"default\" : v);\n\n\nV putIfAbsent(K key, V value)\n// If the key exists and its value is not null, do nothing\n// If the key exists and its value is null, replace the value in the map\n// If the key does not exist, add the key-value entry in the map\n// Returns the value in the map\n\nboolean remove(Object key, Object value)\n// If the key-value entry is present in the map, remove them and return true\n// Otherwise returns false.\n\n\nboolean replace(K key, V oldValue, V newValue)\n // If the key-oldValue entry is present in the map, replace its value with the newValue and return true\n// Otherwise returns false.\n\nV replace(K key, V value)\n// If the key exists in the map, replace its value with the new value\n// Returns the value in the map\n\nV computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction)\n// If the key is absent or its value is null, use the mappingFunction to calculate the new value and save it in the map\n// Returns the value in the map\n// Example: map.computeIfAbsent(key, k -&gt; new HashSet&lt;V&gt;()).add(v)\n\n\nV computeIfPresent(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction)\n// If the key is present and its value is not null, use the remappingFunction to calculate the new value and save it in the map\n// if the remappingFunction value is null, the entry is removed from the map\n// Returns the value in the map\n// Example 1:\n// I've used computeIfPresent as a null-safe way to fetch lowercase values from a map of strings.\n// String s = fields.computeIfPresent(\"foo\", (k,v) -&gt; v.toLowerCase())\n// Example 2:\n// Map&lt;String, Collection&lt;String&gt;&gt; strings = new HashMap&lt;&gt;();\n// void addString(String a) {\n//    String index = a.substring(0, 1);\n//    strings.computeIfAbsent(index, ign -&gt; new HashSet&lt;&gt;()).add(a);\n// }\n// void removeString(String a) {\n//    String index = a.substring(0, 1);\n//    strings.computeIfPresent(index, (i, c) -&gt; c.remove(a) &amp;&amp; c.isEmpty() ? null : c);\n//}\n\nV compute(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction)\n// Use the remappingFunction to calculate the new value and save it in the map if the value is not null\n// The original entry could be absent\n// Returns the value in the map\n// The operation is ATOMIC in the ConcurrentHaspMap\n// Example1: map.compute(key, (k, v) -&gt; (v == null) ? msg : v.concat(msg))}\n// Example2:  ConcurrentMap&lt;String, Integer&gt; map = new ConcurrentHashMap&lt;&gt;();\n//            map.compute(\"apple\", (key, value) -&gt; value == null ? 1 : value + 1);\n// Example3: ConcurrentMap&lt;String, LongAdder&gt; map2 = new ConcurrentHashMap&lt;&gt;();\n//           map2.computeIfAbsent(\"apple\", key -&gt; new LongAdder()).increment();\n\nV merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction)\n// If the key is absent or its value is null, save key and value in the map.\n// If the key is present and its value is not null, use the remappingFunction to calculate the new value and save it in the map\n// If the remappingFunction returns null, the entry is removed from the map\n// Returns the value in the map or null\n</code></pre> <p>JAVA 9</p> <pre><code>Map&lt;K, V&gt; of()\n// Returns an empty and immutable map\n\nMap&lt;K, V&gt; of(K k1, V v1.... k10, v10)\n// Returns an immutable map from the max 10 key-value tuples\n\nMap&lt;K, V&gt; ofEntries(Entry&lt;? extends K, ? extends V&gt;... entries)\n// Returns an immutable map from the input entries.\n</code></pre> <p>JAVA 10</p> <pre><code>Map&lt;K, V&gt; copyOf(Map&lt;? extends K, ? extends V&gt; map)\n// Returns an immutable copy of the input map. Key and values cannot be null.\n</code></pre>","tags":["java","collections","jdk","map","list","set"]},{"location":"languages/java/collections/#jdk-javautilcollection","title":"JDK java.util.Collection","text":"<p>Java 7 and before</p> <pre><code>int size();\nboolean isEmpty();\nboolean contains(Object o);\nIterator&lt;E&gt; iterator();\nObject[] toArray();\n&lt;T&gt; T[] toArray(T[] a);\nboolean add(E e);\nboolean remove(Object o);\nboolean containsAll(Collection&lt;?&gt; c);\nboolean addAll(Collection&lt;? extends E&gt; c);\nboolean removeAll(Collection&lt;?&gt; c);\nboolean retainAll(Collection&lt;?&gt; c);\nvoid clear();\nboolean equals(Object o);\nint hashCode();\n</code></pre> <p>JAVA 8</p> <pre><code>boolean removeIf(Predicate&lt;? super E&gt; filter)\n// Replaces all elements in the collection for whom the filter is true.\n\nSpliterator&lt;E&gt; spliterator()\n// creates a spliterator from the collection\n\nStream&lt;E&gt; stream()\n// Creates a stream from the collection\n\nStream&lt;E&gt; parallelStream()\n// Creates a parallel stream from the collection\n</code></pre> <p>JAVA 11</p> <pre><code>T[] toArray(IntFunction&lt;T[]&gt; generator)\n</code></pre>","tags":["java","collections","jdk","map","list","set"]},{"location":"languages/java/collections/#jdk-sets","title":"JDK sets","text":"<p>Java 7 e precedenti</p> <pre><code>int size();\nboolean isEmpty();\nboolean contains(Object o);\nIterator&lt;E&gt; iterator();\nObject[] toArray();\n&lt;T&gt; T[] toArray(T[] a);\nboolean add(E e);\nboolean remove(Object o);\nboolean containsAll(Collection&lt;?&gt; c);\nboolean addAll(Collection&lt;? extends E&gt; c);\nboolean retainAll(Collection&lt;?&gt; c);\nboolean removeAll(Collection&lt;?&gt; c);\nvoid clear();\nboolean equals(Object o);\nint hashCode();\n</code></pre> <p>JAVA 8</p> <pre><code>Spliterator&lt;E&gt; spliterator()\n// Creates a spliterator from the set\n</code></pre> <p>Java 9</p> <pre><code>Set&lt;E&gt; of()\n// Returns an empty and immutable set\n\nSet&lt;E&gt; of(v1   ....  v10)\n// Creates an immutable set with max 10 not-null input values\n\nSet&lt;E&gt; of(E... elements)\n// Creates an immutable set with the not-null input values\n</code></pre> <p>Java 10</p> <pre><code>&lt;E&gt; Set&lt;E&gt; copyOf(Collection&lt;? extends E&gt; coll)\n// Creates an immutable set with a copy of the values included in the `coll` collection\n</code></pre>","tags":["java","collections","jdk","map","list","set"]},{"location":"languages/java/collections/#jdk-lists","title":"JDK lists","text":"<p>Java 7 e precedenti</p> <pre><code>int size();\nboolean isEmpty();\nboolean contains(Object o);\nIterator&lt;E&gt; iterator();\nObject[] toArray();\n&lt;T&gt; T[] toArray(T[] a);\nboolean add(E e);\nboolean remove(Object o);\nboolean containsAll(Collection&lt;?&gt; c);\nboolean addAll(Collection&lt;? extends E&gt; c);\nboolean addAll(int index, Collection&lt;? extends E&gt; c);\nboolean removeAll(Collection&lt;?&gt; c);\nboolean retainAll(Collection&lt;?&gt; c);\nvoid clear();\nboolean equals(Object o);\nint hashCode();\nE get(int index);\nE set(int index, E element);\nvoid add(int index, E element);\nE remove(int index);\nint indexOf(Object o);\nint lastIndexOf(Object o);\nListIterator&lt;E&gt; listIterator();\nListIterator&lt;E&gt; listIterator(int index);\nList&lt;E&gt; subList(int fromIndex, int toIndex);\n</code></pre> <p>JAVA 8</p> <pre><code>void replaceAll(UnaryOperator&lt;E&gt; operator)\n// Apply the operator to all elements in the list. In-place change\n// Example: lista.replaceAll(Strings::trim)\n// Example: employeeList.replaceAll(employee -&gt; {employee.setSalary(employee.getSalary() * 1.1);\n\nsort(Comparator&lt;? super E&gt; c)\n// Order the collection using the input comparator\n// Example: employeeList.sort((emp1, emp2)-&gt; Double.compare(emp1.getSalary(),emp2.getSalary()));\n\nSpliterator&lt;E&gt; spliterator()\n// Creates a sliterator from the list\n</code></pre> <p>JAVA 9</p> <pre><code>List&lt;E&gt; of()\n// Creates an empty and immutable list\n\n\nList&lt;E&gt; of(e1 ...  e10)\n// Creates an immutable list with the max 10 not-null input values\n\n\nList&lt;E&gt; of(E... elements)\n// Creates an immutable list with the not-null input values\n</code></pre> <p>Java 10</p> <pre><code>&lt;E&gt; List&lt;E&gt; copyOf(Collection&lt;? extends E&gt; coll)\n// Creates an immutable list with a copy of the values included in the `coll` collection\n</code></pre>","tags":["java","collections","jdk","map","list","set"]},{"location":"languages/java/ee-bean-validation/","title":"Bean Validation  (beanvalidation.org)","text":"<p>Last update: 2019</p> Version JSR EE YEAR Libraries 1.0 JSR303 EE6 2009 HibernateValidator 4.* 1.1 JSR349 EE7 2013 HibernateValidator 5.* 2.0 JSR380 EE8 2017 HibernateValidator 6.*","tags":["java","bean validation","JSR","EE"]},{"location":"languages/java/ee-cdi/","title":"EE CDI","text":"<p>Last update: 2021</p> <p>Dependency inject mechanism introduced with EE 6.</p> <p>Annotations:</p> <ul> <li>@Inject</li> <li>@xxScoped (Request, Session, Application, Conversation)</li> <li>@Dependent</li> <li>@Named (makes the bean accessible via EL)</li> <li>@PostConstruct</li> <li>@PreDestroy</li> </ul>","tags":["java","ee","cdi","dependency injection"]},{"location":"languages/java/ee-ejb/","title":"EE EJB","text":"<p>Last update: 2021</p>","tags":["java","ee","ejb"]},{"location":"languages/java/ee-ejb/#session-beans","title":"Session Beans","text":"<ul> <li>Stateful</li> <li>Stateless</li> <li>Singleton</li> </ul>","tags":["java","ee","ejb"]},{"location":"languages/java/ee-ejb/#entity-beans","title":"Entity Beans","text":"<ul> <li>Container managed persistence (CMP)</li> <li>Bean managed persistence (BMP)</li> </ul> <p>The entity beans are and they should be replaced with the JPA entities.</p>","tags":["java","ee","ejb"]},{"location":"languages/java/ee-ejb/#message-driven-beans-mdb","title":"Message Driven Beans (MDB)","text":"","tags":["java","ee","ejb"]},{"location":"languages/java/ee-jaxrs/","title":"EE JAX-RS","text":"<p>Last update: 7 Oct 2022</p> <p>Java or Jakarta RESTful Web Services</p> <p>Web Site</p>","tags":["java","ee","jaxrs","rest"]},{"location":"languages/java/ee-jaxrs/#versions","title":"Versions","text":"Version JSR EE 1.1 Java 2.0 JSR339 Java 2.1 JSR370 Java 2.1.* Jakarta 3.* Jakarta 4.* Jakarta","tags":["java","ee","jaxrs","rest"]},{"location":"languages/java/ee-jaxrs/#implementations","title":"Implementations","text":"<p>JERSEY (org.glassfish.jersey, prima era org.jersey)</p> <ul> <li>1.x  -&gt;  JAX-RS 1.1</li> <li>2.25 -&gt;  JAX-RS 2.0</li> <li>2.27 -&gt;  JAX-RS 2.1</li> </ul> <p>RestEASY</p> <ul> <li>??  -&gt;  JAX-RS 1.1</li> <li>3.1 -&gt;  JAX-RS 2.0</li> <li>3.5 -&gt;  JAX-RS 2.1</li> </ul> <p>CXF  (org.apache.cxf)</p> <ul> <li>2.6 -&gt;  JAX-RS 1.1</li> <li>3.1 -&gt;  JAX-RS 2.0</li> <li>3.2 -&gt;  JAX-RS 2.1</li> </ul>","tags":["java","ee","jaxrs","rest"]},{"location":"languages/java/ee-jndi/","title":"EE JNDI","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","ee","jndi"]},{"location":"languages/java/ee-jndi/#resource","title":"Resource","text":"<pre><code>@Resource(name=\"xxx\")  resource;\n@Resource(lookup=\"xxx\")  resource;\n@Resource(name=\"xxx\",lookup=\"yyy\")  resource;\n</code></pre> <p>The <code>@Resource</code> annotation:</p> <ul> <li>binds the field <code>resource</code> to the name <code>xxx</code> defined in the Environment Naming Context (ENC, <code>java:comp/env/xxx</code>) of the component; the name parameter refers to the JNDI name (<code>env-entry-name</code> in the <code>ejp-jar.xml</code> file)</li> <li>lookup the JNDI resource <code>yyy</code></li> </ul>","tags":["java","ee","jndi"]},{"location":"languages/java/ee-transactions/","title":"JAVA EE Transactions","text":"<p>Last update: 21 Oct 2022</p> <p>The Java EE transactions can be </p> <ul> <li>Container managed</li> <li>Application (o bean) managed</li> </ul> <p>The following resources are related to the transactions:</p> <pre><code>@Resource\nprivate EJBContext context;\n\n@Resource\nprivate UserTransaction userTransaction;\n</code></pre> <p>Links:</p> <ol> <li>Force rollback</li> </ol>","tags":["java","ee","transactions"]},{"location":"languages/java/ee-transactions/#container-managed-transaction-cmt","title":"Container-Managed Transaction (CMT)","text":"<p>The container-managed transactions are handled by the EJB container and the EJB class has the following annotation:</p> <pre><code>@TransactionManagement(TransactionManagementType.CONTAINER)\n</code></pre> <p>The EJB methods are the points where the transaction starts and ends. In general, the transaction is created by the container before invoking the method and it is committed after the method returns.</p> <p>The rollback of a failing transaction is caused by an exception raised by the invoked method. The roolback can also be forced by invoking the <code>context.setRollbackOnly()</code> method.</p> <p>In a container-managed transaction, it is forbidden to:</p> <ul> <li>Invoke the methods <code>commit</code>, <code>setAutoCommit</code>, and <code>rollback</code> of the class <code>java.sql.Connection</code></li> <li>Invoke the method <code>getUserTransaction</code> of the class <code>javax.ejb.EJBContext</code></li> <li>Invoke any method of the class <code>javax.transaction.UserTransaction</code></li> </ul> <p>The annotation <code>@TransactionAttribute</code> determines how the container creates the transactions before invoking the EJB method. Its possible values are:</p> <ul> <li>REQUIRED: the default value. The container creates a new transaction or uses the one created by the invoking method (if any)</li> <li>REQUIRES_NEW: the container always creates a new transaction, even if there is already one transaction active (that will be suspended).</li> <li>MANDATORY: there should be already an active transaction created by the invoking method otherwise a <code>TransactionRequiredException</code> is raised.</li> <li>NOT_SUPPORTED: the method without any transaction. If an active transaction exists, it is suspended.</li> <li>SUPPORTS: if there is an active transaction, it will be kept active while invoking the method. Otherwise, no transaction is created. The use of this value is discouraged because of the uncertainty of the transaction's presence.</li> <li>NEVER: the method should never be executed inside a transaction, An exception is raised if one active transaction exists.</li> </ul> <p>Please note that the MDB just support the REQUIRED and NOT_SUPPORTED attributes.</p> <p>Pseudo-code:</p> <pre><code>try {\n    //..updates\n} catch(Exception ecc) {\n    // An exception has been detected. Rollback the transaction\n    sessioncontext.setRollbackOnly();\n}\n</code></pre>","tags":["java","ee","transactions"]},{"location":"languages/java/ee-transactions/#application-bean-managed-transaction-bmt","title":"Application-bean managed transaction (BMT)","text":"<p>The BMT transactions are handled by our code. The following annotation should be specified at EJB class level</p> <pre><code>    @TransactionManagement(TransactionManagementType.BEAN)\n</code></pre> <p>Our code should create the transactions and close them normally (commit) or abnormally (rollback).</p> <p>The Java Transaction API (JTA) is the API we should refer to handle the bean-managed transactions.</p> <p>Within this API, the interface <code>javax.transaction.UserTransaction</code> is the one we should use to perform actions on the transactions. An instance of this interface can be injected as a resource (see above).</p> <p>Pseudo-codice:</p> <pre><code>try {\n    usertransaction.begin();\n    // ..updates\n    usertransaction.commit();\n} catch(Exception ecc) {\n    usertransaction.rollback;\n}\n</code></pre>","tags":["java","ee","transactions"]},{"location":"languages/java/exceptions/","title":"Exceptions in Java","text":"<p>Last update: 14 Nov 2022</p> <p>Certification topics:</p> <ul> <li>Handle exceptions using try/catch/finally clauses, try-with-resource, and multi-catch statements</li> <li>Create and use custom exceptions</li> </ul> <p>References and credits:</p> <ul> <li>Effective Java (book)</li> <li>Top 20 Java Exception Handling Best Practices (post)</li> </ul> <p>In Java, the exceptions are divided into checked, unchecked and errors.</p>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/exceptions/#checked-exceptions","title":"Checked Exceptions","text":"<p>The checked exceptions are exceptions that must be declared in the throws clause of a method.</p> <p>They extend the class Exception and are intended to be an \"in your face\" type of exception.</p> <p>A checked exception indicates an expected problem that can occur during normal system operation. </p> <p>Some examples are problems communicating with external systems and problems with user input. Note that, depending on your code's intended function, \"user input\" may refer to a user interface, or it may refer to the parameters that another developer passes to your API. </p> <p>Often, the correct response to a checked exception is to try again later or to prompt the user to modify his input.</p>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/exceptions/#unchecked-exceptions","title":"Unchecked Exceptions","text":"<p>The unchecked exceptions are exceptions that do not need to be declared in a throws clause. They extend RuntimeException.</p> <p>An unchecked exception indicates an unexpected problem that is probably due to a bug in the code. The most common example is the NullPointerException.</p> <p>There are many core exceptions in the JDK that are checked exceptions but really shouldn't be, such as IllegalAccessException and NoSuchMethodException.</p> <p>An unchecked exception probably shouldn't be retried, and the correct response is usually to do nothing and let it bubble up out of your method and through the execution stack. This is why it doesn't need to be declared in a throws clause. </p> <p>Eventually, at a high level of execution, the exception should probably be logged (see below).</p>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/exceptions/#errors","title":"Errors","text":"<p>Errors are serious problems that are almost certainly not recoverable.</p> <p>Some examples are OutOfMemoryError, LinkageError, and StackOverflowError.</p>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/exceptions/#ui","title":"UI","text":"<p>When the application user is a human (i.e. we have a UI), we have several options when an internal exception is thrown.</p> <p>Note: validation errors due to invalid form fields are not exceptions.</p> <p>Solution 1</p> <p>Show the exception stack trace to the user who will hardly understand. The application remains in an undefined state.</p> <p>Solution 2</p> <p>Hide the exception to the user. The application seems working properly but the exception affected its behaviour. An example: the user press a button, the logic behind it raises an exception and does not complete its function. The user, not seeing any error, does not understand what is happening.</p> <p>Solution 3</p> <p>The application shows a human-understandable error message that explains a problem has occurred and the requested function cannot be executed. The error message can contain a unique error identifier that can be used to track the error in the log files. After showing the message, the application returns to a well-defined status. This is the preferred solution.</p>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/exceptions/#effective-java","title":"Effective Java","text":"<p>Joshua Bloch's \"Effective Java, 3rd edition\" book contains an entire chapter, the 10th, on exceptions.</p> <p>This chapter contains several items:</p> <ul> <li>Item 69: use exceptions only for exceptional conditions</li> <li>Item 70: use checked exceptions for recoverable conditions and runtime exceptions for programming errors</li> <li>Item 71: avoid unnecessary use of checked exceptions</li> <li>Item 72: Favor the use of standard exceptions</li> <li>Item 73: Throw exceptions appropriate to the abstraction</li> <li>Item 74: Document all exceptions thrown by each method</li> <li>Item 75: Include failure-capture information in detailed messages</li> <li>Item 76: Strive for failure atomicity</li> <li>Item 77: Don't ignore exceptions</li> </ul> <p>From the above items, we can derive the following rules:</p> <ol> <li>Do not use exceptions to handle expected situations.</li> <li>Do not throw Error</li> <li>Do not use the exception message to analyze the cause of the problem. Use custom fields to deliver context information.</li> <li>Do not throw checked exceptions if the caller(s) cannot handle them. In this case, throw an unchecked exception.</li> <li>Use standard JDK exceptions when possible.</li> <li>Do not rethrow layer specific exceptions. For example, do not rethrow SQLException from a data layer. Map the exception to an application-oriented exception.</li> </ol>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/findbugs/","title":"FindBugs &amp; SpotBugs","text":"<p>TO BE UPDATED</p> <ul> <li>FindBugs 2.x  Java 5+ (?)  Last version: 2.0.3</li> <li>FindBugs 3.x  Java 7+      Last version: 3.0.1</li> </ul> <p>Suppress annotation</p> <pre><code>import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n\n@SuppressFBWarnings(value = { \"DMI_EMPTY_DB_PASSWORD\", \"HARD_CODE_PASSWORD\" }, justification = \"Test\")\n@SuppressFBWarnings(value = { \"NP_UNWRITTEN_FIELD\", \"UWF_UNWRITTEN_FIELD\" }, justification = \"UI components defined by Vaadin\")\n</code></pre> <p>Dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt;\n    &lt;artifactId&gt;annotations&lt;/artifactId&gt;\n    &lt;version&gt;3.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>","tags":["java","findbugs","spotbugs","quality","checks"]},{"location":"languages/java/gradle/","title":"Gradle","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","gradle","build"]},{"location":"languages/java/gradle/#best-practice","title":"Best practice","text":"<ol> <li>usare sempre il wrapper (da creare all'inizio del progetto)</li> <li>usare il daemon se si compila in locale</li> </ol>","tags":["java","gradle","build"]},{"location":"languages/java/gradle/#commands","title":"Commands","text":"<p>Create the wrapper</p> <pre><code>gradle wrapper --gradle-version=xxx\n</code></pre> <p>Show the versionMostra versione</p> <pre><code>gradlew -v\n</code></pre> <p>Updates the wrapper</p> <pre><code>gradlew wrapper --gradle-version x.x.x\n</code></pre>","tags":["java","gradle","build"]},{"location":"languages/java/java17/","title":"JAVA 17","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","17","jdk"]},{"location":"languages/java/java17/#record","title":"Record","text":"<ul> <li>immutable data class</li> <li>no setter</li> <li>the getters have the xxx form (not getXxx())</li> <li>equals, hashCode and toString are automatically defined</li> <li>no instance fields</li> <li>static fields are allowed</li> <li>can be defined inside other records, classes and methods (like named tuples)</li> </ul> <pre><code>record MyFirstRecord(int c, String S) {}\n</code></pre> <p>A record can have a simplified constructor useful to add assertions and validations:</p> <pre><code>record MyFirstRecord(int c, String S) {\n    MyFirstRecord() {\n        // c and S assigned automatically\n        if (c &lt; 42) throw new IllegalArgumentException();\n    }\n}\n</code></pre>","tags":["java","17","jdk"]},{"location":"languages/java/java17/#sealed-class","title":"Sealed class","text":"<p>A sealed class or interface restricts which classes or interfaces can extend or implement it. </p>","tags":["java","17","jdk"]},{"location":"languages/java/java17/#pattern-matching-instanceof","title":"Pattern matching instanceOf","text":"<p>No need to cast the value before using it.</p> <p>Before Java 17</p> <pre><code>if (shape instanceof Rectangle) {\n    Rectangle r = (Rectangle) shape; // CAST\n    return 2 * r.length() + 2 * r.width();\n}\n</code></pre> <p>Java 17</p> <pre><code>if (shape instanceof Rectangle r) {\n    return 2 * r.length() + 2 * r.width();\n}\n</code></pre>","tags":["java","17","jdk"]},{"location":"languages/java/java17/#text-blocks-15","title":"Text blocks (15+)","text":"<p>Strings defined on multiple lines without escaping.</p> <pre><code>String html = \"\"\"\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7    &lt;html&gt;\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7        &lt;body&gt;\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7            &lt;p&gt;Hello World.&lt;/p&gt;\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7        &lt;/body&gt;\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7    &lt;/html&gt;\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\"\"\";\n</code></pre> <p>The 3 <code>\"</code> open and close the text block and they also define the indention (shown with the dots).</p>","tags":["java","17","jdk"]},{"location":"languages/java/java17/#switch-expression","title":"Switch expression","text":"<p>The switch can be also used as expression, returning a value.</p> <p>Unlike switch statements, the cases of switch expressions must be exhaustive, which means that for all possible values.</p> <p>The <code>yield</code> statement does not require the <code>break</code> statement.</p> <p>Syntax 1 (recommended)</p> <pre><code>val result = switch(label) {\n    case label1 -&gt; value1;\n    case label2 -&gt; {\n        ....\n        yield value2;\n        }\n    case label3 -&gt; throw new Exception()\n    default -&gt; ...\n}\n</code></pre> <p>Syntax 2</p> <pre><code>val result = switch(label) {\n    case label1 : \n        yield value1;\n    case label2:\n        ....\n        yield value2;\n    case label3: throw new Exception()\n    default: ...\n}\n</code></pre>","tags":["java","17","jdk"]},{"location":"languages/java/javaee-jakarta/","title":"JAVA EE / JAKARTA","text":"<p>Last update: 10 Feb 2023</p>","tags":["javaee","jakarta"]},{"location":"languages/java/javaee-jakarta/#versions","title":"Versions","text":"Name Release Year EE 1.2 1999 EE 1.3 2001 EE 1.4 2003 EE 5 2006 EE 6 2009 EE 7 2013 EE 8 2017 Jakarta 8 2019 Jakarta 9 2020 Jakarta 9.1 2021 Jakarta 10 2022","tags":["javaee","jakarta"]},{"location":"languages/java/javaee-jakarta/#modules","title":"Modules","text":"Name Description EE6 EE7 EE8 Batch Batch applications for the Java Platform - 1.0 JSR352 1.0 JSR352 Bean Validation Bean Validation 1.0 JSR303 1.1 JSR349 2.0 JSR380 CDI Contexts and Dependency Injection for the Java EE Platform 1.0 JSR299 1.1 JSR346 2.0 JSR365 Common Annotations Common Annotations for the Java Platform 1.1 JSR250 1.2 JSR250 1.3 JSR250 Concurrency Concurrency utilities for Java EE - 1.0 JSR236 1.0 JSR236 Debugging Debugging support for Other Languages 1.0 JSR45 1.0 JSR45 1.0 JSR45 Deployment Java EE Application Deployment 1.2 JSR88 1.2 JSR88 1.2 JSR88 DI Dependency Injection for Java 1.0 JSR330 1.0 JSR330 1.0 JSR330 EJB Enterprise JavaBeans 3.1 JSR318 3.2 JSR345 3.2 JSR345 EL Unified Expression Language 2.2 JSR245 3.0 JSR341 3.0 JSR341 Interceptors Interceptors 1.1 JSR318 1.2 JSR318 1.2 JSR318 JACC Java Authentication Service Provider Contract for Containers 1.3/1.4 JSR115 1.5 JSR115 1.5 JSR115 JAF JavaBeans Activation Framework 1.1 JSR925 1.1 JSR925 1.1 JSR925 JASPIC Java Authentication Service Provider Interface for Containers 1.0 JSR196 1.1 JSR196 1.1 JSR196 JavaMail JavaMail API 1.4 JSR919 1.5 JSR919 1.6 JSR919 JAX-RPC Java API for XML-based RPC 1.1 JSR101 1.1 JSR101 1.1 JSR101 JAX-RS Java API for RESTful Web Services 1.1 JSR311 2.0 JSR339 2.1 JSR370 JAX-WS Java API for XML based Services 2.2 JSR224 2.2 JSR224 2.2 JSR224 JAXB Java Architecture for XML Binding 2.2 JSR222 2.2 JSR222 2.2 JSR222 JAXM Java API for XML Messaging 1.3 JSR67 1.3 JSR67 JAXP Java API form XML processing 1.3 JSR206 1.3 JSR206 1.6 JSR206 JAXR Java API for XML registries 1.0 JSR93 1.0 JSR93 1.0 JSR93 JCA Java EE Connector Architecture 1.6 JSR322 1.7 JSR322 1.7 JSR322 jCache - 1.0 JSR107 JDBC Java Database Connectivity 4.0 JSR221 4.0 JSR221 4.0 JSR221 JMS Java Message Service API 1.1 JSR914 2.0 JSR343 2.0 JSR343 JMX Java Manegement Extensions 2.0 JSR255 2.0 JSR255 2.0 JSR255 JPA Java Persistence API 2.0 JSR317 2.1 JSR338 2.2 JSR338 JSF JavaServer Faces 2.0 JSR314 2.2 JSR344 2.3 JSR372 JSON-B Java API for JSON Binding - - 1.0 JSR367 JSON-P Java API for JSON Processing - 1.0 JSR353 1.1 JSR374 JSP Java Server Pages 2.2 JSR245 2.3 JSR245 2.3 JSR245 JSTL JavaServer Pages Standard Tag Library 1.2 JSR52 1.2 JSR52 1.2 JSR52 JTA Java Transaction API 1.1 JSR907 1.2 JSR907 1.2 JSR907 Managed Beans Managed Beans 1.0 JSR316 1.0 JSR316 1.0 JSR316 Management J2EE Management 1.1 JSR77 1.1 JSR77 1.1 JSR77 Security Java EE Security API - - 1.0 JSR375 Servlet Java Servlet 3.0 JSR315 3.1 JSR340 4.0 JSR369 StAX Streaming API for XML 1.0 JSR173 1.0 JSR173 1.0 JSR173 Web Services Implementing Enterprise WebServices 1.3 JSR109 1.3 JSR109 1.3 JSR109 WebSocket Java API for WebSocket - 1.1 JSR356 1.1 JSR356 WS MetaData Web Services Metadata for the Java platform 2.1 JSR181 2.1 JSR181 2.1 JSR181","tags":["javaee","jakarta"]},{"location":"languages/java/jdbc/","title":"JDBC","text":"<p>Last update: 14 Nov 2022</p> <p>Certification topics:</p> <ul> <li>Connect to and perform database SQL operations, process query results using JDBC API </li> </ul> <p>Links:</p> <ul> <li>JDBC Tutorial</li> </ul>","tags":["java","jdbc"]},{"location":"languages/java/jdbc/#packages-javasql-and-javaxsql","title":"Packages java.sql and javax.sql","text":"<p>The DataSource interface is an alternative to the DriverManager for establishing a connection with a data source.</p> <p>Connection pooling and Statement pooling</p> <p>Distributed transactions</p> <p>Rowsets</p>","tags":["java","jdbc"]},{"location":"languages/java/jdk-commands/","title":"JDK command line tools","text":"<p>Last update: 12 Nov 2022</p>","tags":["java","jdk","cli","tools","hotspot","j9","openj9"]},{"location":"languages/java/jdk-commands/#hotspot-and-j9-common-commands","title":"HotSpot and J9 common commands","text":"<ul> <li>jar         Creates the .jar files</li> <li>jarsigner   Signs the .jar files</li> <li>java        Starts thte JVM and execute the Java code in the application</li> <li>javac       Java compiler</li> <li>javadoc     Extracts the documentation info from the Java source files and produce the JavaDoc</li> <li>javap       Show the bytecode of a .class file</li> <li>jconsole    profiler, Java Monitoring and Management Console</li> <li>jdb         Debugger</li> <li>jdeprscan   DEPRecated Scanner, looks for deprecated code in .class and .jar files</li> <li>jdeps       Show the dependencies</li> <li>jimage      Handle the JIMAGE package</li> <li>jjs         Nashhorn JavaScript engine</li> <li>jlink       9+ Linker to create custom JRE</li> <li>jmod        Handles the JMOD package</li> <li>jrunscript  Generic command line script shell. By default, it uses JavaScript</li> <li>jshell      9+ Java Shell</li> <li>keytool     Handles the keystore</li> <li>pack200     Compress files</li> <li>rmic        RMI Compiler</li> <li>rmid        RMI Activation daemon</li> <li>rmiregistry RMI registry</li> <li>serialver   Shows the serial version UID</li> <li>unpack200   Decompress a pack2000 file</li> </ul>","tags":["java","jdk","cli","tools","hotspot","j9","openj9"]},{"location":"languages/java/jdk-commands/#hotspot-only-commands","title":"HotSpot only commands","text":"<ul> <li>jaotc       Ahead-of-time Compiler</li> <li>jcmd        JVM runtime diagnostic</li> <li>jhsdb       Core dump analyzer</li> <li>jinfo       Shows JVM runtime info</li> <li>jmap        Shows the runtime memory map (replaced by jcmd)</li> <li>jps         Lists all the applications running in a JVM</li> <li>jstack      Shows the Java threads stack trace</li> <li>jstat       Shows JVM statistics</li> <li>jstatd      Daemon version of jstat</li> </ul>","tags":["java","jdk","cli","tools","hotspot","j9","openj9"]},{"location":"languages/java/jdk-commands/#j9-only-commands","title":"J9 only commands","text":"<ul> <li>jdmpview    Analyse a core dump file</li> <li>jextract    Extract info from a core dump file</li> <li>traceformat Process the trace data</li> </ul>","tags":["java","jdk","cli","tools","hotspot","j9","openj9"]},{"location":"languages/java/jdk-commands/#commands-libraries-and-projects-once-in-the-jdk-and-are-now-independent","title":"Commands, libraries and projects once in the JDK and are now independent","text":"<ul> <li>Visual VM           jvisualvm</li> <li>Mission Control     jmc</li> <li>JavaFX              javafxpackager, javapackager</li> <li>CORBA               idlj, orbd, servertool, tnameserv</li> <li>JAXB                schemagen, xjc</li> <li>JAXWS/WSDL          wsgen, wsimport</li> </ul>","tags":["java","jdk","cli","tools","hotspot","j9","openj9"]},{"location":"languages/java/jdk-commands/#commands-removed-after-java-8","title":"Commands removed after Java 8","text":"<ul> <li>appletviewer    Removed in 11</li> <li>extcheck        Removed in 9</li> <li>javah           Native-Header Generation Tool, Removed in 10</li> <li>javaws          WebStart, Removed in 11</li> <li>jhat            Heap visualizer, Removed in 9</li> <li>jsadebugd       Removed in 9</li> <li>native2ascii    Removed in 9. https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8074431</li> <li>policytool      Removed in 10</li> </ul>","tags":["java","jdk","cli","tools","hotspot","j9","openj9"]},{"location":"languages/java/jdk-data-structures/","title":"DATA STRUCTURES IN THE JDK","text":"<p>Last update: 9 Nov 2022</p>","tags":["java","jdk","data structures","list","set","map","queue","iterator"]},{"location":"languages/java/jdk-data-structures/#list","title":"LIST","text":"<p>Iteration/enumeration</p> <pre><code>The enumeration is the old way (Java 1) to iterate a collection of items. It has been replaced by the iterator.\n</code></pre> <p>ArrayList</p> <pre><code>The standard list. It is based on an array.\n</code></pre> <p>CopyOnWriteArrayList</p> <pre><code>Useful when removing elements during iteration.\nIt should not be used when the list is very big because it creates several copies of the underlying array. \nOne of the copies is kept to be used for the iterator.\nUseful when there are many readers and few writers like in observers/listener lists.\nobservers/listeners\n</code></pre> <p>LinkedList (Do not use)</p> <pre><code>It is based on nodes, not arrays.\nDo not use, very inefficient.\n</code></pre> <p>Vector (Do not use)</p> <pre><code>Synchronised array.\nAlternative: Collections.synchronizedList( new ArrayList&lt;&gt;(...))\n</code></pre> <p>Stack  (Do not use)</p> <pre><code>Extend Vector.\nAlternative: ConcurrentLinkedDequeue\n</code></pre>","tags":["java","jdk","data structures","list","set","map","queue","iterator"]},{"location":"languages/java/jdk-data-structures/#set","title":"SET","text":"<p>TreeSet</p> <pre><code>The items in the collection should be Comparable: the identity of the items is based on the method compareTo, not on the methods equals or hashCode.\nThe items are ordered.\nThe binary tree is a Red-Black type so tree branches are balanced in length.\nNot thread-safe.\n</code></pre> <p>ConcurrentSkipListSet</p> <pre><code>A thread-safe variant of TreeSet that uses a SkipList list that uses multiple pointers to get directly to the middle and other intermediate points of the list, to have a log(n) read access therefore faster than the LinkedList.\n\nThe performances are similar to the TreeSet but concurrently.\nIt is not frequently used, even in the JDK itself.\n</code></pre> <p>CopyOnWriteArraySet</p> <pre><code>It is a thread-safe structure similar to CopyOnWriteArrayList.\nItems are not ordered. The method equals is used for univocity.\nSuggested for small collections because all operations have quadratic (n*n) cost.\n</code></pre> <p>HashSet</p> <pre><code>A not thread-safe set which uses the method hashCode for univocity.\nItems are not ordered.\n</code></pre> <p>ConcurrentHashMap.getKeySet</p> <pre><code>A way to get a thread-safe set similar to TreeSet.\n</code></pre> <p>LinkedHashSet</p> <pre><code>A not thread-safe set based on LinkedHashMap (see later)\n</code></pre> <p>EnumSet     Non thread-safe. It is a set where items are enums.     It is optimised for set operations.</p>","tags":["java","jdk","data structures","list","set","map","queue","iterator"]},{"location":"languages/java/jdk-data-structures/#map","title":"MAP","text":"<p>HashMap</p> <pre><code>Not thread-safe. \nThe key hashCode is used to determine the bucket in the map where to store the key-value couple.\nInside the bucket, the couple is saved in a linked list.\nFrom Java 8, if the bucket contains more than 11 couples, the linked list is replaced by a tree set for performance reasons. it is so better to use a key which implements the Comparable interface.\n</code></pre> <p>ConcurrentHashMap</p> <pre><code>The thread-safe variant of the HashMap.\nFrom Jaba 8, its overhead is low and there are no more strong reasons to prefer the HashMap.\nBe aware that two concurrency bugs in the computeIfAbsent method have been fixed in Java 9.\n</code></pre> <p>TreeMap</p> <pre><code>Not thread-safe. The keys are ordered and they should implement the Comparable interface.\n</code></pre> <p>ConcurrentSkipListMap</p> <pre><code>It is the concurrent version of the TreeMap.\nNot so used.\n</code></pre> <p>Hashtable</p> <pre><code>It is a Java 1 concurrent map with very slow reads.\nBetter to use the ConcorrentHashMap\n</code></pre> <p>LinkedHashMap</p> <pre><code>Not thread-safe. It is similar to the HashMap but with links that record the insertion order.\nThe order can be changed to a least recently used (LRU) order.\n</code></pre> <p>EnumMap</p> <pre><code>Not thread-safe. It is a specialised map where the keys are enums.\nNot so used but it is convenient as a list index instead of the Enum.ordinal() method.\n</code></pre> <p>IndentityHashMap</p> <pre><code>Not thread-safe. The key's presence is checked with the identity (==) instead of the equality ( equals() ).\nNot so used.\n</code></pre> <p>Properties</p> <pre><code>It exists since Java 1. Originally it used a HashTable but now it is based on a ConcurrentHashMap.\n</code></pre> <p>WeakHashMap</p> <pre><code>Similar to the HashMap where the keys are WeakReferences. The idea is the map can be emptied by the garbage collector.\nOperation time is unpredictable because the purge is implemented in the calls. Moreover, the WeakReferences are not handled very well by the garbage collector.\n</code></pre>","tags":["java","jdk","data structures","list","set","map","queue","iterator"]},{"location":"languages/java/jdk-data-structures/#iterators","title":"ITERATORS","text":"<p>fail-fast</p> <pre><code>The iterator checks if the underlying collection has been modified. If so, it throws a ConcurrentModificationException.\nThe not thread-safe collections usually have a fail-fast iterator.\nIt is not guaranteed (best-effort) that the iterator detects the change in the collection.\n</code></pre> <p>weakly consistent (or fail-safe)</p> <pre><code>The thread-safe collections use fail-safe iterators. These iterators can handle the changes in the underlying collection; they never throw a ConcurrentModificationException.\nThey cannot guarantee (weakly consistency) that they report the latest state of the collection.\nThe number of the collection's items iterated is the one when the iterator has been created.\n</code></pre> <p>snapshot</p> <pre><code>The snapshot iterator uses a read-only snapshot of the content of the collection. It is used in the COW collections (CopyOnWriteArrayList and CopyOnWriteArraySet).\n</code></pre> <p>undefined</p> <pre><code>Iterator added to the old Vector and HashTable collections.\nTheir behaviour is undefined when there are concurrent writes.\n</code></pre>","tags":["java","jdk","data structures","list","set","map","queue","iterator"]},{"location":"languages/java/jdk-data-structures/#queue-e-dequeue","title":"QUEUE e DEQUEUE","text":"<p>QUEUE   = FIFO, pronounced \"kiu\" DEQUEUE = FIFO e LIFO, pronounced \"deck\", (Double Ended Queue)</p> <pre><code>Queue operations have different names and implementing methods:\n\n    - INSERT: add or offer\n    - REMOVE: remove or poll\n    - CHECK: element or peek\n    - STACK (dequeue only): push (=addFirst) and pop (=removeFirst)\n\nThe order is usually FIFO but the priority queues use a comparator and the dequeues can also use a LIFO order.\n\nAll queues and dequeues do not allow null values.\n</code></pre> <p>ConcurrentLinkedQueue, ConcurrentLinkedDequeue</p> <pre><code>They're both multi-producer and multi-consumer. The queue length is unbounded (=without limits).\nThe size() method has a high cost because all items should be iterated. Moreover, there is no meaning to count the items in a queue.\nThe iterator is weakly consistent.\nThe ConcurrentLinkedDequeue can be used as a thread-safe stack.\nThe ConcurrentLinkedQueue can be used as a thread-safe LinkedList but the method get(index) should not be used.\n</code></pre> <p>ArrayDeque</p> <pre><code>It is not thread-safe; the underlying structure is an array.\nFast, most of the operation costs have constant time ( O(1)).\nNulls not accepted. Fail-fast operator.\n</code></pre> <p>Blocking queues and dequeues</p> <pre><code>They block the write operations if the queue is full and the read operations if the queue is empty.\nThey are thread-safe with timeouts that can prevent forever blocking.\nNulls not accepted.\n</code></pre> <p>LinkedBlockingQueue e LinkedBlockingDeque </p> <pre><code>They are optimised for the single-producer single-consumer use case. In other cases, they can be slower than the other queues.\nIt is possible to define a limit in the queue length.\n</code></pre> <p>ArrayBlockingQueue</p> <pre><code>Bounded queue based on a fixed length array. Bounded\nMemory efficient, it uses a single lock so there is contention when there are more producers and/or consumers.\n</code></pre> <p>DelayQueue</p> <pre><code>Not so used in the JDK.\nThe elements in the queue are visible to the consumer only after a delay.\n</code></pre> <p>SynchronousQueue</p> <pre><code>A synchronous queue with max one element. It is used to implement the cachedThreadPool.\n\nA blocking queue in which each inserts operation must wait for a corresponding remove operation by another thread, and vice versa.\n\nA synchronous queue does not have any internal capacity, not even a capacity of one. You cannot peek at a synchronous queue because an element is only present when you try to remove it; you cannot insert an element (using any method) unless another thread is trying to remove it; you cannot iterate as there is nothing to iterate.\n\nThe head of the queue is the element that the first queued inserting thread is trying to add to the queue; if there is no such queued thread then no element is available for removal and poll() will return null.\n\nFor purposes of other Collection methods (for example, contains), a SynchronousQueue acts as an empty collection. This queue does not permit null elements.\n</code></pre> <p>LinkedTransferQueue </p> <pre><code>Introduced in a JAVA 7 beta version to be used in the ForkJoinPool but then no more used.\n</code></pre> <p>PriorityQueue and PriorityBlockingQueue</p> <pre><code>These are unbounded queues that sort the elements in the queue.\nThe PriorityQueue is not thread-safe while the PriorityBlockingQueue is.\nThe priority is based on the natural order (if items implement Comparable) or via a comparator.\n</code></pre>","tags":["java","jdk","data structures","list","set","map","queue","iterator"]},{"location":"languages/java/maven/","title":"APACHE MAVEN","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","maven","build"]},{"location":"languages/java/maven/#commands","title":"Commands","text":"<p>Download the used dependencies' JavaDoc</p> <pre><code>  mvn dependency:resolve -Dclassifier=javadoc\n</code></pre> <p>Download the used dependencies' source code</p> <pre><code>  mvn dependency:resolve -Dclassifier=sources\n</code></pre> <p>Analyse the project dependencies reporting the missing and the unused</p> <pre><code>  mvn dependency:analyze\n</code></pre> <p>Show all dependencies</p> <pre><code>  mvn dependency:resolve\n  mvn dependency:tree\n</code></pre> <p>Runs Jetty</p> <pre><code>  mvn package jetty:run -Dmaven.test.skip=true   -DskipTests\n</code></pre> <p>Create local dependency</p> <pre><code>  mvn install:install-file -DgroupId=com.oracle -DartifactId=ojdbc14 -Dversion=10.2.0.2.0 -Dpackaging=jar -Dfile=ojdbc.jar -DgeneratePom=true\n</code></pre> <p>Run a main class</p> <pre><code>  mvn exec:java -Dexec.mainClass=\"com.example.Main\"\n</code></pre> <p>Release</p> <pre><code>  mvn release:prepare -DdryRun=true -Darguments=\"-DskipTests\"\n  mvn release:perform\n  mvn release:clean\n\n  mvn release:update-versions -DautoVersionSubmodules=true\n  mvn --batch-mode release:update-versions -DdevelopmentVersion=1.2.0-SNAPSHOT\n</code></pre> <p>Use a specific plugin version</p> <pre><code>  mvn groupID:artifactID:version:goal\n  mvn org.apache.maven.plugins:maven-deploy-plugin:2.8.2:deploy\n</code></pre> <p>Show the maven configuration (from the <code>.settings.xml</code> file)</p> <pre><code>  mvn help:effective-settings\n</code></pre> <p>System dependency <pre><code>      &lt;dependency&gt;\n         &lt;groupId&gt;ldapjdk&lt;/groupId&gt;\n         &lt;artifactId&gt;ldapjdk&lt;/artifactId&gt;\n         &lt;scope&gt;system&lt;/scope&gt;\n         &lt;version&gt;1.0&lt;/version&gt;\n         &lt;systemPath&gt;${basedir}\\src\\lib\\ldapjdk.jar&lt;/systemPath&gt;\n      &lt;/dependency&gt;\n</code></pre></p>","tags":["java","maven","build"]},{"location":"languages/java/maven/#passwords","title":"Passwords","text":"<p>Read instructions</p> <pre><code>  mvn --encrypt-master-password &lt;password&gt;\n  mvn --encrypt-password &lt;password&gt;\n</code></pre>","tags":["java","maven","build"]},{"location":"languages/java/mockito/","title":"MOCKITO","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","test","mockito","mock"]},{"location":"languages/java/mockito/#annotations","title":"Annotations","text":"<pre><code>@RunWith(MockitoJUnitRunner.class)\n\n@Mock private XXX xxx;\n\n@InjectMocks  private XXX sut;\n</code></pre>","tags":["java","test","mockito","mock"]},{"location":"languages/java/mockito/#capturing","title":"Capturing","text":"<pre><code>@Captor ArgumentCaptor&lt;XXX&gt; captorXxx;\n@Captor ArgumentCaptor&lt;AuctionEvent&gt; arg;\n\ncaptorXxx.capture()\ncaptorXxx.getValue()\n</code></pre>","tags":["java","test","mockito","mock"]},{"location":"languages/java/mockito/#testing-an-abstract-class","title":"Testing an abstract class","text":"<pre><code>@RunWith(MockitoJUnitRunner.class)\npublic class DerivatiProcessTest {\n\n    private DerivatiProcess sut;\n\n    @Before\n    public void setUp() {\n        sut = Mockito.mock(DerivatiProcess.class, Mockito.CALLS_REAL_METHODS);\n    }\n</code></pre>","tags":["java","test","mockito","mock"]},{"location":"languages/java/openj9/","title":"OPEN J9","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","jdk","ibm","j9","openj9"]},{"location":"languages/java/openj9/#non-heap-memory","title":"Non heap memory","text":"<ul> <li>JIT code cache   (def 256M su 64bit)</li> <li>JIT data cache   (def = Xmx ?)</li> <li>Class storage</li> <li>Misc non heap</li> </ul>","tags":["java","jdk","ibm","j9","openj9"]},{"location":"languages/java/openj9/#heap-memory","title":"Heap memory","text":"<p>Global</p> <pre><code>Initial memory (Xms)\nMax memory     (Xmx)\n</code></pre> <p>With the <code>gencon</code> garbage collector, the heap memory is divided in:</p> <pre><code>Nursery / New\n  initial     Xmns    default: 25% di Xms\n  max         Xmnx    default: 25% di Xmx\n\nTenure (Old)\n  initial     Xmos    default: 75% di Xms\n  max         Xmox    default: = Xmx\n</code></pre> <p>Note that maxTenure + maxNursery &gt; maxHeap !!</p>","tags":["java","jdk","ibm","j9","openj9"]},{"location":"languages/java/openj9/#using-dockerk8s","title":"Using Docker/K8S","text":"<p>Set the max heap to the 70% of the container memory</p> <p>Example with a container limited to 512 MB</p> <pre><code>heap      = 356 MB\nnursery   =  90 MB\ntenure    = 448 MB\nnon heap  =  64 MB (estimated)\n</code></pre> <p>OPTIONS</p> <pre><code>-cp|classpath &lt;class search path of directories and zip/jar files&gt; A : separated list of directories, JAR archives, and ZIP archives to search for class files.\n\n-D&lt;name&gt;=&lt;value&gt;                  set a system property\n\n-verbose:[class|gc|jni]                   enable verbose output\n\n-version      print product version and exit\n-version:&lt;value&gt;                   require the specified version to run\n-showversion  print product version and continue\n\n-jre-restrict-search | -no-jre-restrict-search                   include/exclude user private JREs in the version search\n\n-? -help      print this help message\n\n-X            print help on non-standard options\n\n-ea|enableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;]    enable assertions with specified granularity\n\n-da|disableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;]   disable assertions with specified granularity\n\n-esa | -enablesystemassertions                   enable system assertions\n-dsa | -disablesystemassertions                  disable system assertions\n\n-agentlib:&lt;libname&gt;[=&lt;options&gt;] load native agent library &lt;libname&gt;, e.g. -agentlib:hprof see also, -agentlib:jdwp=help and -agentlib:hprof=help\n\n-agentpath:&lt;pathname&gt;[=&lt;options&gt;]   load native agent library by full pathname\n\n-javaagent:&lt;jarpath&gt;[=&lt;options&gt;]    load Java programming language agent, see java.lang.instrument\n\n-splash:&lt;imagepath&gt;\n              show splash screen with specified image\n</code></pre>","tags":["java","jdk","ibm","j9","openj9"]},{"location":"languages/java/openj9/#non-standard-options","title":"Non standard options","text":"<pre><code>java -X\nThe following options are non-standard and subject to change without notice.\n\n  -Xcompressedrefs\n  This command-line option enables the compressed references feature. When the JVM is launched with this command line option, \n  it would use 32-bit wide memory references to address the heap. This feature can be used up to a certain heap size (around \n  29GB depending on the platform), controlled by -Xmx parameter. \n\n  -Xnocompressedrefs\n   These command-line options explicitly disable the compressed references feature. When the JVM is launches with this command \n   line option it uses full 64-bit wide memory references to address the heap. This option can be used by the user to override \n   the default enablement of pointer compression, if needed. \n\n  -Xbootclasspath:&lt;path&gt;    set bootstrap classpath to &lt;path&gt; \n  -Xbootclasspath/p:&lt;path&gt;  prepend &lt;path&gt; to bootstrap classpath \n  -Xbootclasspath/a:&lt;path&gt;  append &lt;path&gt; to bootstrap classpath \n\n  -Xrun&lt;library&gt;[:options]  load native agent library\n                            (deprecated in favor of -agentlib)\n\n  -Xshareclasses[:options]  Enable class data sharing (use help for details)\n\n  -Xint           run interpreted only (equivalent to -Xnojit -Xnoaot)\n  -Xnojit         disable the JIT\n  -Xnoaot         do not run precompiled code\n  -Xquickstart    improve startup time by delaying optimizations\n  -Xfuture        enable strictest checks, anticipating future default\n\n  -verbose[:(class|gcterse|gc|dynload|sizes|stack|debug)]\n\n  -Xtrace[:option,...]  control tracing use -Xtrace:help for more details\n\n  -Xcheck[:option[:...]]  control checking use -Xcheck:help for more details\n\n  -Xhealthcenter  enable the Health Center agent\n\n  -Xdiagnosticscollector enable the Diagnotics Collector\n\n  -XshowSettings                show all settings and continue\n  -XshowSettings:all            show all settings and continue\n  -XshowSettings:vm             show all vm related settings and continue\n  -XshowSettings:properties     show all property settings and continue\n  -XshowSettings:locale         show all locale related settings and continue\n\nArguments to the following options are expressed in bytes.\nValues suffixed with \"k\" (kilo) or \"m\" (mega) will be factored accordingly.\n\n  -Xmca&lt;x&gt;        set RAM class segment increment to &lt;x&gt;\n  -Xmco&lt;x&gt;        set ROM class segment increment to &lt;x&gt;\n\nNew Space\n  -Xmn&lt;x&gt;         set initial/maximum new space size to &lt;x&gt;\n  -Xmns&lt;x&gt;        set initial new space size to &lt;x&gt;\n  -Xmnx&lt;x&gt;        set maximum new space size to &lt;x&gt;\n\nOld space\n  -Xmo&lt;x&gt;         set initial/maximum old space size to &lt;x&gt;\n  -Xmos&lt;x&gt;        set initial old space size to &lt;x&gt;\n  -Xmox&lt;x&gt;        set maximum old space size to &lt;x&gt;\n  -Xmoi&lt;x&gt;        set old space increment to &lt;x&gt;\n\nHeap Size\n  -Xms&lt;x&gt;         set initial memory size to &lt;x&gt;\n  -Xmx&lt;x&gt;         set memory maximum to &lt;x&gt;\n\nStack Size\n  -Xmso&lt;x&gt;        set OS thread stack size to &lt;x&gt;\n  -Xiss&lt;x&gt;        set initial java thread stack size to &lt;x&gt;\n  -Xssi&lt;x&gt;        set java thread stack increment to &lt;x&gt;\n  -Xss&lt;x&gt;         set maximum java thread stack size to &lt;x&gt;\n\n  -Xmr&lt;x&gt;         set remembered set size to &lt;x&gt;\n  -Xmrx&lt;x&gt;        set maximum size of remembered set to &lt;x&gt;\n  -Xscmx&lt;x&gt;       set size of new shared class cache to &lt;x&gt;\n  -Xscminaot&lt;x&gt;   set minimum shared classes cache space reserved for AOT data to &lt;x&gt;\n  -Xscmaxaot&lt;x&gt;   set maximum shared classes cache space allowed for AOT data to &lt;x&gt;\n  -Xmine&lt;x&gt;       set minimum size for heap expansion to &lt;x&gt;\n  -Xmaxe&lt;x&gt;       set maximum size for heap expansion to &lt;x&gt;\n\n  -Xlp                          enable large page support\n  -Xrunjdwp:&lt;options&gt;           enable debug, JDWP standard options\n  -Xjni:&lt;options&gt;               set JNI options \n</code></pre>","tags":["java","jdk","ibm","j9","openj9"]},{"location":"languages/java/openj9/#garbage-collector","title":"Garbage collector","text":"<pre><code>-Xgcpolicy\nThe IBM virtual machine for Java provides four policies for garbage collection. Each policy provides unique benefits.\nNote While each policy provides unique benefits, for WebSphere Application Server Version 8.0 and later, gencon is \nthe default garbage collection policy. Previous versions of the application server specify that optthruput is the \ndefault garbage collection policy.\n\n    gencon is the default policy. This policy works with the generational garbage collector. The generational scheme \n    attempts to achieve high throughput along with reduced garbage collection pause times. To accomplish this goal, \n    the heap is split into new and old segments. Long lived objects are promoted to the old space while short-lived \n    objects are garbage collected quickly in the new space. The gencon policy provides significant benefits for many \n    applications. However, it is not suited for all applications, and is typically more difficult to tune.\n\n    optthruput provides high throughput but with longer garbage collection pause times. During a garbage collection, all \n    application threads are stopped for mark, sweep, and compaction, when compaction is needed.\n\n    optavgpause is the policy that reduces garbage collection pause time by performing the mark and sweep phases of \n    garbage collection while an application is running. This policy causes a small performance impact to overall throughput.\n\n    subpool is a policy that increases performance on multiprocessor systems, that commonly use more than 8 processors. \n    This policy is only available on IBM System i\u00ae System p and System z\u00ae processors. The subpool policy is similar to \n    the gencon policy except that the heap is divided into subpools that provide improved scalability for object allocation.\n\n  -Xminf&lt;x&gt;       minimum percentage of heap free after GC\n  -Xmaxf&lt;x&gt;       maximum percentage of heap free after GC\n  -Xgcthreads&lt;x&gt;                set number of GC threads\n  -Xnoclassgc                   disable dynamic class unloading\n  -Xclassgc                     enable dynamic class unloading\n  -Xalwaysclassgc               enable dynamic class unloading on every GC\n  -Xnocompactexplicitgc         disable compaction on a system GC\n  -Xcompactexplicitgc           enable compaction on every system GC\n  -Xcompactgc                   enable compaction\n  -Xnocompactgc                 disable compaction\n</code></pre>","tags":["java","jdk","ibm","j9","openj9"]},{"location":"languages/java/portlet-portal/","title":"PORTLET - PORTAL","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","portal","portlet","JSR186","JSR286"]},{"location":"languages/java/portlet-portal/#versions","title":"Versions","text":"<ul> <li> <p>Portlet 1.0 (JSR 168) does not provide a native solution to let portlets communicate with each other. One alternative is to use shared data in the web session.</p> </li> <li> <p>Portlet 2.0 (JSR 286) defines events and public render parameters</p> </li> </ul>","tags":["java","portal","portlet","JSR186","JSR286"]},{"location":"languages/java/portlet-portal/#events","title":"Events","text":"<ul> <li>Events are defined in the <code>portlet.xml</code> file, in the <code>event-definition</code> section</li> <li>Every portlet defines:<ul> <li>the emitted (generated) events (supported-publishing-event)</li> <li>the listened events (supported-processing-event)</li> </ul> </li> <li>The events are generated with the <code>setEvent</code> method</li> <li>The <code>EventPortlet.processEvent</code> method handles the listened events</li> <li>The event names are <code>QNames</code></li> </ul>","tags":["java","portal","portlet","JSR186","JSR286"]},{"location":"languages/java/slf4j/","title":"SLF4J","text":"<p>Last update: 2 Oct 2022</p>","tags":["java","slf4j","logging"]},{"location":"languages/java/slf4j/#examples","title":"Examples","text":"<p>How to define a logger</p> <pre><code>private final static Logger logger = LoggerFactory.getLogger(&lt;nome_classe&gt;.class);\nprivate final static XLogger logger = XLoggerFactory.getXLogger(&lt;nome_classe&gt;.class);\n</code></pre> <p>How to log to info and debug levels:</p> <pre><code>logger.info(\"Message without parameters\");\n\n// no need to create an Object array\nlogger.info(\"Messaggio with many parameters {} {} {}\", arg1, arg2, agr3,...);  \n</code></pre> <p>Note: avoid using the <code>trace</code> level. Prefer the <code>debug</code> level</p> <p>How to log an exception</p> <pre><code>logger.error(\"message\", exception)\nlogger.error(\"message with an argument {}\", arg1, exception)\n</code></pre> <p>Avoid:</p> <pre><code>logger.error(e.getMessage(), exception);\nlogger.catching(exception);\n</code></pre>","tags":["java","slf4j","logging"]},{"location":"languages/java/spring-boot/","title":"Spring Boot","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","spring","boot","actuactor"]},{"location":"languages/java/spring-boot/#actuactor","title":"Actuactor","text":"<p>Dependency: <code>spring-boot-starter-actuactor</code></p> <p>Endpoints:</p> <ul> <li>autoconfig</li> <li>info</li> <li>configprops</li> <li>dump</li> <li>metrics</li> <li>health</li> <li>heapdump</li> <li>mappings</li> <li>beans</li> <li>trace</li> <li>env</li> </ul>","tags":["java","spring","boot","actuactor"]},{"location":"languages/java/spring-boot/#configuration-and-spring-profiles","title":"Configuration and Spring profiles","text":"<p>The files <code>application.properties</code> and <code>application.yml</code> contains the parameters to configure Spring Boot and the Spring components.</p> <p>The JVM variable (option <code>-D</code>) <code>spring.profiles.active</code> mark which is the active Spring profile. The profile defines which properties file should be read. The main file (application.*) is always read. Then, the file <code>application-&lt;profile&gt;.*</code> is read.</p> <p>We can create a data class with a @Component annotation with the @ConfigurationProperties which collect a set of configuration parameters.</p> <pre><code>    Environment variable: SPRING_PROFILES_ACTIVE\n    JVM variable:         spring.profiles.active\n</code></pre>","tags":["java","spring","boot","actuactor"]},{"location":"languages/java/spring-boot/#logging","title":"Logging","text":"<p>The spring boot started dependencies like starter-web depends on the starter-logging that uses SLF4J and LogBack and the redirectors from CommonsLogging, JUL and Log4J.</p>","tags":["java","spring","boot","actuactor"]},{"location":"languages/java/spring-security/","title":"Spring Security","text":"<p>Last update: 7 Oct 2022</p> <p>Spring security:</p> <ul> <li>is a set of servlet filters that help you add authentication and authorization to your web application.</li> <li>auto-generates login/logout pages</li> <li>protects against common exploits like CSRF.</li> </ul> <p>Spring Security Guide</p>","tags":["java","spring","security"]},{"location":"languages/java/spring-security/#main-modules","title":"Main modules","text":"<p>Without Spring Boot</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>With Spring Boot</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>","tags":["java","spring","security"]},{"location":"languages/java/spring-security/#web-security","title":"Web security","text":"<p>Example</p> <pre><code>@Configuration\n@EnableWebSecurity\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http\n        .authorizeRequests()\n\n            // unauthenticated\n            .mvcMatchers(\"/\", \"/home\").permitAll()\n\n            // authenticated with specific authority\n            .mvcMatchers(\"/admin\").hasAuthority(\"ROLE_ADMIN\")\n            .mvcMatchers(\"/callcenter\").hasAnyAuthority(\"ROLE_ADMIN\", \"ROLE_CALLCENTER\")\n\n            // authenticated with specific role\n            .mvcMatchers(\"/admin\").hasRole(\"ADMIN\")\n            .mvcMatchers(\"/callcenter\").hasAnyRole(\"ADMIN\", \"CALLCENTER\")\n\n            // using SpEL\n            .mvcMatchers(\"/admin\")\n                .access(\"hasRole('admin') and hasIpAddress('192.168.1.0/24') and @myCustomBean.checkAccess(authentication,request)\")\n\n            // all other request requires authentication without specific role\n            .anyRequest().authenticated()\n\n            .and()\n        .formLogin()  // enable the login page\n            .loginPage(\"/login\") // custom login page\n            .permitAll()\n            .and()\n        .logout() // enable the Spring generated logout page\n            .permitAll()\n        .and()\n        .httpBasic(); // enable basic authentication\n    }\n}\n</code></pre> <p>For further info on <code>SpEL</code> here</p> <p>Matchers:</p> <pre><code>mvcMatchers: (preferred)\n    '/a'    : match '/a' and '/a/'\n    '/a/*'  : match '/a/xx' but not '/a' and not '/a/b/c'\n    '/a/**' : match '/a', '/a/b', '/a/b/c'\nantMatchers: old\n    The main difference is on the '/a' matching: the mvcMatcher match '/a/' while the antMatcher not.\nregexMacthers: for more complex cases\n</code></pre>","tags":["java","spring","security"]},{"location":"languages/java/spring-security/#authentication","title":"Authentication","text":"<p>There are three possible cases.</p> <p>Case 1</p> <p>The user directory is handled by the application, using a database for example.</p> <p>Two beans should be implemented: * UserDetailsService which finds the user from their username * PasswordEncoder which defines how the password are encrypted</p> <p>The <code>UserDetails</code> interface is the user profile. The class <code>org.springframework.security.core.userdetails.User</code> is a concrete implementation that can be used.</p> <p>Some concrete implementations of the <code>UserDetailsService</code> interface are the classes <code>InMemoryUserDetailsManager</code> and <code>JdbcUserDetailsManager</code>.</p> <p>The most used concrete implementation of the <code>PasswordEncoder</code> interface is the <code>BCryptPasswordEncoder</code> class. The class <code>PasswordEncoderFactories.createDelegatingPasswordEncoder()</code> use a prefix to understand which hashing algorithm should be used.</p> <p>Case 2</p> <p>The user directory is handled by an external application, a LDAP server for example.</p> <p>In this case, we should implement the <code>AuthenticationProvider</code> interface.</p> <p>Example:</p> <p><pre><code>public class CustomAuthenticationProvider implements AuthenticationProvider {\n\n    Authentication authenticate(Authentication authentication) throws AuthenticationException {\n\n        String username = authentication.getPrincipal().toString();\n        String password = authentication.getCredentials().toString();\n\n        // Call the external service to authenticate the user and return the profile\n        User user = callAnotherAutheticationService(username, password);\n\n        if (user == null) {\n            throw new AuthenticationException(\"could not login\");\n        }\n        return new UserNamePasswordAuthenticationToken(\n            user.getUsername(), \n            user.getPassword(), \n            user.getAuthorities()); \n    }\n        // other method ignored\n}\n</code></pre> Case 3</p> <p>When we use OAuth2 / OpenID </p>","tags":["java","spring","security"]},{"location":"languages/java/spring-security/#authorization","title":"Authorization","text":"<p>Definitions:</p> <ul> <li>Authority: it is usually a string like \"USER\", \"ADMIN\", or \"ROLE_GUEST\"</li> <li>Role: it is an authority with the <code>ROLE_</code> prefix</li> </ul> <p>Authority can be also a role if it has the ROLE_ prefix.</p> <p>In general, the role is a high-level concept, collecting several authorizations described as authorities. The authorities are mapped to specific actions the user can perform.</p> <p>Example:</p> <pre><code>Authority: read, write, update, delete\nRole Reader  --&gt; authority read\nRole Manager --&gt; authority read, write, update\nRole Admin   --&gt; authority read, write, update, delete\n</code></pre> <p>Authority:</p> <pre><code>Interface   GrantedAuthority \nClass       SimpleGrantedAuthority\n</code></pre> <p>The authorization checks usually start at the web security level, where each URL is mapped to roles and authorities. </p> <p>In addition, the authorization checks can be also implemented at the method level (who can call the method), at every level of the application. This is required for non-web applications.</p> <p>The security at the method level should be enabled:</p> <pre><code>@Configuration\n@EnableGlobalMethodSecurity(\n    prePostEnabled = true, // enable the Spring @PreAuthorize e @PostAuthorize annotations\n    securedEnabled = true, // enable the Spring @Secured annotation\n    jsr250Enabled = true) // enable the Java standard @RolesAllowed annotation   \npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n    ....\n}\n</code></pre> <p>The <code>@Secured</code> and <code>@RolesAllowed</code> annotations are equivalent and they require roles.</p> <p>The <code>@PreAuthorize</code> and <code>@PostAuthorize</code> annotations can express more complex conditions thanks to the SpEL.</p> <p>Examples:</p> <pre><code>@Secured(\"ROLE_ADMIN\")\n\n@RolesAllowed(\"ADMIN\")\n\n@PreAuthorize(\"isAnonymous()\")\n\n@PreAuthorize(\"#contact.name == principal.name\")\nvoid aMethod(Contact contact) {...}\n\n@PreAuthorize(\"#name == authentication.principal.username\")\nvoid aMethod(String name) {...}\n\n@PreAuthorize(\"ROLE_ADMIN\")\n</code></pre> <p>The <code>@PostAuthorize</code> annotation can be used when the access control depends on the value returned by the method. The method is always executed but its returned value is transmitted only if the PostAuthorize succeed.</p> <p>Example:</p> <pre><code>@PostAuthorize(\"returnObject.roles.contains('reader')\")\n</code></pre> <p>If the authorization condition is too complex to be easily expressed with a SpEL string, the condition can be implemented by a class that extends from the <code>PermissionEvaluator</code> interface. The class should be registered before using it.</p> <pre><code>@PostAuthorize(\"hasPermission(returnObject, 'ROLE_admin')\")\n</code></pre>","tags":["java","spring","security"]},{"location":"languages/java/spring-security/#cross-site-request-forgery-csrf","title":"Cross-Site-Request-Forgery: CSRF","text":"<p>The <code>CSRFFilter</code> filter looks for a CSRF token in every POST/PUT/DELETE request.</p> <p>The CSRF token is usually a hidden form field or a cookie or an HTTP header.</p> <p>The CSRF token value is generated by Spring Security and saved in the HTTP session.</p> <p>If you are only providing a stateless REST API where CSRF protection does not make any sense,  you would completely disable CSRF protection</p> <pre><code>http.csrf().disable();\n</code></pre>","tags":["java","spring","security"]},{"location":"languages/java/spring-security/#spring-security-with-spring-mvc","title":"Spring Security with Spring MVC","text":"<p>When using Spring Security and Spring MVC:</p> <ol> <li>We can use the mvcMatchers instead of the andMatchers.</li> <li> <p>We can inject the user (principal) in the MVC controllers:</p> <pre><code>@RequestMapping(\"/xxx\")\npublic String xxxx(@AuthenticationPrincipal CustomUser customUser\n</code></pre> </li> <li> <p>Inject the CSRF token</p> </li> </ol>","tags":["java","spring","security"]},{"location":"languages/java/streams/","title":"Streams in Java","text":"<p>Last update: 14 Nov 2022</p> <p>Certification topics:</p> <ul> <li>Use java streams to filter, transform and process data</li> <li>Perform decomposition and reduction, including grouping and partitioning on sequential and parallel streams</li> </ul> <p>Links:</p> <ul> <li>Package stream JavaDoc</li> </ul>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/streams/#interfaces","title":"Interfaces","text":"<ul> <li>java.util.stream.BaseStream</li> <li>java.util.stream.Collector</li> <li>java.util.stream.DoubleStream</li> <li>java.util.stream.IntStream</li> <li>java.util.stream.LongStream</li> <li>java.util.stream.Stream</li> </ul>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/streams/#classes","title":"Classes","text":"<ul> <li>java.util.stream.Collectors</li> <li>java.util.stream.StreamSupport</li> </ul>","tags":["java","exceptions","best practice","antipattern"]},{"location":"languages/java/vavr/","title":"VAVR","text":"<p>Last update: 7 Oct 2022</p>","tags":["java","vavr"]},{"location":"languages/java/vavr/#link","title":"LINK","text":"<ul> <li>SITE:     https://www.vavr.io/</li> <li>JAVADOC:  https://www.javadoc.io/doc/io.vavr/vavr/0.10.3/index.html</li> <li>MANUAL:   https://www.vavr.io/vavr-docs/</li> </ul>","tags":["java","vavr"]},{"location":"languages/java/vavr/#either","title":"EITHER","text":"<p>Either.sequence</p> <p>Reduces many Eithers into a single Either by transforming an Iterable&gt; into a Either, Seq\\&gt;. <p>If any of the given Eithers is a Either.Left then sequence returns a Either.Left containing a non-empty Seq of all left values.</p> <p>If none of the given Eithers is a Either.Left then sequence returns a Either.Right containing a (possibly empty) Seq of all right values. </p> <pre><code>Either&lt;Integer, String&gt; e1 = Either.right(\"ok1\");\nEither&lt;Integer, String&gt; e2 = Either.right(\"ok2\");\nEither&lt;Integer, String&gt; e3 = Either.left(3);\nEither&lt;Integer, String&gt; e4 = Either.left(4);\n\nEither&lt;Seq&lt;Integer&gt;,Seq&lt;String&gt; e10 = Either.sequence(List.of(e1, e2)); // e10 = rigth(Seq(\"ok1\", \"ok2\")\n\nEither&lt;Seq&lt;Integer&gt;,Seq&lt;String&gt; e11 = Either.sequence(List.of(e1, e2, e3, e4)); // e10 = left(Seq(3, 4)\n</code></pre> <p>Either.sequenceRigth</p> <p>Works like <code>Either.sequence</code> if there are no left either.</p> <p>If there is at least one left, it returns the first one, not all of them as <code>Either.sequence</code> does.</p>","tags":["java","vavr"]},{"location":"methodologies/","title":"Methodologies","text":"<p>Articles and personal notes on the software development methodologies I use, or I would like to use ;-)</p> <ul> <li>Refactoring</li> <li>Domain Driven Design</li> <li>C4 Model</li> </ul>","tags":["software development","methodologies","design patterns"]},{"location":"methodologies/c4-model/","title":"C4 Model","text":"<p>Last update: 22 Nov 2024</p>","tags":["software","methodologies","model","c4"]},{"location":"methodologies/c4-model/#links","title":"Links","text":"<ul> <li>c4model.com</li> <li>Visualizing software architecture with the C4 Model (Video) </li> </ul>","tags":["software","methodologies","model","c4"]},{"location":"methodologies/c4-model/#introduction","title":"Introduction","text":"<p>The C4 model has been created by S. Brown in 2006. It is a graphical and hierarchical representation of a software system. AT the lower levels, the C4 model use UML diagrams.</p> <p>The C4 in the name means Context, Containers, Components and Code.</p> <p>The upper level (1) is the <code>Context</code> view that shows how the software system interacts with the users and the external systems.</p> <p>Below, there is the level 2, the <code>Container</code> diagram. Here, container means an application, a data store, an API. It is not related to the Docker containers.</p> <p>A container can be a relational database, a Spring Boot application.</p> <p>Below, there is the level 3, the <code>Component</code> diagram. The components are the parts of a container. For example, the main web controllers in a Spring Boot application.</p> <p>The lower level is level 4, the <code>Code</code>, where UML class and ER diagrams can describe the elements inside a component.</p> <p>S. Brown does not suggest drawing level 4 diagrams. IDEs can do it from the code. See the video in the links.</p> <p>The C4 model can be considered as the static structure for other models and diagrams:</p> <ul> <li>UML deployment diagrams (to connect C4 containers with the infrastructure)</li> <li>Infrastructure diagrams (physical, virtual, hardware, firewalls, routers, load balancers...)</li> <li>UML Entity relationship diagrams (ERD) for data</li> <li>UML sequence and collaboration diagrams to describe runtime and behavior</li> <li>Business process and workflow</li> </ul>","tags":["software","methodologies","model","c4"]},{"location":"methodologies/c4-model/#connections","title":"Connections","text":"<p>S. Brown suggest:</p> <ul> <li>using one direction only lines (with only one arrow head) to make more evident the relations between the two connected elements;</li> <li>describing with a short phrase the intent of the connection. Ex. \"Make API calls using [JSON/HTTPS]\" (instead of \"uses\")</li> </ul>","tags":["software","methodologies","model","c4"]},{"location":"methodologies/ddd/","title":"Domain Driven Design (DDD)","text":"<p>Last update: 19 Oct 2024</p> <p>Domain-Driven Design (DDD) is a software development approach that emphasizes modeling a software system around the business domain it serves.</p> <p>It focuses on understanding the core business concepts and processes, and then designing the software to reflect those concepts accurately.</p> <p>Key principles of DDD include:</p> <ul> <li><code>Ubiquitous Language</code>: A shared vocabulary used by developers, business analysts, and domain experts to communicate about the business domain.</li> <li><code>Bounded Contexts</code>: Defining boundaries around different parts of the system that have distinct responsibilities and can be modeled independently.</li> <li><code>Entities</code>: Entities represent the fundamental building blocks of your business domain. They have a unique identity that persists over time, even if their attributes change. This means that an entity remains the same object, even if its properties are updated. Entities have a state and a behavior. Their life cycle (creation, update, deletion) is managed by the application.</li> <li><code>Aggregates</code>: Grouping related entities together into a cohesive unit that can be treated as a single object.</li> <li><code>Value Objects</code>: Objects that represent values rather than entities, such as money or color.</li> <li><code>Domain Events</code>: Capturing significant occurrences within the domain that can be used to trigger actions or updates in other parts of the system.</li> </ul> <p>By following these principles, DDD aims to create software that is more aligned with the business, easier to understand, and more maintainable.</p>","tags":["software","methodologies","ddd","domain driven design"]},{"location":"methodologies/ddd/#books-and-references","title":"Books and references","text":"<ul> <li>\"Domain-Driven Design: Tackling Complexity in the Heart of Software Systems\" (Eric Evans)</li> <li>\"Implementing Domain-Driven Design\" (Vaughn Vernon)</li> <li>\"Domain-Driven Design Distilled\" (Vaughn Vernon)</li> </ul>","tags":["software","methodologies","ddd","domain driven design"]},{"location":"methodologies/ddd/#internal-structure","title":"Internal structure","text":"<p>The suggested internal structure for a DDD project is the following:</p>","tags":["software","methodologies","ddd","domain driven design"]},{"location":"methodologies/ddd/#core-domain-layer","title":"Core Domain Layer","text":"<ul> <li>Entities: These represent the fundamental building blocks of your business domain. They should be immutable and encapsulate the domain logic.</li> <li>Value Objects: Represent values rather than entities. They are often immutable and should be compared by value, not reference.</li> <li>Aggregates: Group related entities together into a cohesive unit. They have a root entity that is responsible for ensuring the data consistency of the aggregate.</li> <li>Domain Services: Handle complex business rules that don't fit naturally within an entity or value object.</li> <li>Domain Events: Capture significant occurrences within the domain that can be used to trigger actions or updates in other parts of the system.</li> </ul>","tags":["software","methodologies","ddd","domain driven design"]},{"location":"methodologies/ddd/#application-layer","title":"Application Layer","text":"<ul> <li>Application Services: Facilitate interactions between the user interface or other external systems and the domain layer. They orchestrate the flow of data and business logic.</li> </ul>","tags":["software","methodologies","ddd","domain driven design"]},{"location":"methodologies/ddd/#infrastructure-layer","title":"Infrastructure Layer","text":"<ul> <li>Persistence: Handles the storage and retrieval of domain objects. In a Java project, this can be implemented using frameworks like JPA or Hibernate.</li> <li>External Services: Interfaces with external systems or APIs.</li> <li>Utilities: Provides reusable utility classes for common tasks.</li> </ul>","tags":["software","methodologies","ddd","domain driven design"]},{"location":"methodologies/ddd/#userapi-interface-layer","title":"User/API Interface Layer","text":"<ul> <li>Presentation Layer (optional): Handles the user interface, such as web or mobile applications.</li> <li>API layer (optional): APIs and web-services exposed by the application.</li> </ul> <p>An example of a Java application package structure:</p> <pre><code>    src/main/java/com/yourcompany/\n        domain/\n            entities/\n            valueobjects/\n            aggregates/\n            services/\n            events/\n        application/\n            services/\n        infrastructure/\n            persistence/\n            externalservices/\n            utils/\n        ui/\n            web/\n            mobile/\n        api/\n</code></pre>","tags":["software","methodologies","ddd","domain driven design"]},{"location":"methodologies/refactoring/","title":"Refactoring","text":"<p>Last update: 14 Nov 2022</p> <p>Links:</p> <ul> <li>Refactoring by Martin Fowler (book)</li> </ul>","tags":["software","refactoring"]},{"location":"methodologies/refactoring/#bad-smells-or-what-we-should-avoid","title":"Bad Smells or what we should avoid","text":"<ul> <li>Duplicated code</li> <li>Too long methods</li> <li>Complex conditional statements</li> <li>Primitive obsession, when we use int and strings instead of value objects</li> <li>Indecent exposure, when we expose local methods and classes</li> <li>Solution sprawl is a solution distributed across different classes</li> <li>Alternative classes with different interfaces, when the same classes implement different interfaces and not just one</li> <li>Lazy classes: classes that do nothing</li> <li>Large (God) classes: classes that do too much</li> <li>Switch statements, when we use switches instead of proper polymorphism</li> <li>Combinatorial explosions, when the logic conditions are too complex</li> <li>Oddball solutions: different solutions to the same problem</li> </ul>","tags":["software","refactoring"]},{"location":"methodologies/refactoring/#compose-methods-from-fowlers-book","title":"Compose methods (from Fowler's book)","text":"<ul> <li>Extract Method (110) - move a code block to a separate method</li> <li>Inline Method (117) - replace a call to a method with the code of called method</li> <li>Inline Temp (119) - replace a temporary variable with its value</li> <li>Replace Temp With Query (120) - replace a temporary variable with a method call</li> <li>Introduce explaining variable (124) - use a temporary variable to better clarify code purpose</li> <li>Split temporary variable (128) - when a temporary variable is used more than one time</li> <li>Remove Assignment to Parameters (131)</li> <li>Replace Method with Method Object (135) - Replace a complex method with an object with a method</li> <li>Substitute algorithm (139) - Replace one algorithm with a better one</li> </ul>","tags":["software","refactoring"]},{"location":"methodologies/refactoring/#moving-features-between-objects-from-fowlers-book","title":"Moving Features between Objects (from Fowler's book)","text":"<ul> <li>Move method (142) - from one class to another one, where it is used most</li> <li>Move field (146) - from one class to another one, where it is used most</li> <li>Extract class (149) - when a class does too much (too many responsibilities)</li> <li>Inline class (154) - when a class does nothing</li> <li>Hide delegate (157) - introduce delegation methods to hide internal dependencies</li> <li>Remove Middle Man (160) - remove a class which delegates only</li> <li>Introduce foreign method (162) - when we want to add a method to an unmodifiable class</li> <li>Introduce local extension (164) - when we want to add many methods to an unmodifiable class</li> </ul>","tags":["software","refactoring"]},{"location":"methodologies/refactoring/#organizing-data-from-fowlers-book","title":"Organizing Data (from Fowler's book)","text":"<ul> <li>Self Encapsulate Field (171) - use the accessors method to access a field</li> <li>Replace data value with an object (175) - Replace a value with an object to add functionality</li> <li>Change value with reference (179) - change a value object to an object with identity</li> <li>Change reference to Value (183) - change an object with identity to a value object</li> <li>Replace array with Object (186) - when the array values have a different meaning</li> <li>Duplicate observed data (189) - split data from UI logic</li> <li>Change unidirectional association to Bidirectional (197) - make bidirectional the relation between two classes</li> <li>Change bidirectional association to unidirectional (200) - make unidirectional the relation between two classes</li> <li>Replace Magic number with symbolic constant (204) - create constants for relevant numbers in the code</li> <li>Encapsulate field (206) - replace a public field with a private field with accessor methods</li> <li>Encapsulate collections (208) - Replace accessor methods for collections fields with add and remove methods</li> <li>Replace record with data class (217) - Create an interface class with accessors for the record fields</li> <li>Replace type code with class (218) - Replace integer or string constants with classes or enums</li> </ul>","tags":["software","refactoring"]},{"location":"security/","title":"Security","text":"<p>Articles and personal notes on application security in the context of software development.</p> <ul> <li>Api Security</li> <li>Web Security</li> <li>Tokens</li> <li>OAuth and OpenID</li> </ul>","tags":["software development","security"]},{"location":"security/api-security/","title":"API security","text":"<p>Last update: 01 Aug 2023</p>","tags":["software","security","api"]},{"location":"security/api-security/#introduction","title":"Introduction","text":"<p>A reverse proxy or, better, an API gateway, is usually placed in front of an API server/service to:</p> <ul> <li>handle TLS/SSL terminations</li> <li>validate the credentials (user authentication, tokens validation)</li> <li>facade back-end services</li> </ul>","tags":["software","security","api"]},{"location":"security/api-security/#the-security-pipeline","title":"The security pipeline","text":"<p>Every API service should implement a security pipeline. The steps in the pipeline, in the order they process the upcoming requests, are the following:</p> <p>1) Rate limiting / throttling 2) Auditing/logging: track and log all requests and responses, assigning a unique ID to all inbound requests to enable inter-service tracing. 3) Implement/ verify the CORS mechanism 4) Check generic (non-endpoint specific) validation rules, like the \"Content-Type\" header value should be \"application/json\" for all endpoints. 5) Identification and authentication: user authentication and/or token validation. User sessions can be created after authentication. 6) Access control/authorization. If the user is authenticated (previous step), user permissions should be checked against the request requirements. See also the Principle of least privilege Endpoint-specific validation rules, input data validation</p>","tags":["software","security","api"]},{"location":"security/api-security/#api-key","title":"API KEY","text":"<p>The API key is an opaque secret generated server-side and shared with a confidential client.</p> <p>Every client request should include the API key in an HTTP header.</p> <p>API Key vs tokens:</p> <ol> <li>Using tokens, it is easier to have more granular and variable authorizations while API key authorizations are fixed and usually inherited by the creator of the key.</li> <li>Tokens (like JWT) can also be used for other purposes like the user profile (see the OIDC's ID Token)</li> <li>Tokens are also used in other authentication/authorization mechanisms like OAuth/OIDC.</li> <li>The API key does not have an explicit expiration date</li> <li>The API key should not be used when the client is public (not confidential) because the secret cannot be protected. </li> <li>The API key is opaque so it cannot be inspected.</li> <li>It's not easy to rotate API keys.</li> </ol> <p>On the other side, API keys are easy to use</p>","tags":["software","security","api"]},{"location":"security/api-security/#jwt-tokens-verification","title":"JWT Tokens verification","text":"<p>The application that receives a token should verify it before consumption.</p> <p>The first check to do is token integrity. The signature should be calculated again and compared with the signature included in the token.</p> <p>The second check is JWT validity: not expired (exp claim), not already used (jti claim).</p>","tags":["software","security","api"]},{"location":"security/oauth-openid/","title":"OAuth and OpenID Connect","text":"<p>Last update: 31 Jul 2023</p>","tags":["software","security","oauth","openid","oidc"]},{"location":"security/tokens/","title":"Tokens","text":"<p>Last update: 01 Aug 2023</p>","tags":["software","security","web","authentication","authorization","token","jwt"]},{"location":"security/tokens/#introduction","title":"Introduction","text":"<p>The security of web applications is often based on tokens.</p>","tags":["software","security","web","authentication","authorization","token","jwt"]},{"location":"security/tokens/#jwt","title":"JWT","text":"<p>The JSON Web Token (JWT), pronounced \"JOT\", is a popular kind of security token and it is based upon the JSON format.</p> <p>The typical JWT usage is as an authentication token:</p> <pre><code>Authentication: Bearer &lt;jwt&gt;\n</code></pre> <p>The JWT is divided into three base64 encoded sections, separated by a \".\".</p> <pre><code>header \".\" payload \".\" signature\n</code></pre> <p>Standard HEADER claims:</p> <ul> <li>typ: the type of the token (how the payload can be interpreted)</li> <li>alg: the name of the algorithm used to make the signature</li> </ul> <p>Header example :</p> <pre><code>    {\n        typ: 'JWT', \n        alg: 'HS256'\n    }\n</code></pre> <p>Standard PAYLOAD claims:</p> <ul> <li>iss: the issuer of the token</li> <li>sub: the subject of the token</li> <li>aud: the audience of the token</li> <li>exp: this will probably be the registered claim most often used. This will define the expiration as a NumericDate value. The expiration MUST be after the current date/time.</li> <li>nbf: (not before) defines the time before which the JWT MUST NOT be accepted for processing</li> <li>iat: (issued at) the time the JWT was issued. Can be used to determine the age of the JWT</li> <li>jti: unique identifier for the JWT. Can be used to prevent the JWT from being replayed. This is helpful for a one-time use token.</li> </ul> <p>Example:</p> <pre><code>    {  \n      \"iss\": \"http://example.org\",  \n      \"aud\": \"http://example.com\",  \n      \"iat\": 1356999524,  \n      \"nbf\": 1357000000,  \n      \"exp\": 1407019629,  \n      \"jti\": \"id123456\",  \n      \"typ\": \"https://example.com/register\",  \n      \"custom-property\": \"foo\",  \n      \"name\": \"Rob McLarty\",  \n      \"id\": 78  \n    }\n</code></pre> <p>Having an expiration date (exp claim) allows the emitting server to not handle the token disablement process. The expiration date is usually within 24 hours but it can be 1 or 2 hours long.</p> <p>JWT advantages:</p> <ul> <li>There is no need for server-side sessions</li> <li>the same token can be used for different endpoints/applications if they share the same secret or the same token validation endpoint.</li> <li>Cookies are not required so no CSRF issues.</li> <li>Compatible with CORS</li> <li>Compatible with non-browser-based applications</li> </ul>","tags":["software","security","web","authentication","authorization","token","jwt"]},{"location":"security/web-security/","title":"Web security","text":"<p>Last update: 01 Aug 2023</p>","tags":["software","security","web","authentication","authorization"]},{"location":"security/web-security/#references","title":"References","text":"<ol> <li>Web security by MDN</li> <li>Web security by the Chrome team</li> <li>Information security on Wikipedia</li> <li>Principle of least privilege</li> </ol>","tags":["software","security","web","authentication","authorization"]},{"location":"security/web-security/#introduction","title":"Introduction","text":"<p>Web security is the intersection of the following:</p> <ul> <li>Network security --&gt; TLS, firewall, load-balancer</li> <li>Information security (InfoSec): how we handle the data -&gt; cryptography, access control...</li> <li>Application security (AppSec): how we use the applications --&gt; vulnerability, credentials storage...</li> </ul> <p>Security-related tools:</p> <ul> <li>WAF, Web Application Firewall, HTTP level firewalls</li> <li>IDS, Intrusion Detection System</li> <li>IPS, Intrusion Prevention System</li> <li>HSH, Hardware Security Module</li> </ul> <p>The security goals are non-functional requirements (NFR) for all applications.</p> <p>Information security's (see reference 3) primary focus is data Confidentiality, Integrity and Availability (the CIA triad).</p> <p>STRIDE is a security model which describes the possible threads.</p> <ul> <li>Spoofing: when a person or a program assumes another person's identity</li> <li>Tampering: changing data and messages</li> <li>Repudiation: when someone disputes to be the author or an action</li> <li>Information disclosure (data leak, privacy breach): acquiring and publishing private info</li> <li>Denial of service (DoS)</li> <li>elevation of privilege</li> </ul>","tags":["software","security","web","authentication","authorization"]},{"location":"security/web-security/#http-codes","title":"HTTP codes","text":"<p>Security-related HTTP answers:</p> <ul> <li>401 User is not authenticated.</li> <li>403 The user is not allowed to perform the requested operation, even if they are authenticated.</li> </ul>","tags":["software","security","web","authentication","authorization"]},{"location":"security/web-security/#sop-origin","title":"SOP, ORIGIN","text":"<p>ORIGIN: in any URL, the origin is the union of the protocol, the server and the port (if specified). For example the origin in the URL https://www.example.com/pages/index.html is https://www.example.com. (port is 443 here).</p> <p>The same-origin policy (SOP) is a critical security mechanism that restricts how a document or script loaded from one origin can interact with a resource from another origin. It helps isolate potentially malicious documents, reducing possible attack vectors.</p> <p>SOP usually block read calls while writing calls (for ex. form submits) are usually allowed.</p> <p>For more details MDN on SOP</p>","tags":["software","security","web","authentication","authorization"]},{"location":"security/web-security/#cookie","title":"COOKIE","text":"<p>Cookie contents:</p> <ul> <li><code>path</code> = \"/\" is the path used by the browser to establish if the cookie should be sent to the server or not depending on the current page</li> <li><code>expiration</code> (expires and max-age) = cookie expiration date. If not specified, the cookie is a session cookie and it is deleted by the browser at the end of the session. If specified, the cookie is persistent. If the date is in the past, the cookie is deleted.</li> <li><code>attributes</code> = Secure (send only on HTTPS),  HttpOnly (not readable from JS code), SameSite</li> <li><code>Domain</code>: the domain of the cookie. If not specified, the cookie is \"host-only\"</li> <li><code>name</code> = the name of the cookie</li> </ul> <p>The suggested cookies attributes:</p> <ul> <li>Session cookie: HttpOnly, Secure, Path=/</li> <li>CSRF token cookie: Secure, SameSite=strict, Path=/</li> </ul>","tags":["software","security","web","authentication","authorization"]},{"location":"security/web-security/#csrf-cross-site-request-forgery-and-samesite-cookie-attribute","title":"CSRF (Cross-Site Request Forgery) and SameSite cookie attribute","text":"<p>Cross-site Request Forgery (CSRF) is a web attack where actions are performed using the user's identity without them knowing what is happening.</p> <p>See Wikipedia for more details.</p> <p>The user is logged in the website A and opens a new browser tab on Website B (the attacker). Website B calls website A back-end services; the browser sends the session cookie to the website A back-end which accepts the request. Even if the browser implements the same-origin policy, still many requests can be done from website B pages to website A server. </p> <p>The first defence line is to not have GET endpoints that perform create or update or delete actions. Use POST, UPDATE or DELETE methods as appropriate.</p> <p>The second defence line is to use the <code>SameSite</code> attribute for the session cookie; when set, the browser will not send the cookie if the page does not have the same site (domain) as the cookie.</p> <p>The SameSite attribute can have the following values:</p> <ul> <li>strict: the cookie is not sent in all cross-site requests</li> <li>lax: the cookie is sent on navigation requests only</li> <li>none: the cookie is always sent</li> </ul> <p>Alternative to the SameSite attribute is the <code>double-submit cookie</code> mechanism. At the first request, the server creates a special CSRF token cookie with a value that the client (the page) should include at every request using the X-CSRF-Token header. The server compares the value with the one that has been generated and allows the request accordingly.</p>","tags":["software","security","web","authentication","authorization"]},{"location":"security/web-security/#cors","title":"CORS","text":"<p>Cross-Origin Resource Sharing (CORS) is a security mechanism that allows websites and APIs to control and overcome the same origin policy implemented by the browser.</p> <p>See MDN for more details.</p> <p>The browser performs a pre-flight request, using the HTTP method OPTIONS, to evaluate if the cross-origin request is possible or not. </p> <p>The pre-flight response from the server includes several CORS headers:</p> <pre><code>Access-Control-Allow-Origin\nAccess-Control-Allow-Headers\nAccess-Control-Allow-Methods\nAccess-Control-Allow-Credentials (should be true if we use session cookies)\nAccess-Control-Max-Age\nAccess-Control-Expose-Headers\n</code></pre> <p>The pre-flight response status code can be:</p> <ul> <li>403: the request cannot be performed because not allowed by the resource server</li> <li>204 (no-contents): the request can be done</li> </ul>","tags":["software","security","web","authentication","authorization"]},{"location":"tools/","title":"Tools","text":""},{"location":"tools/#environments","title":"Environments","text":"<ul> <li>WSL2 on Windows 10/11</li> </ul>"},{"location":"tools/#ides-and-clients","title":"IDEs and clients","text":"<ul> <li>Bruno</li> <li>VSCode</li> </ul>"},{"location":"tools/#setup","title":"Setup","text":"<ul> <li>SDKMAN!</li> </ul>"},{"location":"tools/#version-control-systems","title":"Version control systems","text":"<ul> <li>GIT</li> <li>GIT-CRYPT</li> <li>SubVersion - SVN</li> </ul>"},{"location":"tools/#brokers-event-streaming","title":"Brokers - event streaming","text":"<ul> <li>Apache Kafka</li> <li>RabbitMQ</li> </ul>"},{"location":"tools/#database","title":"Database","text":"<ul> <li>IBM DB2</li> <li>PostgreSQL</li> <li>MySQL</li> <li>MongoDB</li> </ul>"},{"location":"tools/#cloud-kubernetes","title":"Cloud / Kubernetes","text":"<ul> <li>Kubernetes</li> <li>Helm</li> </ul>"},{"location":"tools/#security","title":"Security","text":"<ul> <li>GPG</li> <li>Spring Security</li> </ul>"},{"location":"tools/#language-specific","title":"Language-specific","text":"<ul> <li>Maven</li> <li>Java JDK tools</li> <li>Gradle</li> </ul>"},{"location":"tools/#application-servers","title":"Application Servers","text":"<ul> <li>GlassFish</li> </ul>"},{"location":"tools/git-crypt/","title":"GIT CRYPT","text":"<p>Last update: 30 Nov 2024</p>","tags":["tools","git","git-crypt","encryption"]},{"location":"tools/git-crypt/#introduction","title":"Introduction","text":"<p>git-crypt is an encryption/decryption tool integrated with git.</p> <p>It works under two modes:</p> <ol> <li>Symmetric key</li> <li>GPG key</li> </ol>","tags":["tools","git","git-crypt","encryption"]},{"location":"tools/git-crypt/#installation","title":"Installation","text":"<p>git-crypt binaries for Windows and Linux are available from the project repository on GitHub.</p> <p>On macOS, you can use brew:</p> <pre><code>brew install git-crypt\n</code></pre> <p>For other installation instructions, you can refer to the official documentation.</p>","tags":["tools","git","git-crypt","encryption"]},{"location":"tools/git-crypt/#configuration","title":"Configuration","text":"<p>We use the <code>.gitattributes</code> file to define which files in the repo should be encrypted:</p> <pre><code>secretfile filter=git-crypt diff=git-crypt\n*.key filter=git-crypt diff=git-crypt\nsecretdir/** filter=git-crypt diff=git-crypt\n</code></pre>","tags":["tools","git","git-crypt","encryption"]},{"location":"tools/git-crypt/#symmetric-key","title":"Symmetric key","text":"<p>To initialize the repository to use git-crypt</p> <pre><code>git-crypt init\n</code></pre> <p>This command will generate a symmetric key. To save the key to a file</p> <pre><code>git-crypt export-key /path/to/key\n</code></pre> <p>The file containing the key should be stored in a safe position and shared with everybody who need to access the encrypted files.</p> <p>IMPORTANT: the key file should not be committed in the repo.</p> <p>The other users of the repository should clone the project and run the following command:</p> <pre><code>git-crypt unlock /path/to/key\n</code></pre>","tags":["tools","git","git-crypt","encryption"]},{"location":"tools/git-crypt/#gpg-key","title":"GPG key","text":"<p>To initialize the repository to use git-crypt</p> <pre><code>git-crypt init\n</code></pre> <p>To add a GPP key to the repository git-crypt keys list:</p> <pre><code>git-crypt add-gpg-user key_id | GPG fingerprint | email\n</code></pre> <p>The other users of the repository should clone the project and run the following command:</p> <pre><code>git-crypt unlock\n</code></pre> <p>git-crypt will use the user GPG key(s) to unlock the repo.</p>","tags":["tools","git","git-crypt","encryption"]},{"location":"tools/git-crypt/#other-commands","title":"Other commands","text":"<p>To check if git-crypt has been configured correctly and all sensitive files are encrypted:</p> <pre><code>git-crypt status\n</code></pre>","tags":["tools","git","git-crypt","encryption"]},{"location":"tools/git/","title":"GIT","text":"<p>Last update: 12 Nov 2024</p>","tags":["tools","git"]},{"location":"tools/git/#basic-commands","title":"Basic commands","text":"<p>Clone an existing repository</p> <pre><code>git clone URL\ngit clone git://...\ngit clone https://\n</code></pre> <p>To create the initial repository:</p> <pre><code>git init\n</code></pre> <p>Add a new file or update it in the stage</p> <pre><code>git add\n</code></pre> <p>Commit (staged files)</p> <pre><code>git commit -m \"comment\"\n</code></pre> <p>Push changes</p> <pre><code>git push -v\ngit push --tags\n</code></pre> <p>Check Status</p> <pre><code>git status\ngit status --cached\n</code></pre> <p>Rebase ignoring whitespace differences</p> <pre><code>git rebase -v --ignore-whitespace\n</code></pre> <p>Prefer <code>git rebase</code> over <code>git merge</code> to have a linear commit history and to avoid merge commits.</p>","tags":["tools","git"]},{"location":"tools/git/#commits","title":"Commits","text":"<p>Each commit should be consistent, encompassing a single reason for all changes.</p> <p>If there are multiple elements to modify or add, please use multiple commits.</p> <p>Commit messages should begin with an action verb in the imperative form such as \u201cfix\u201d, \u201cupdate\u201d, or \u201cadd\u201d.</p> <p>Examples:</p> <pre><code>Update spring boot version to 3.2\nAdd configuration file for Prettier\nFix malformed URL creation\n</code></pre> <p>For those who are non-native English speakers: avoid using past tense forms like \u201cadded \u2026\u201d, \u201cupdated \u2026\u201d or third person forms like \"updates\u2026, \u201cfixes \u2026\u201d. Also, refrain from specifying the subject, such as \u201cit adds\u2026\u201d.</p>","tags":["tools","git"]},{"location":"tools/git/#aliases","title":"Aliases","text":"<p>Some useful shell aliases</p> <pre><code>alias gf='git fetch --all --prune -v'\nalias gr='git rebase -v --ignore-whitespace'\nalias gs='git status'\nalias gl='git log --oneline --graph --decorate'\n</code></pre>","tags":["tools","git"]},{"location":"tools/git/#configuration","title":"Configuration","text":"<p>Scopes:</p> <pre><code>* system: all users all repos\n* global: current user\n* local:  current repo\n</code></pre> <p>Commands:</p> <pre><code>git config --global merge.tool /c/APP/meld/meld/meld.exe\ngit config --list\ngit config user.name  \"John Doe\"\ngit config user.email \"j.doe@example.it\"\n\ngit config --global core.filemode false  ## Tell git to ignore file permission changes\ngit config --global core.editor \"vim\"\ngit config --global core.editor \"'c:/program files (x86)/sublime text 3/sublimetext.exe' -w\"\n\ngit config --global alias.st status      ## Alias\ngit config --global alias.ck checkout    ## Alias\n\ngit config push.default upstream   ## Avoid push on all branches\n</code></pre> <p>The .gitignore file</p> <pre><code>.gitignore            # for the current repo\n~/.config/git/ignore  # for all repos\n</code></pre>","tags":["tools","git"]},{"location":"tools/git/#logging","title":"Logging","text":"<p>Basic logging:</p> <pre><code>git log --pretty=oneline -5\n</code></pre> <p>Logging configuration:</p> <pre><code>git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\"\n</code></pre> <p>Show what has been changed</p> <pre><code>git log -p\n</code></pre> <p>Show last n commits</p> <pre><code>git log -&lt;n&gt;\n</code></pre> <p>Show stats for each new/changed file</p> <pre><code>git log --stat\n</code></pre> <p>Compare two branches</p> <pre><code>git log --left-right --graph --cherry-pick --oneline branch1...branch2\n</code></pre>","tags":["tools","git"]},{"location":"tools/git/#tags","title":"Tags","text":"<p>Create a new tag</p> <pre><code>git tag -a v1.4 -m 'my version 1.4'\n</code></pre> <p>Push tags</p> <pre><code>git push --follow-tags -v\n</code></pre> <p>List tags ordered by creation date</p> <pre><code>git for-each-ref --sort=creatordate --format '%(refname) %(creatordate)' refs/tags\n</code></pre>","tags":["tools","git"]},{"location":"tools/git/#branches","title":"Branches","text":"<p>Link a local branch to a remote one</p> <pre><code>git push --set-upstream origin &lt;remote-branch&gt;\n\ngit branch --set-upstream-to=origin/remote-branch local-branch\n</code></pre> <p>Create a new local branch</p> <pre><code>git checkout -b &lt;branch&gt;\n</code></pre> <p>Avoid pushing to all branches</p> <pre><code>git config push.default upstream\n</code></pre> <p>Clean the branch:</p> <pre><code>git clean -i .\ngit clean -f .\n</code></pre> <p>Remove all local commits and changes and reset the local branch to the remote branch status</p> <pre><code>git reset --hard @{u}\n</code></pre>","tags":["tools","git"]},{"location":"tools/git/#special-commands","title":"Special commands","text":"<p>GIT internal DB maintenance:</p> <pre><code>git gc --auto --prune\ngit fsck --full\ngit prune --expire now\n</code></pre> <p>Updates and cleanup remote branch list</p> <pre><code>git remote update origin --prune\n</code></pre> <p>Patch</p> <pre><code>git format-patch -1 &lt;sha1&gt; --stdout &gt; myPatch.patch\ngit log --pretty=oneline -7 | tac  | awk '{print \"git format-patch -1 \"  $1  \" --stdout &gt; \"}' &gt; 1\ngit am --signoff -k &lt; &lt;patch-file&gt;\n</code></pre> <p>Create a second remote repository</p> <pre><code>git remote add backup  URL\ngit remote set-url --add  --push backup  URL\ngit push backup &lt;branch-name&gt;\n</code></pre> <p>Change the committer email (DANGEROUS):</p> <pre><code>git filter-branch -f --env-filter '\nif [ \"$GIT_COMMITTER_EMAIL\" = \"old-email\" ];\nthen\n    export GIT_COMMITTER_EMAIL=\"new-email\"\n    export GIT_AUTHOR_EMAIL=\"new-email\"\nfi\n' HEAD\n</code></pre>","tags":["tools","git"]},{"location":"tools/kubernetes/","title":"Kubernetes command-line tools","text":"<p>Last update: 07 May 2023</p> <p>Several command-line tools can help us manage our Kubernetes clusters. One of them, <code>kubectl</code>, is required while the others are useful extensions.</p> <p>The tools are:</p> <ul> <li>kubectl, the main Kubernetes command line tool</li> <li>krew, the kubectl plugin manager</li> <li>kubectx (ctx), to easily switch the cluster</li> <li>kubens (ns), to easily switch the namespace</li> <li>stern, to see the pod logs</li> </ul>","tags":["tools","kubernetes","k8s","command-line","kubectl"]},{"location":"tools/kubernetes/#kubectl","title":"kubectl","text":"<p><code>kubectl</code> is the main Kubernetes command-line tool. </p> <p>Instructions on how to install it can be found here. WSL users: follow Linux instructions.</p> <p>Important: <code>kubectl</code> is a Kubernetes API client so its version should almost match (+- a minor revision) the Kubernets API server version. For example, a 1.22 kubectl client works with Kubernetes 1.21, 1.22, and 1.23 clusters.</p>","tags":["tools","kubernetes","k8s","command-line","kubectl"]},{"location":"tools/kubernetes/#krew","title":"krew","text":"<p>krew is the kubectl plugin manager. Many Kubernetes tools are also or only available as krew plugins.</p> <p>Instructions on how to install krew can be found here.</p>","tags":["tools","kubernetes","k8s","command-line","kubectl"]},{"location":"tools/kubernetes/#kubectx","title":"kubectx","text":"<p>kubectx allows us to switch from one Kubernetes cluster to another. Kubectx is available as a standalone executable or as a Krew plugin.</p> <p>To install Kubectx as a Krew plugin:</p> <pre><code>kubectl krew install ctx\n</code></pre> <p>Please note that the Kubectx plugin name is <code>ctx</code> not <code>kubectx</code>.</p> <p>kubectx reads the list of the available clusters from the <code>KUBECONFIG</code> environment variable that points to the cluster configuration files:</p> <pre><code>KUBECONFIG=\"&lt;path&gt;/cluster1.conf:&lt;path&gt;/cluster2.conf\"\n</code></pre> <p>To list the available clusters:</p> <pre><code>kubectl ctx\n</code></pre> <p>To select cluster1:</p> <pre><code>kubectl ctx cluster1\n</code></pre>","tags":["tools","kubernetes","k8s","command-line","kubectl"]},{"location":"tools/kubernetes/#kubens","title":"kubens","text":"<p>kubens allows us to switch from one Kubernetes cluster namespace to another namespace in the same cluster. Kubens is available as a standalone executable (available from the same Kubectx site) or as a Krew plugin.</p> <p>To install kubens as a Krew plugin:</p> <pre><code>kubectl krew install ns\n</code></pre> <p>Please note that the Kubens plugin name is <code>ns</code> not <code>kubens</code>.</p> <p>To list the available namespaces in the cluster:</p> <pre><code>kubectl ns\n</code></pre> <p>To select the namespace prod:</p> <pre><code>kubectl ns prod\n</code></pre>","tags":["tools","kubernetes","k8s","command-line","kubectl"]},{"location":"tools/kubernetes/#stern","title":"stern","text":"<p>Stern is a pod log viewer. It can show and tail logs from multiple pods and multiple containers on the same pod. The log lines from the different sources will be shown in different colours.</p> <p>Stern can be installed as a standalone tool or as Krew plugin:</p> <pre><code>kubectl krew install ns\n</code></pre> <p>To log from all pod containers (if more than one):</p> <pre><code>kubectl stern &lt;pod-name&gt;\n</code></pre> <p>The name of the pod can use wildcards, quite useful when the pod as more than one replica.</p> <p>If you're interested in a specific od container:</p> <pre><code>kubectl stern &lt;pod-name&gt; -c &lt;container-name&gt;\n</code></pre>","tags":["tools","kubernetes","k8s","command-line","kubectl"]},{"location":"tools/subversion/","title":"SubVersion (svn)","text":"<p>Last update: 16 Aug 2022</p> <p>Checkout</p> <pre><code>svn co --username &lt;user&gt; --password &lt;password&gt; svn://&lt;host&gt;/&lt;repo&gt;\n</code></pre> <p>Status</p> <pre><code>svn status\n</code></pre> <p>Local copy cleanup</p> <pre><code>svn cleanup\n</code></pre> <p>Branch/tag creation</p> <pre><code>svn copy &lt;trunk&gt; branches/&lt;branch&gt; -m \"...\"\nsvn copy https://&lt;host&gt;/&lt;repo&gt;/&lt;proj&gt;  https://&lt;host&gt;/&lt;repo&gt;/tags/&lt;proj&gt;/&lt;tag&gt; -m \"...\"\n</code></pre> <p>Branch checkout</p> <pre><code>cd branches\nsvn co http://&lt;host&gt;/svn/&lt;repo&gt;/branches/&lt;branch&gt;\n</code></pre> <p>List repo files</p> <pre><code>svn ls  https://&lt;host&gt;/&lt;repo&gt;/tags\n</code></pre> <p>Update branch with trunk updates</p> <pre><code>cd branches/&lt;branch&gt;\nsvn merge ^/calc/trunk\n</code></pre> <p>Update trunk with branch contents</p> <pre><code>cd trunk\nsvn update\nsvn merge --reintegrate branches/&lt;branch&gt;\n</code></pre> <p>Ignore current dir</p> <pre><code>svn propedit svn:ignore .\n</code></pre> <p>Global ignores</p> <pre><code>svn propedit svn:global-ignores .\n</code></pre>","tags":["tools","subversion","svn"]},{"location":"tools/application-servers/glassfish/","title":"GlassFish","text":"<p>Last update: 10 Feb 2023</p>","tags":["application server","glassfish","oracle"]},{"location":"tools/application-servers/glassfish/#passwords-handling","title":"Passwords handling","text":"<p>Examples:</p> <pre><code>asadmin --host localhost --port 4848 login\nasadmin --host localhost --port 4848 --user admin change-admin-password\n</code></pre>","tags":["application server","glassfish","oracle"]},{"location":"tools/application-servers/glassfish/#domains","title":"Domains","text":"<p>The <code>asadmin</code> tool is located in the  <code>&lt;appserver-installation-directory&gt;/bin</code> dir.</p> <p>Start the default domain:</p> <pre><code>asadmin start-domain\n</code></pre> <p>Start the domain xxxx:</p> <pre><code>asadmin start-domain xxxx\n</code></pre> <p>Stop the default domain:</p> <pre><code>asadmin stop-domain\n</code></pre> <p>Delete a domain:</p> <pre><code>asadmin delete-domain xxxx\n</code></pre> <p>Restart the domain:</p> <pre><code>asadmin restart-domain xxxx\n</code></pre> <p>List available domains:</p> <pre><code>asadmin list-domains\n</code></pre> <p>Create a new domain xxxx:</p> <pre><code>asadmin create-domain --user admin --nopassword --instanceport 8100 xxxx\n</code></pre>","tags":["application server","glassfish","oracle"]},{"location":"tools/brokers/kafka/","title":"Apache Kafka","text":"<p>Last update: 24 Aug 2024</p> <p>Apache Kafka is a distributed event store and stream-processing platform written in Java and Scala and developed by LinkedIn and now Apache Foundation.</p> <p>Study material for the Confluent Developer certification can be found here.</p>","tags":["tools","broker","event-streaming","apache","kafka"]},{"location":"tools/cloud/helm/","title":"Helm","text":"<p>Last update: 2 Oct 2022</p> <p>Helm is a package manager for Kubernetes (K8S). Its server-side part is named Tiller.</p>","tags":["tools","kubernetes","k8s","cloud","helm"]},{"location":"tools/cloud/helm/#installation","title":"Installation","text":"<p>Follow intructions from the Helm web site</p> <p>Check if <code>Tiller</code> is installed in the cluster:</p> <pre><code>kubectl get serviceaccount --all-namespaces | grep tiller\n</code></pre> <p>Install <code>Tiller</code> in the k8s cluster:</p> <pre><code>kubectl create serviceaccount tiller -n kube-system\nkubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller -n kube-system\nhelm init --service-account tiller\n</code></pre>","tags":["tools","kubernetes","k8s","cloud","helm"]},{"location":"tools/cloud/helm/#command-line-commands","title":"Command line commands","text":"<p>Download a Helm chart content:</p> <pre><code>helm fetch --untar &lt;helm-chart&gt;\n</code></pre> <p>Install a Helm chart from the current dir:</p> <pre><code>helm install .\n</code></pre> <p>Upgrade a Helm chart:</p> <pre><code>helm upgrade\n</code></pre> <p>Rollback an upgrade:</p> <pre><code>helm rollback\n</code></pre>","tags":["tools","kubernetes","k8s","cloud","helm"]},{"location":"tools/db/db2/","title":"IBM DB2","text":"<p>Last update: 26 Sep 2022</p>","tags":["tools","ibm","db2","udb","dbms"]},{"location":"tools/db/db2/#general-info","title":"General info","text":"<ul> <li>On Unix/Linux, the operating system user is also the DB instance administrator.</li> <li>On Windows, the DB administrator is not the Windows user.</li> <li>Every DB2 instance can contain one or more databases</li> <li>Every database can contain one or more schemas.</li> </ul> <p>Row size limit:</p> <ul> <li>4005 bytes in a table space with a 4K page size</li> <li>8101 bytes in a table space with an 8K page size</li> <li>16293 bytes in a table space with a 16K page size</li> <li>32677 bytes in a table space with a 32K page size\u00a0</li> </ul>","tags":["tools","ibm","db2","udb","dbms"]},{"location":"tools/db/db2/#tables-and-indexes-maintenance","title":"Tables and indexes maintenance","text":"<p>Show statistics about tables and indexes.</p> <pre><code>RUNSTATS ON TABLE &lt;table-name&gt; AND SAMPLED DETAILED INDEXES ALL\nRUNSTATS ON TABLE &lt;table-name&gt; WITH DISTRIBUTION AND DETAILED INDEXES ALL ALLOW WRITE ACCESS\n</code></pre> <p>To verify which tables require a reorg:</p> <pre><code>select tabschema, tabname from sysibmadm.admintabinfo where reorg_pending = 'Y'\n</code></pre> <p>Executing the reorg of the tables/indexes</p> <pre><code>REORG TABLE &lt;table-name&gt;\nREORG INDEXES ALL FOR TABLE &lt;table-name&gt;\n</code></pre> <p>Check the current tables and indexes status</p> <pre><code>reorgchk current statistics on table all\nreorgchk update statistics on table all\nreorgchk update statistics on table user\n</code></pre>","tags":["tools","ibm","db2","udb","dbms"]},{"location":"tools/db/db2/#command-line","title":"Command line","text":"<p>How to start DB2</p> <pre><code>db2start\n</code></pre> <p>Connect</p> <pre><code>db2 connect to &lt;database-name&gt; user &lt;user-name&gt; using password\n</code></pre> <p>Dump a database contents into a file</p> <pre><code>db2look -d &lt;database-name&gt; -a -e -o &lt;sql file name&gt;\n</code></pre> <p>Show DB instance Status</p> <pre><code>db2pd -everything\n</code></pre> <p>Global registry</p> <pre><code>db2greg -dump\n</code></pre> <p>Run the internal monitoring</p> <pre><code>db2top -d &lt;database-name&gt;\n</code></pre> <p>Backup</p> <pre><code>db2 backup database &lt;database-name&gt; online to &lt;file-name&gt; with 2 buffers buffer 1024 parallelism 1 compress include logs without prompting;\n</code></pre> <p>Restore</p> <pre><code>db2 restore db &lt;database-name&gt; from &lt;file-name&gt; taken at &lt;timestamp&gt;\ndb2 restore db &lt;database-name&gt; continue\ndb2 rollforward db &lt;database-name&gt; to end of logs and complete\n</code></pre>","tags":["tools","ibm","db2","udb","dbms"]},{"location":"tools/db/db2/#queries-performances-analysis","title":"Queries performances analysis","text":"<p>Design Advisor:</p> <pre><code>db2advis -d &lt;database-name&gt; -n &lt;schema-name&gt; -s 'select * from ..'\ndb2advis -d &lt;database-name&gt; -i &lt;file with one or more queries&gt;\n</code></pre> <p>Query explains:</p> <pre><code>db2expln -d &lt;database-name&gt; -t -q 'select * from .. '\ndb2expln -d &lt;database-name&gt; -t -stmtfile &lt;file with one or more queries&gt; -terminator \\;\n</code></pre>","tags":["tools","ibm","db2","udb","dbms"]},{"location":"tools/db/mongodb/","title":"MongoDB","text":"<p>Last update: 28 Sep 2022</p>","tags":["tools","mongodb","dbms"]},{"location":"tools/db/mongodb/#general-info","title":"General info","text":"<p>Comparison between a SQL DBMS and MongoDB</p> SQL MongoDB schema db table collection record document <p>In MongoDB, every document should contain an _id field.  Its type is String if manually assigned or ObjectId if assigned by MongoDB.</p> <p>An URI in srv format:</p> <pre><code>mongodb+srv://USERNAME:PASSWORD@HOST/DATABASE?options\n</code></pre>","tags":["tools","mongodb","dbms"]},{"location":"tools/db/mongodb/#command-line-tools","title":"Command line tools","text":"<ul> <li>mongod : run the server in interactive mode</li> <li>mongo  : shell client (CLI)</li> <li>mongodb-compass: graphical client</li> </ul>","tags":["tools","mongodb","dbms"]},{"location":"tools/db/mongodb/#commands","title":"Commands","text":"<p>Select the DB that it is created if not existing</p> <pre><code>use &lt;db&gt;\n</code></pre> <p>Show all collections in a db</p> <pre><code>show collections\n</code></pre> <p>Insert one or more documents in a collection</p> <pre><code>db.&lt;coll&gt;.insertOne ( &lt;document&gt; )\ndb.&lt;coll&gt;.insertMany ( [ &lt;document1&gt;, &lt;document2&gt;..] )\n</code></pre> <p>Insert one or more documents in a collection skipping any error</p> <pre><code>db.&lt;coll&gt;.insertMany ( [ &lt;document1&gt;, &lt;document2&gt;..], { \"ordered\":false} )\n</code></pre> <p>Query by fields</p> <pre><code>db.&lt;coll&gt;.find ( {fields} )\n</code></pre> <p>Query by fields with projection (1=show the field, 0=hide the field)</p> <pre><code>db.&lt;coll&gt;.find ( {fields}, {projection})\n</code></pre> <p>Query with formatted results</p> <pre><code>db.&lt;coll&gt;.find ( ... ).pretty()\n</code></pre> <p>Count the documents returned by the query</p> <pre><code>db.&lt;coll&gt;.find ( ... ).count()\n</code></pre> <p>Update one document</p> <pre><code>db.&lt;coll&gt;.updateOne(&lt;selector&gt;, {operators})\n</code></pre> <p>Update many document</p> <pre><code>db.&lt;coll&gt;.updateMany(&lt;selector&gt;, {operators})\n</code></pre> <p>Upsert (insert or update)</p> <pre><code>db.&lt;coll&gt;.upsert\n</code></pre> <p>Updates operators:</p> <ul> <li>scalar: $set, $unset, $min, $max, $inc..</li> <li>array: $addToSet, $pop, $popAll, $push, $pull</li> </ul>","tags":["tools","mongodb","dbms"]},{"location":"tools/db/mysql/","title":"MySQL","text":"<p>Last update: 28 Sep 2022</p>","tags":["tools","oracle","mysql","dbms"]},{"location":"tools/db/mysql/#command-line-commands","title":"Command line commands","text":"<p>How to login</p> <pre><code>mysql -u &lt;user-name&gt; -p\n</code></pre>","tags":["tools","oracle","mysql","dbms"]},{"location":"tools/db/mysql/#users-and-grants","title":"Users and grants","text":"<p>Create new user:</p> <pre><code>grant all privileges on &lt;db-name&gt;.* to '&lt;user-name&gt;'@'localhost' IDENTIFIED BY '&lt;user-password&gt;' WITH GRANT OPTION;\n</code></pre> <p>Add grants:</p> <pre><code>grant all on &lt;db-name&gt;.* to '&lt;user-name&gt;'@'localhost'\n</code></pre>","tags":["tools","oracle","mysql","dbms"]},{"location":"tools/db/postgresql/","title":"PostgreSQL","text":"<p>Last update: 28 Sep 2022</p>","tags":["tools","postgresql","dbms"]},{"location":"tools/db/postgresql/#command-line","title":"Command line","text":"<p>Connect client to an instance</p> <pre><code>psql --dbname=&lt;db-name&gt; \\\n   --host=&lt;host&gt; \\\n   --port=&lt;port&gt; \\\n   --username=&lt;username&gt;\n</code></pre>","tags":["tools","postgresql","dbms"]},{"location":"tools/db/postgresql/#client","title":"Client","text":"<p>Exit from the client</p> <pre><code>\\q\n</code></pre> <p>Enable verbose messages</p> <pre><code>\\set VERBOSITY verbose\n</code></pre>","tags":["tools","postgresql","dbms"]},{"location":"tools/db/postgresql/#backup","title":"BACKUP","text":"<p>Dump database schema to file</p> <pre><code>pg_dump &lt;db-name&gt; --format=p --username=&lt;username&gt; --verbose  --schema-only --file=&lt;filename&gt;.ddl\n</code></pre> <p>Dump database data to file    </p> <pre><code>pg_dump &lt;db-name&gt; --format=p --username=&lt;username&gt; --verbose  --data-only --column-inserts --file=&lt;filename&gt;.ddl\n</code></pre> <p>Dump everything</p> <pre><code>pg_dump &lt;db-name&gt; --format=c --username=&lt;username&gt; --verbose  --file=&lt;filename&gt;.backup\n</code></pre> <p>Restore everything</p> <pre><code>pg_restore -U &lt;username&gt; --dbname=&lt;db-name&gt; &lt;filename&gt;.backup\n</code></pre> <p>Restore data    </p> <pre><code>pg_restore &lt;db-name&gt; --username=&lt;username&gt; --data_only --verbose --file=&lt;filename&gt;.ddl\n</code></pre>","tags":["tools","postgresql","dbms"]},{"location":"tools/ide/bruno/","title":"Bruno","text":"<p>Last update: 8 Sep 2024</p> <p>Article on Medium</p> <p>Links:</p> <ul> <li>Bruno website</li> <li>Bruno command line tools</li> <li>GitHub</li> <li>Documentation</li> <li>Visual Studio Code extension</li> <li>Migrating from Postman</li> </ul>","tags":["tools","ide","client","bruno","api","postman"]},{"location":"tools/ide/bruno/#how-to-use-bruno","title":"How to use Bruno","text":"<p>The suggested directory structure is like the following:</p> <pre><code>\u251c\u2500\u2500 .env                     optional, for secrets and passwords\n\u251c\u2500\u2500 API 1                    dir to group requests belonging to the same API\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 request-1.bru\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 request-2.bru\n\u251c\u2500\u2500 API 2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 request-1.bru\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 bruno.json              the main Bruno file\n\u251c\u2500\u2500 environments            the Bruno environments\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 prod.bru\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test.bru\n\u251c\u2500\u2500 node_modules\n\u251c\u2500\u2500 package-lock.json\n\u2514\u2500\u2500 package.json            BRUNO CLI and additional node.js dependencies\n</code></pre> <p>API 1 and API 2 are folders grouping the requests for each API.</p>","tags":["tools","ide","client","bruno","api","postman"]},{"location":"tools/ide/vscode/","title":"VSCode","text":"<p>Last update: 19 Jan 2025</p>","tags":["tools","ide","client","vscode"]},{"location":"tools/ide/vscode/#gitignore-configuration","title":".gitignore configuration","text":"<p>The files generated by VSCode should be excluded (ignored) from the repo except the file that contains the recommended extensions for the project. This file is named <code>extensions.json</code>.</p> <p><code>.gitignore</code> fragment:</p> <pre><code>.vscode/*\n!.vscode/extensions.json\n</code></pre> <p>The first line excludes all VSCode files, the second line re-enables the file with the recommended extensions.</p>","tags":["tools","ide","client","vscode"]},{"location":"tools/ide/vscode/#recommended-extensions-file","title":"Recommended extensions file","text":"<p>The recommended extensions for the project can be saved in the project <code>extensions.json</code> file. All members of the team working on the project will be suggested by VSCode to install that extensions.</p> <p>The content of the file is the following:</p> <pre><code>{\n    \"recommendations\": [\n        \"extension1-id\",\n        \"extension2-id\",\n        ....\n    ]\n}\n</code></pre> <p>Please note that the extensions are identified by their ID which can be found in the extension page as Identifier.</p>","tags":["tools","ide","client","vscode"]},{"location":"tools/ide/vscode/#suggested-plugins","title":"Suggested plugins","text":"<p>The below list is my personal list of suggested plugins:</p> ID Name Purpose Scope editorconfig.editorconfig Editorconfig support File encoding and formatting All projects sonarsource.sonarlint-vscode Sonarlint/qube Source quality checks All projects davidanson.vscode-markdownlint Markdown supporto Markdown files editing and linking All projects ltex-plus.vscode-ltex-plus Language tool Spelling and grammar checks All projects mechatroner.rainbow-csv CSV support Syntax and checks for CSV files All projects esbenp.prettier-vscode Prettier support Prettier formatter Front-end projects dbaeumer.vscode-eslint ESLint support JavaScript checker Front-end projects stylelint.vscode-stylelint CSS support CSS checker Front-end projects bruno-api-client.bruno Bruno support The alternative to Postman All projects vscjava.vscode-java-pack Java/JVM support Microsoft extensions bundle for Java Java/JVM projects vmware.vscode-boot-dev-pack Spring Boot support Vmware extensions bundle for Spring Framework and Spring Boot development Java/JVM projects using Spring vscjava.vscode-lombok Lombok support Support for Lombok Java projects nicolasvuillamy.vscode-groovy-lint Groovy support Linter and formatter Groovy/Grails projects <p>Possible extensions file contents:</p> <pre><code>{\n    \"recommendations\": [\n        \"editorconfig.editorconfig\",\n        \"esbenp.prettier-vscode\",\n        \"dbaeumer.vscode-eslint\",\n        \"stylelint.vscode-stylelint\",\n        \"sonarsource.sonarlint-vscode\"\n        \"bruno-api-client.bruno\"\n        \"mechatroner.rainbow-csv\"\n        \"davidanson.vscode-markdownlint\",\n        \"ltex-plus.vscode-ltex-plus\"\n        \"nicolasvuillamy.vscode-groovy-lint\",\n        \"vscjava.vscode-java-pack\",\n        \"vmware.vscode-boot-dev-pack\",\n        \"vscjava.vscode-lombok\",\n        \"nicolasvuillamy.vscode-groovy-lint\"\n    ]\n}\n</code></pre>","tags":["tools","ide","client","vscode"]},{"location":"tools/security/gpg/","title":"GNU Privacy Guard (GnuPG or GPG)","text":"<p>Last update: 02 Oct 2022</p>","tags":["tools","security","privacy","gpg"]},{"location":"tools/security/gpg/#gpg-key","title":"GPG key","text":"<p>To create a new GPG key, just follow these instructions.</p>","tags":["tools","security","privacy","gpg"]},{"location":"tools/security/gpg/#command-line-commands","title":"Command line commands","text":"<p>Export a public GPG key in binary format:</p> <pre><code>gpg --output public.gpg --export &lt;email&gt;\n</code></pre> <p>Export a public GPG key in text format:</p> <pre><code>gpg --output public.txt --export --armor &lt;email&gt;\n</code></pre>","tags":["tools","security","privacy","gpg"]},{"location":"tools/setup/sdkman/","title":"SDKMAN!","text":"<p>Last update: 11 Feb 2025</p> <p>Links:</p> <ul> <li>SDKMAN! website</li> </ul> <p>SDKMAN! lets you install several versions of the same tool, such as different JDK releases, and decide which version to use for a specific project or terminal.</p> <p>For example, the following command:</p> <pre><code>sdk use java 8.0.352-tem\n</code></pre> <p>Set the current JDK to the Temurin 8.0.352. The JDK should be first installed with the command:</p> <pre><code>sdk install java 8.0.352-tem\n</code></pre>","tags":["tools","setup","sdkman"]},{"location":"tools/setup/sdkman/#how-to-install-sdkman","title":"How to install SDKMAN!","text":"<p>Just follow the instructions on the home page.</p>","tags":["tools","setup","sdkman"]},{"location":"tools/setup/sdkman/#how-to-configure-sdkman","title":"How to configure SDKMAN!","text":"<p>The command to configure SDKMAN! is</p> <pre><code>sdk config\n</code></pre> <p>There are a few options to configure how SDKMAN! works, the most important being the auto env option.</p> <p>If set to true, the auto env option (<code>sdkman_auto_env</code> variable in the configuration) enables the terminal environment to be automatically configured when we enter a directory containing a file named <code>.sdkmanrc</code>.</p> <p>The file .sdkmanrc contains the tools and their versions that should be enabled in the terminal.</p> <p>An example of the .sdkmanrc file is</p> <pre><code># Enable auto-env through the sdkman_auto_env config\n# Add key=value pairs of SDKs to use below\njava=21.0.6-tem\nmaven=3.9.9\n</code></pre> <p>The file can be initially created with the command:</p> <pre><code>sdk env init\n</code></pre> <p>When we enter the directory, SDKMAN! will configure the terminal environment with the tools and versions specified in the file.</p> <p>If the tool has not been installed yet, SDKMAN! suggest to install it with a message similar to this</p> <pre><code>Stop! java 21.0.5-tem is not installed. \nRun 'sdk env install' to install it.\n</code></pre> <p>This option lets the autoconfiguration of the environment when you enter a directory. When auto env is enabled, SDKMAN! looks for a file name  </p>","tags":["tools","setup","sdkman"]},{"location":"tools/wsl/","title":"WSL2 on Windows 10/11","text":"<ul> <li>Installation</li> <li>Windows Terminal</li> <li>VS-Code</li> <li>Java tools</li> <li>Node.js tools</li> <li>Docker</li> </ul>"},{"location":"tools/wsl/docker/","title":"Docker on on WSL2","text":"<p>Last update: 7 May 2023</p> <p>Docker Engine and command line tools can be installed on the WSL Linux distribution.</p>","tags":["tools","ide","wsl","wsl2","docker"]},{"location":"tools/wsl/docker/#installation","title":"Installation","text":"<p>1) Install the docker engine on the Linux distribution. I suggest installing Docker using the Linux repository, following the official guide.</p> <p>Warning: the guide suggests checking the installation using the docker \"hello-world\" image but the check will fail because there are other steps to be executed.</p> <p>2) Enable non-root users to run docker. Just follows the procedure here</p> <p>3) Enable the user to start the docker service</p> <p>The docker service is not automatically executed (as a Linux daemon) inside the WSL2. Moreover, you need root rights to start it.</p> <p>If you want to avoid running the service without being the root user, run the following command:</p> <pre><code>sudo visudo -f /etc/sudoers.d/passwordless_docker_start\n</code></pre> <p>and enter the following content in the open file, replacing <code>&lt;username&gt;</code> with your user name:</p> <pre><code>&lt;username&gt;   ALL = (root) NOPASSWD: /usr/sbin/service docker start\n</code></pre> <p>See here for more details.</p> <p>After this change, you can run the following command</p> <pre><code>service docker start\n</code></pre> <p>without entering sudo credentials.</p> <p>4) If you like to have the docker daemon running when you open the WSL terminal window, you can add the following lines to your .profile file:</p> <pre><code>    if [ -n \"`service docker status | grep not`\" ]; then\n        sudo /usr/sbin/service docker start\n    fi\n</code></pre>","tags":["tools","ide","wsl","wsl2","docker"]},{"location":"tools/wsl/docker/#commands","title":"Commands","text":"<p>Note: the docker-compose command is now obsolete, replaced by the following command:</p> <pre><code>docker compose &lt;compose-options&gt;\n</code></pre> <p>See here for details.</p>","tags":["tools","ide","wsl","wsl2","docker"]},{"location":"tools/wsl/installation/","title":"WSL2 installation and commands","text":"<p>Last update: 7 Mar 2023</p> <p>WSL stands for Windows Subsystem for Linux. Its current version is 2.</p> <p>This guide shows how to install and use WSL2 on Windows 10/11.</p>","tags":["tools","wsl","wsl2","installation"]},{"location":"tools/wsl/installation/#installation","title":"Installation","text":"<p>Open a command window (cmd, PowerShell or Windows Terminal) and run the command</p> <pre><code>wsl --install\n</code></pre> <p>It will enable WSL and install the latest Linux Ubuntu version.</p> <p>If you want a different Linux distribution, you can get the list of the supported Linux distros with the following command:</p> <pre><code>wsl -l -o\n</code></pre> <p>See here for more details.</p>","tags":["tools","wsl","wsl2","installation"]},{"location":"tools/wsl/installation/#commands","title":"Commands","text":"<p>To check the status of WSL engine and distributions, use the following command:</p> <pre><code>wsl --status\n</code></pre> <p>To upgrade WSL to the latest version/patch, use the following command:</p> <pre><code>wsl --upgrade\n</code></pre>","tags":["tools","wsl","wsl2","installation"]},{"location":"tools/wsl/java-tools/","title":"Java development tools on WSL2","text":"<p>Last update: 7 Mar 2023</p> <p>If you want to develop Java applications using WSL2 and VSCode, you need to install all required tools on the WSL Linux distribution (not on Windows!):</p> <ul> <li>The JDK</li> <li>Maven or Gradle</li> <li>Maybe Groovy</li> </ul>","tags":["tools","ide","wsl","wsl2","windows","java"]},{"location":"tools/wsl/java-tools/#sdkman","title":"SDKMan","text":"<p>I usually install SDKMan! to have Java development tools installed: the JDK, Maven...</p> <p>SDKMan lets you install several versions of the same tool like, for example, different JDK releases, and decide which version to use for a specific project or terminal.</p> <p>For example:</p> <pre><code>sdk use java 8.0.352-tem\n</code></pre> <p>set the current JDK to the Temurin 8.0.352. The JDK should be first installed with the command:</p> <pre><code>sdk install java 8.0.352-tem\n</code></pre>","tags":["tools","ide","wsl","wsl2","windows","java"]},{"location":"tools/wsl/nodejs-tools/","title":"Node.js development tools on WSL2","text":"<p>Last update: 7 Mar 2023</p> <p>If you want to develop Node.js applications using WSL2 and VSCode, you need to install all required tools on the WSL Linux distribution (not on Windows!).</p>","tags":["tools","ide","wsl","wsl2","windows","nodejs","web"]},{"location":"tools/wsl/nodejs-tools/#nvm","title":"NVM","text":"<p>To install Node.js and related tools on the WSL, there are many opetions but I suggest to use nvm, a Node version manager.</p> <p>To install the latest Node LTS version, run the following command:</p> <pre><code>nvm install --lts\n</code></pre>","tags":["tools","ide","wsl","wsl2","windows","nodejs","web"]},{"location":"tools/wsl/vscode/","title":"VS Code on WSL2","text":"<p>Last update: 6 Mar 2023</p> <p>Visual Studio Code supports the development of projects located on the WSL (Ubuntu or other Linux distro) filesystem.</p>","tags":["tools","ide","wsl","wsl2","windows"]},{"location":"tools/wsl/vscode/#installation","title":"Installation","text":"<p>You need to install VSCode on Windows and then install the \"Remote Development Extension Pack\" which allows VSCode to access the WSL filesystem via a client-server connection.</p> <p>See here for more details.</p>","tags":["tools","ide","wsl","wsl2","windows"]},{"location":"tools/wsl/vscode/#commands","title":"Commands","text":"<p>Open the WSL terminal, go to the project dir and run the following command:</p> <pre><code>code .\n</code></pre> <p>VSCode will install the VSCode server extension (just the first time) and start VSCode on Windows.</p> <p>Project building and linting will run inside the WSL Linux while the editing is on Windows. Java, Node.js and any tool you need to work on the project should be installed on Linux.</p>","tags":["tools","ide","wsl","wsl2","windows"]},{"location":"tools/wsl/windows-terminal/","title":"Windows Terminal","text":"<p>Last update: 6 Mar 2023</p> <p>Windows Terminal is a new Windows command-line application from Microsoft which supports traditional Windows shells like cmd and PowerShell in addition to WSL Linux shells (bash, sh,..).</p> <p>Windows Terminal is the preferred way to use WSL.</p>","tags":["tools","wsl","wsl2","windows","terminal","cnd","powershell"]},{"location":"tools/wsl/windows-terminal/#installation","title":"Installation","text":"<p>Install the Windows Terminal application, which replaces the old \"cmd\" and supports Linux shells, in addition to the Windows PowerShell and Command.</p> <p>Just follow the Microsft guide to install the Windows Terminal application.</p>","tags":["tools","wsl","wsl2","windows","terminal","cnd","powershell"]}]}